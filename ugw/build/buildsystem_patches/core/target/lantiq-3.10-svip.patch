VINETICLCSW-953: initial svip support in 3.10

diff --git a/package/pjsip/Makefile b/package/pjsip/Makefile
--- a/package/pjsip/Makefile
+++ b/package/pjsip/Makefile
@@ -70,7 +70,7 @@ CONFIGURE_ARGS += \
 EXTRA_CFLAGS:=-I$(STAGING_DIR)/usr/include/drv_tapi -I$(STAGING_DIR)/usr/include/drv_vmmc
 endif
 
-Package/pjsip-oss=$(call Package/pjsip-template,oss,BROKEN)
+Package/pjsip-oss=$(call Package/pjsip-template,oss,)
 Package/pjsip-ltq-tapi=$(call Package/pjsip-template,ltq-tapi,$(LTQ_TAPI_DEPENDS))
 
 USE_LOCAL=$(shell ls ./src/ 2>/dev/null >/dev/null && echo 1)
diff --git a/package/pjsip/patches/0001-configure-fixup.patch b/package/pjsip/patches/0001-configure-fixup.patch
--- a/package/pjsip/patches/0001-configure-fixup.patch
+++ b/package/pjsip/patches/0001-configure-fixup.patch
@@ -13,21 +13,6 @@ Index: pjproject-1.14.2/aconfigure.ac
  AC_SUBST(LD)
  if test "$LDOUT" = ""; then LDOUT="-o "; fi
  AC_SUBST(LDOUT)
-@@ -584,13 +583,7 @@
- 	;;
-   *)
- 	dnl # Check if ALSA is available
--	ac_pjmedia_snd=pa_unix
--	AC_CHECK_HEADER(alsa/version.h,
--			[AC_SUBST(ac_pa_use_alsa,1)
--			 LIBS="$LIBS -lasound"
--			],
--		        [AC_SUBST(ac_pa_use_alsa,0)])
--	AC_MSG_RESULT([Checking sound device backend... unix])
-+        AC_SUBST(ac_pa_use_alsa,0)
- 
- 	dnl # Check if OSS is disabled
- 	AC_SUBST(ac_pa_use_oss,1)
 @@ -617,6 +610,15 @@
  	       fi]
  	      )
diff --git a/target/linux/lantiq/config-default b/target/linux/lantiq/config-default
--- a/target/linux/lantiq/config-default
+++ b/target/linux/lantiq/config-default
@@ -151,6 +151,7 @@ CONFIG_RTL8366_SMI=y
 CONFIG_SERIAL_LANTIQ=y
 # CONFIG_SOC_AMAZON_SE is not set
 # CONFIG_SOC_FALCON is not set
+# CONFIG_SOC_SVIP is not set
 CONFIG_SOC_TYPE_XWAY=y
 CONFIG_SOC_XWAY=y
 # CONFIG_SPI_XWAY_BV is not set
diff --git a/target/linux/lantiq/modules.mk b/target/linux/lantiq/modules.mk
--- a/target/linux/lantiq/modules.mk
+++ b/target/linux/lantiq/modules.mk
@@ -78,3 +78,38 @@ define KernelPackage/spi-lantiq-ssc-csi/
 endef
 
 $(eval $(call KernelPackage,spi-lantiq-ssc-csi))
+
+define KernelPackage/lantiq-svip-ve
+  TITLE:=Lantiq SVIP virtual ethernet
+  SUBMENU:=Lantiq
+  DEPENDS:=@TARGET_lantiq
+  KCONFIG:=CONFIG_LANTIQ_SVIP_VIRTUAL_ETH=y
+endef
+
+define KernelPackage/lantiq-ve/description
+  Lantiq SVIP virtual ethernet
+endef
+
+$(eval $(call KernelPackage,lantiq-svip-ve))
+
+define KernelPackage/lantiq-svip-nat
+  TITLE:=Lantiq SVIP NAT
+  SUBMENU:=Lantiq
+  DEPENDS:=@TARGET_lantiq
+  KCONFIG:=CONFIG_LTQ_SVIP_NAT=y \
+	  CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK=y \
+	  CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE=10 \
+	  CONFIG_LTQ_SVIP_NAT_RULES_TOTAL=768 \
+	  CONFIG_LTQ_SVIP_NAT_UDP_PORT_BASE=50000
+endef
+
+define KernelPackage/lantiq-nat/description
+  Performs MAC and IP address translation of incoming and ougoing
+  IP packets relative the address mapping details provided by the
+  SVIP NAT rules. The packets will be intercept in the IP module and
+  when an appropriate NAT rule exists the source and destination address
+  details are replaced, and the packets are sent out the destined Ethernet
+  interface.
+endef
+
+$(eval $(call KernelPackage,lantiq-svip-nat))
diff --git a/target/linux/lantiq/patches-3.10/2200-add-svip-platform.patch b/target/linux/lantiq/patches-3.10/2200-add-svip-platform.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2200-add-svip-platform.patch
@@ -0,0 +1,2318 @@
+# HG changeset patch
+# Parent 131489e966dcfb49ecea78ecd320bb5fab17f94e
+add SVIP platform
+
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/boot_reg.h b/arch/mips/include/asm/mach-lantiq/svip/boot_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/boot_reg.h
+@@ -0,0 +1,37 @@
++/******************************************************************************
++
++	Copyright (c) 2012
++	Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __BOOT_REG_H
++#define __BOOT_REG_H
++
++#define LTQ_BOOT_CPU_OFFSET		0x20
++
++#define LTQ_BOOT_RVEC(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x00)
++#define LTQ_BOOT_NVEC(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x04)
++#define LTQ_BOOT_EVEC(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x08)
++#define LTQ_BOOT_CP0_STATUS(cpu)	(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x0C)
++#define LTQ_BOOT_CP0_EPC(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x10)
++#define LTQ_BOOT_CP0_EEPC(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x14)
++	/* only for CP1 */
++#define LTQ_BOOT_SIZE(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x18)
++	/* only for CP0 */
++#define LTQ_BOOT_RCU_SR(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x18)
++#define LTQ_BOOT_CFG_STAT(cpu)		(volatile u32*)(KSEG1 + \
++	LTQ_L2_SPRAM_BASE_ADDR + (cpu * LTQ_BOOT_CPU_OFFSET) + 0x1C)
++
++#endif
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h b/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h
+@@ -0,0 +1,342 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __EBU_REG_H
++#define __EBU_REG_H
++
++#define ebu_w32(x, y) 		ltq_w32((x), ltq_ebu_membase + (y))
++#define ebu_r32(x) 		ltq_r32(ltq_ebu_membase + (x))
++#define ebu_w32_mask(clear, set, reg)	\
++	ltq_w32_mask(clear, set, ltq_ebu_membase + (reg))
++
++#define ltq_ebu_w32(x, y) 		ebu_w32(x, y)
++#define ltq_ebu_r32(x) 		ebu_r32(x)
++#define ltq_ebu_w32_mask(clear, set, reg)	ebu_w32_mask(clear, set, reg)
++
++/** EBU register structure */
++struct svip_reg_ebu {
++	volatile unsigned long  clc;  /*  0x0000 */
++	volatile unsigned long  reserved0;  /*  0x04 */
++	volatile unsigned long  id;  /*  0x0008 */
++	volatile unsigned long  reserved1;  /*  0x0c */
++	volatile unsigned long  con;  /*  0x0010 */
++	volatile unsigned long  reserved2[3];  /*  0x14 */
++	volatile unsigned long  addr_sel_0;  /*  0x0020 */
++	volatile unsigned long  addr_sel_1;  /*  0x0024 */
++	volatile unsigned long  addr_sel_2;  /*  0x0028 */
++	volatile unsigned long  addr_sel_3;  /*  0x002c */
++	volatile unsigned long  reserved3[12];  /*  0x30 */
++	volatile unsigned long  con_0;  /*  0x0060 */
++	volatile unsigned long  con_1;  /*  0x0064 */
++	volatile unsigned long  con_2;  /*  0x0068 */
++	volatile unsigned long  con_3;  /*  0x006c */
++	volatile unsigned long  reserved4[16];  /*  0x70 */
++	volatile unsigned long  nand_con;  /*  0x00B0 */
++	volatile unsigned long  nand_wait;  /*  0x00B4 */
++	volatile unsigned long  nand_ecc0;  /*  0x00B8 */
++	volatile unsigned long  nand_ecc_ac;  /*  0x00BC */
++};
++
++/*******************************************************************************
++ * EBU
++ ******************************************************************************/
++#define LTQ_EBU_CLC   0x0000
++#define LTQ_EBU_ID   0x0008
++#define LTQ_EBU_CON   0x0010
++#define LTQ_EBU_ADDR_SEL_0   0x0020
++#define LTQ_EBU_ADDR_SEL_1   0x0024
++#define LTQ_EBU_ADDR_SEL_2   0x0028
++#define LTQ_EBU_ADDR_SEL_3   0x002c
++#define LTQ_EBU_CON_0   0x0060
++#define LTQ_EBU_CON_1   0x0064
++#define LTQ_EBU_CON_2   0x0068
++#define LTQ_EBU_CON_3   0x006c
++#define LTQ_EBU_NAND_CON   0x00B0
++#define LTQ_EBU_NAND_WAIT   0x00B4
++#define LTQ_EBU_NAND_ECC0   0x00B8
++#define LTQ_EBU_NAND_ECC_AC   0x00BC
++
++/*******************************************************************************
++ * EBU Clock Control Register
++ ******************************************************************************/
++
++/* EBU Disable Status Bit (1) */
++#define LTQ_EBU_CLC_DISS   (0x1 << 1)
++#define LTQ_EBU_CLC_DISS_GET(val)   ((((val) & LTQ_EBU_CLC_DISS) >> 1) & 0x1)
++/* Used for Enable/disable Control of the EBU (0) */
++#define LTQ_EBU_CLC_DISR   (0x1)
++#define LTQ_EBU_CLC_DISR_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_EBU_CLC_DISR_GET(val)   ((((val) & LTQ_EBU_CLC_DISR) >> 0) & 0x1)
++#define LTQ_EBU_CLC_DISR_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CLC_DISR) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * EBU Identification Register (Internal)
++ ******************************************************************************/
++
++/* Module Number (31:8) */
++#define LTQ_EBU_ID_MODNUM   (0xffffff << 8)
++#define LTQ_EBU_ID_MODNUM_GET(val)   ((((val) & LTQ_EBU_ID_MODNUM) >> 8) & 0xffffff)
++/* Revision Number (7:0) */
++#define LTQ_EBU_ID_REVNUM   (0xff)
++#define LTQ_EBU_ID_REVNUM_GET(val)   ((((val) & LTQ_EBU_ID_REVNUM) >> 0) & 0xff)
++
++/*******************************************************************************
++ * External Bus Unit Control Register
++ ******************************************************************************/
++
++/* Driver Turn-Around Control, Chip Select Triggered (22:20) */
++#define LTQ_EBU_CON_DTACS   (0x7 << 20)
++#define LTQ_EBU_CON_DTACS_VAL(val)   (((val) & 0x7) << 20)
++#define LTQ_EBU_CON_DTACS_GET(val)   ((((val) & LTQ_EBU_CON_DTACS) >> 20) & 0x7)
++#define LTQ_EBU_CON_DTACS_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_DTACS) | (((val) & 0x7) << 20))
++/* Driver Turn-Around Control, Read-write Triggered (18:16) */
++#define LTQ_EBU_CON_DTARW   (0x7 << 16)
++#define LTQ_EBU_CON_DTARW_VAL(val)   (((val) & 0x7) << 16)
++#define LTQ_EBU_CON_DTARW_GET(val)   ((((val) & LTQ_EBU_CON_DTARW) >> 16) & 0x7)
++#define LTQ_EBU_CON_DTARW_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_DTARW) | (((val) & 0x7) << 16))
++/* Time-Out Control (15:8) */
++#define LTQ_EBU_CON_TOUTC   (0xff << 8)
++#define LTQ_EBU_CON_TOUTC_VAL(val)   (((val) & 0xff) << 8)
++#define LTQ_EBU_CON_TOUTC_GET(val)   ((((val) & LTQ_EBU_CON_TOUTC) >> 8) & 0xff)
++#define LTQ_EBU_CON_TOUTC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_TOUTC) | (((val) & 0xff) << 8))
++/* Arbitration Mode (7:6) */
++#define LTQ_EBU_CON_ARBMODE   (0x3 << 6)
++#define LTQ_EBU_CON_ARBMODE_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_EBU_CON_ARBMODE_GET(val)   ((((val) & LTQ_EBU_CON_ARBMODE) >> 6) & 0x3)
++#define LTQ_EBU_CON_ARBMODE_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_ARBMODE) | (((val) & 0x3) << 6))
++/* Arbitration Synchronization (5) */
++#define LTQ_EBU_CON_ARBSYNC   (0x1 << 5)
++#define LTQ_EBU_CON_ARBSYNC_VAL(val)   (((val) & 0x1) << 5)
++#define LTQ_EBU_CON_ARBSYNC_GET(val)   ((((val) & LTQ_EBU_CON_ARBSYNC) >> 5) & 0x1)
++#define LTQ_EBU_CON_ARBSYNC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_ARBSYNC) | (((val) & 0x1) << 5))
++
++/*******************************************************************************
++ * Address Select Registers
++ ******************************************************************************/
++
++/* Memory Region Base Address (31:12) */
++#define LTQ_EBU_ADDR_SEL_0_BASE   (0xfffff << 12)
++#define LTQ_EBU_ADDR_SEL_0_BASE_VAL(val)   (((val) & 0xfffff) << 12)
++#define LTQ_EBU_ADDR_SEL_0_BASE_GET(val)   ((((val) & LTQ_EBU_ADDR_SEL_0_BASE) >> 12) & 0xfffff)
++#define LTQ_EBU_ADDR_SEL_0_BASE_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_ADDR_SEL_0_BASE) | (((val) & 0xfffff) << 12))
++/* Memory Region Address Mask (7:4) */
++#define LTQ_EBU_ADDR_SEL_0_MASK   (0xf << 4)
++#define LTQ_EBU_ADDR_SEL_0_MASK_VAL(val)   (((val) & 0xf) << 4)
++#define LTQ_EBU_ADDR_SEL_0_MASK_GET(val)   ((((val) & LTQ_EBU_ADDR_SEL_0_MASK) >> 4) & 0xf)
++#define LTQ_EBU_ADDR_SEL_0_MASK_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_ADDR_SEL_0_MASK) | (((val) & 0xf) << 4))
++/* Memory Region Mirror Enable Control (1) */
++#define LTQ_EBU_ADDR_SEL_0_MRME   (0x1 << 1)
++#define LTQ_EBU_ADDR_SEL_0_MRME_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_EBU_ADDR_SEL_0_MRME_GET(val)   ((((val) & LTQ_EBU_ADDR_SEL_0_MRME) >> 1) & 0x1)
++#define LTQ_EBU_ADDR_SEL_0_MRME_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_ADDR_SEL_0_MRME) | (((val) & 0x1) << 1))
++/* Memory Region Enable Control (0) */
++#define LTQ_EBU_ADDR_SEL_0_REGEN   (0x1)
++#define LTQ_EBU_ADDR_SEL_0_REGEN_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_EBU_ADDR_SEL_0_REGEN_GET(val)   ((((val) & LTQ_EBU_ADDR_SEL_0_REGEN) >> 0) & 0x1)
++#define LTQ_EBU_ADDR_SEL_0_REGEN_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_ADDR_SEL_0_REGEN) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * Bus Configuration Registers
++ ******************************************************************************/
++
++/* Memory Region Write Protection (31) */
++#define LTQ_EBU_CON_0_WRDIS   (0x1 << 31)
++#define LTQ_EBU_CON_0_WRDIS_VAL(val)   (((val) & 0x1) << 31)
++#define LTQ_EBU_CON_0_WRDIS_GET(val)   ((((val) & LTQ_EBU_CON_0_WRDIS) >> 31) & 0x1)
++#define LTQ_EBU_CON_0_WRDIS_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_WRDIS) | (((val) & 0x1) << 31))
++/* Address Swapping (30) */
++#define LTQ_EBU_CON_0_ADSWP   (0x1 << 30)
++#define LTQ_EBU_CON_0_ADSWP_VAL(val)   (((val) & 0x1) << 30)
++#define LTQ_EBU_CON_0_ADSWP_GET(val)   ((((val) & LTQ_EBU_CON_0_ADSWP) >> 30) & 0x1)
++#define LTQ_EBU_CON_0_ADSWP_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_ADSWP) | (((val) & 0x1) << 30))
++/* Address Generation Control (26:24) */
++#define LTQ_EBU_CON_0_AGEN   (0x7 << 24)
++#define LTQ_EBU_CON_0_AGEN_VAL(val)   (((val) & 0x7) << 24)
++#define LTQ_EBU_CON_0_AGEN_GET(val)   ((((val) & LTQ_EBU_CON_0_AGEN) >> 24) & 0x7)
++#define LTQ_EBU_CON_0_AGEN_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_AGEN) | (((val) & 0x7) << 24))
++/* Extended Address Setup Control (22) */
++#define LTQ_EBU_CON_0_SETUP   (0x1 << 22)
++#define LTQ_EBU_CON_0_SETUP_VAL(val)   (((val) & 0x1) << 22)
++#define LTQ_EBU_CON_0_SETUP_GET(val)   ((((val) & LTQ_EBU_CON_0_SETUP) >> 22) & 0x1)
++#define LTQ_EBU_CON_0_SETUP_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_SETUP) | (((val) & 0x1) << 22))
++/* Variable Wait-State Insertion Control (21:20) */
++#define LTQ_EBU_CON_0_WAIT   (0x3 << 20)
++#define LTQ_EBU_CON_0_WAIT_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_EBU_CON_0_WAIT_GET(val)   ((((val) & LTQ_EBU_CON_0_WAIT) >> 20) & 0x3)
++#define LTQ_EBU_CON_0_WAIT_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_WAIT) | (((val) & 0x3) << 20))
++/* Active WAIT Level Control (19) */
++#define LTQ_EBU_CON_0_WINV   (0x1 << 19)
++#define LTQ_EBU_CON_0_WINV_VAL(val)   (((val) & 0x1) << 19)
++#define LTQ_EBU_CON_0_WINV_GET(val)   ((((val) & LTQ_EBU_CON_0_WINV) >> 19) & 0x1)
++#define LTQ_EBU_CON_0_WINV_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_WINV) | (((val) & 0x1) << 19))
++/* External Device Data Width Control (17:16) */
++#define LTQ_EBU_CON_0_PW   (0x3 << 16)
++#define LTQ_EBU_CON_0_PW_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_EBU_CON_0_PW_GET(val)   ((((val) & LTQ_EBU_CON_0_PW) >> 16) & 0x3)
++#define LTQ_EBU_CON_0_PW_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_PW) | (((val) & 0x3) << 16))
++/* Address Latch Enable ALE Duration Control (15:14) */
++#define LTQ_EBU_CON_0_ALEC   (0x3 << 14)
++#define LTQ_EBU_CON_0_ALEC_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_EBU_CON_0_ALEC_GET(val)   ((((val) & LTQ_EBU_CON_0_ALEC) >> 14) & 0x3)
++#define LTQ_EBU_CON_0_ALEC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_ALEC) | (((val) & 0x3) << 14))
++/* Byte Control Signal Timing Mode Control (13:12) */
++#define LTQ_EBU_CON_0_BCGEN   (0x3 << 12)
++#define LTQ_EBU_CON_0_BCGEN_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_EBU_CON_0_BCGEN_GET(val)   ((((val) & LTQ_EBU_CON_0_BCGEN) >> 12) & 0x3)
++#define LTQ_EBU_CON_0_BCGEN_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_BCGEN) | (((val) & 0x3) << 12))
++/* Write Access Wait-State Control (10:8) */
++#define LTQ_EBU_CON_0_WAITWRC   (0x7 << 8)
++#define LTQ_EBU_CON_0_WAITWRC_VAL(val)   (((val) & 0x7) << 8)
++#define LTQ_EBU_CON_0_WAITWRC_GET(val)   ((((val) & LTQ_EBU_CON_0_WAITWRC) >> 8) & 0x7)
++#define LTQ_EBU_CON_0_WAITWRC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_WAITWRC) | (((val) & 0x7) << 8))
++/* Read Access Wait-State Control (7:6) */
++#define LTQ_EBU_CON_0_WAITRDC   (0x3 << 6)
++#define LTQ_EBU_CON_0_WAITRDC_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_EBU_CON_0_WAITRDC_GET(val)   ((((val) & LTQ_EBU_CON_0_WAITRDC) >> 6) & 0x3)
++#define LTQ_EBU_CON_0_WAITRDC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_WAITRDC) | (((val) & 0x3) << 6))
++/* Hold/Pause Cycle Control (5:4) */
++#define LTQ_EBU_CON_0_HOLDC   (0x3 << 4)
++#define LTQ_EBU_CON_0_HOLDC_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_EBU_CON_0_HOLDC_GET(val)   ((((val) & LTQ_EBU_CON_0_HOLDC) >> 4) & 0x3)
++#define LTQ_EBU_CON_0_HOLDC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_HOLDC) | (((val) & 0x3) << 4))
++/* Recovery Cycle Control (3:2) */
++#define LTQ_EBU_CON_0_RECOVC   (0x3 << 2)
++#define LTQ_EBU_CON_0_RECOVC_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_EBU_CON_0_RECOVC_GET(val)   ((((val) & LTQ_EBU_CON_0_RECOVC) >> 2) & 0x3)
++#define LTQ_EBU_CON_0_RECOVC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_RECOVC) | (((val) & 0x3) << 2))
++/* Wait Cycle Multiplier Control (1:0) */
++#define LTQ_EBU_CON_0_CMULT   (0x3)
++#define LTQ_EBU_CON_0_CMULT_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_EBU_CON_0_CMULT_GET(val)   ((((val) & LTQ_EBU_CON_0_CMULT) >> 0) & 0x3)
++#define LTQ_EBU_CON_0_CMULT_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_CON_0_CMULT) | (((val) & 0x3) << 0))
++
++
++/*******************************************************************************
++ * NAND Flash Control Register
++ ******************************************************************************/
++
++/* ECC Enabling (31) */
++#define LTQ_EBU_NAND_CON_ECC_ON   (0x1 << 31)
++#define LTQ_EBU_NAND_CON_ECC_ON_VAL(val)   (((val) & 0x1) << 31)
++#define LTQ_EBU_NAND_CON_ECC_ON_GET(val)   ((((val) & LTQ_EBU_NAND_CON_ECC_ON) >> 31) & 0x1)
++#define LTQ_EBU_NAND_CON_ECC_ON_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_ECC_ON) | (((val) & 0x1) << 31))
++/* Latch enable (23:18) */
++#define LTQ_EBU_NAND_CON_LAT_EN   (0x3f << 18)
++#define LTQ_EBU_NAND_CON_LAT_EN_VAL(val)   (((val) & 0x3f) << 18)
++#define LTQ_EBU_NAND_CON_LAT_EN_GET(val)   ((((val) & LTQ_EBU_NAND_CON_LAT_EN) >> 18) & 0x3f)
++#define LTQ_EBU_NAND_CON_LAT_EN_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_LAT_EN) | (((val) & 0x3f) << 18))
++/* Output ChipSelect# Selection (11:10) */
++#define LTQ_EBU_NAND_CON_OUT_CS_S   (0x3 << 10)
++#define LTQ_EBU_NAND_CON_OUT_CS_S_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_EBU_NAND_CON_OUT_CS_S_GET(val)   ((((val) & LTQ_EBU_NAND_CON_OUT_CS_S) >> 10) & 0x3)
++#define LTQ_EBU_NAND_CON_OUT_CS_S_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_OUT_CS_S) | (((val) & 0x3) << 10))
++/* Input ChipSelect# Selection (9:8) */
++#define LTQ_EBU_NAND_CON_IN_CS_S   (0x3 << 8)
++#define LTQ_EBU_NAND_CON_IN_CS_S_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_EBU_NAND_CON_IN_CS_S_GET(val)   ((((val) & LTQ_EBU_NAND_CON_IN_CS_S) >> 8) & 0x3)
++#define LTQ_EBU_NAND_CON_IN_CS_S_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_IN_CS_S) | (((val) & 0x3) << 8))
++/* Set PRE (7) */
++#define LTQ_EBU_NAND_CON_PRE_P   (0x1 << 7)
++#define LTQ_EBU_NAND_CON_PRE_P_VAL(val)   (((val) & 0x1) << 7)
++#define LTQ_EBU_NAND_CON_PRE_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_PRE_P) >> 7) & 0x1)
++#define LTQ_EBU_NAND_CON_PRE_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_PRE_P) | (((val) & 0x1) << 7))
++/* Set WP Active Polarity (6) */
++#define LTQ_EBU_NAND_CON_WP_P   (0x1 << 6)
++#define LTQ_EBU_NAND_CON_WP_P_VAL(val)   (((val) & 0x1) << 6)
++#define LTQ_EBU_NAND_CON_WP_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_WP_P) >> 6) & 0x1)
++#define LTQ_EBU_NAND_CON_WP_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_WP_P) | (((val) & 0x1) << 6))
++/* Set SE Active Polarity (5) */
++#define LTQ_EBU_NAND_CON_SE_P   (0x1 << 5)
++#define LTQ_EBU_NAND_CON_SE_P_VAL(val)   (((val) & 0x1) << 5)
++#define LTQ_EBU_NAND_CON_SE_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_SE_P) >> 5) & 0x1)
++#define LTQ_EBU_NAND_CON_SE_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_SE_P) | (((val) & 0x1) << 5))
++/* Set CS Active Polarity (4) */
++#define LTQ_EBU_NAND_CON_CS_P   (0x1 << 4)
++#define LTQ_EBU_NAND_CON_CS_P_VAL(val)   (((val) & 0x1) << 4)
++#define LTQ_EBU_NAND_CON_CS_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_CS_P) >> 4) & 0x1)
++#define LTQ_EBU_NAND_CON_CS_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_CS_P) | (((val) & 0x1) << 4))
++/* Set CLE Active Polarity (3) */
++#define LTQ_EBU_NAND_CON_CLE_P   (0x1 << 3)
++#define LTQ_EBU_NAND_CON_CLE_P_VAL(val)   (((val) & 0x1) << 3)
++#define LTQ_EBU_NAND_CON_CLE_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_CLE_P) >> 3) & 0x1)
++#define LTQ_EBU_NAND_CON_CLE_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_CLE_P) | (((val) & 0x1) << 3))
++/* Set ALE Active Polarity (2) */
++#define LTQ_EBU_NAND_CON_ALE_P   (0x1 << 2)
++#define LTQ_EBU_NAND_CON_ALE_P_VAL(val)   (((val) & 0x1) << 2)
++#define LTQ_EBU_NAND_CON_ALE_P_GET(val)   ((((val) & LTQ_EBU_NAND_CON_ALE_P) >> 2) & 0x1)
++#define LTQ_EBU_NAND_CON_ALE_P_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_ALE_P) | (((val) & 0x1) << 2))
++/* NAND CS Mux with EBU CS Enable (1) */
++#define LTQ_EBU_NAND_CON_CSMUX_E   (0x1 << 1)
++#define LTQ_EBU_NAND_CON_CSMUX_E_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_EBU_NAND_CON_CSMUX_E_GET(val)   ((((val) & LTQ_EBU_NAND_CON_CSMUX_E) >> 1) & 0x1)
++#define LTQ_EBU_NAND_CON_CSMUX_E_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_CSMUX_E) | (((val) & 0x1) << 1))
++/* NAND FLASH Mode Support (0) */
++#define LTQ_EBU_NAND_CON_NANDMODE   (0x1)
++#define LTQ_EBU_NAND_CON_NANDMODE_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_EBU_NAND_CON_NANDMODE_GET(val)   ((((val) & LTQ_EBU_NAND_CON_NANDMODE) >> 0) & 0x1)
++#define LTQ_EBU_NAND_CON_NANDMODE_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_CON_NANDMODE) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * NAND Flash State Register
++ ******************************************************************************/
++
++/* Reserved (31:3) */
++#define LTQ_EBU_NAND_WAIT_RES   (0x1fffffff << 3)
++#define LTQ_EBU_NAND_WAIT_RES_GET(val)   ((((val) & LTQ_EBU_NAND_WAIT_RES) >> 3) & 0x1fffffff)
++/* NAND Write Complete (3) */
++#define LTQ_EBU_NAND_WAIT_WR_C   (0x1 << 3)
++#define LTQ_EBU_NAND_WAIT_WR_C_GET(val)   ((((val) & LTQ_EBU_NAND_WAIT_WR_C) >> 3) & 0x1)
++/* Record the RD Edge (rising ) (2) */
++#define LTQ_EBU_NAND_WAIT_RD_EDGE   (0x1 << 2)
++#define LTQ_EBU_NAND_WAIT_RD_EDGE_GET(val)   ((((val) & LTQ_EBU_NAND_WAIT_RD_EDGE) >> 2) & 0x1)
++/* Record the BY# Edge (falling) (1) */
++#define LTQ_EBU_NAND_WAIT_BY_EDGE   (0x1 << 1)
++#define LTQ_EBU_NAND_WAIT_BY_EDGE_GET(val)   ((((val) & LTQ_EBU_NAND_WAIT_BY_EDGE) >> 1) & 0x1)
++/* Rd/BY# value (0) */
++#define LTQ_EBU_NAND_WAIT_RDBY_VALUE   (0x1)
++#define LTQ_EBU_NAND_WAIT_RDBY_VALUE_GET(val)   ((((val) & LTQ_EBU_NAND_WAIT_RDBY_VALUE) >> 0) & 0x1)
++
++/*******************************************************************************
++ * NAND ECC Result Register 0
++ ******************************************************************************/
++
++/* Reserved (31:24) */
++#define LTQ_EBU_NAND_ECC0_RES   (0xff << 24)
++#define LTQ_EBU_NAND_ECC0_RES_GET(val)   ((((val) & LTQ_EBU_NAND_ECC0_RES) >> 24) & 0xff)
++/* ECC value (23:16) */
++#define LTQ_EBU_NAND_ECC0_ECC_B2   (0xff << 16)
++#define LTQ_EBU_NAND_ECC0_ECC_B2_VAL(val)   (((val) & 0xff) << 16)
++#define LTQ_EBU_NAND_ECC0_ECC_B2_GET(val)   ((((val) & LTQ_EBU_NAND_ECC0_ECC_B2) >> 16) & 0xff)
++#define LTQ_EBU_NAND_ECC0_ECC_B2_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_ECC0_ECC_B2) | (((val) & 0xff) << 16))
++/* ECC value (15:8) */
++#define LTQ_EBU_NAND_ECC0_ECC_B1   (0xff << 8)
++#define LTQ_EBU_NAND_ECC0_ECC_B1_VAL(val)   (((val) & 0xff) << 8)
++#define LTQ_EBU_NAND_ECC0_ECC_B1_GET(val)   ((((val) & LTQ_EBU_NAND_ECC0_ECC_B1) >> 8) & 0xff)
++#define LTQ_EBU_NAND_ECC0_ECC_B1_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_ECC0_ECC_B1) | (((val) & 0xff) << 8))
++/* ECC value (7:0) */
++#define LTQ_EBU_NAND_ECC0_ECC_B0   (0xff)
++#define LTQ_EBU_NAND_ECC0_ECC_B0_VAL(val)   (((val) & 0xff) << 0)
++#define LTQ_EBU_NAND_ECC0_ECC_B0_GET(val)   ((((val) & LTQ_EBU_NAND_ECC0_ECC_B0) >> 0) & 0xff)
++#define LTQ_EBU_NAND_ECC0_ECC_B0_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_ECC0_ECC_B0) | (((val) & 0xff) << 0))
++
++/*******************************************************************************
++ * NAND ECC Address Counter Register
++ ******************************************************************************/
++
++/* Reserved (31:9) */
++#define LTQ_EBU_NAND_ECC_AC_RES   (0x7fffff << 9)
++#define LTQ_EBU_NAND_ECC_AC_RES_GET(val)   ((((val) & LTQ_EBU_NAND_ECC_AC_RES) >> 9) & 0x7fffff)
++/* ECC address counter (8:0) */
++#define LTQ_EBU_NAND_ECC_AC_ECC_AC   (0x1ff)
++#define LTQ_EBU_NAND_ECC_AC_ECC_AC_VAL(val)   (((val) & 0x1ff) << 0)
++#define LTQ_EBU_NAND_ECC_AC_ECC_AC_GET(val)   ((((val) & LTQ_EBU_NAND_ECC_AC_ECC_AC) >> 0) & 0x1ff)
++#define LTQ_EBU_NAND_ECC_AC_ECC_AC_SET(reg,val) (reg) = ((reg & ~LTQ_EBU_NAND_ECC_AC_ECC_AC) | (((val) & 0x1ff) << 0))
++
++#endif /* __LTQ_EBU_H */
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/irq.h b/arch/mips/include/asm/mach-lantiq/svip/irq.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/irq.h
+@@ -0,0 +1,26 @@
++/*
++ *  This program is free software; you can redistribute it and/or modify it
++ *  under the terms of the GNU General Public License version 2 as published
++ *  by the Free Software Foundation.
++ *
++ *  Copyright (C) 2010 John Crispin <blogic@openwrt.org>
++ */
++
++#ifndef _LANTIQ_LTQ_IRQ_H__
++#define _LANTIQ_LTQ_IRQ_H__
++
++#define NR_IRQS 264
++#define MIPS_CPU_IRQ_BASE 0
++#define INT_NUM_IRQ0		8
++#define INT_NUM_IM0_IRL0	(INT_NUM_IRQ0 + 0)
++#define INT_NUM_IM1_IRL0	(INT_NUM_IRQ0 + 32)
++#define INT_NUM_IM2_IRL0	(INT_NUM_IRQ0 + 64)
++#define INT_NUM_IM3_IRL0	(INT_NUM_IRQ0 + 96)
++#define INT_NUM_IM4_IRL0	(INT_NUM_IRQ0 + 128)
++#define INT_NUM_IM5_IRL0	(INT_NUM_IRQ0 + 160)
++#define INT_NUM_IM_OFFSET	(INT_NUM_IM1_IRL0 - INT_NUM_IM0_IRL0)
++#define LTQ_DMA_CH0_INT		(INT_NUM_IM2_IRL0)
++#define MIPS_CPU_TIMER_IRQ	(INT_NUM_IM5_IRL0 + 2) 
++#define MAX_IM			6
++
++#endif /* _LANTIQ_LTQ_IRQ_H__ */
+\ No newline at end of file
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+@@ -0,0 +1,112 @@
++/*
++ * This program is free software; you can redistribute it and/or modify it
++ * under the terms of the GNU General Public License version 2 as published
++ * by the Free Software Foundation.
++ *
++ * Copyright (C) 2010 John Crispin <blogic@openwrt.org>
++ */
++
++#ifndef _LTQ_SVIP_H__
++#define _LTQ_SVIP_H__
++
++#ifdef CONFIG_SOC_SVIP
++
++#include <linux/pinctrl/pinctrl.h>
++#include <lantiq.h>
++
++/* Chip IDs */
++#define SOC_ID_SVIP		0x169
++
++/* SoC Types */
++#define SOC_TYPE_SVIP		0x01
++
++#define LTQ_EBU_BASE				0x14102000
++#define LTQ_ASC0_BASE_ADDR			0x14100100
++#define LTQ_ASC1_BASE_ADDR			0x14100200
++#define LTQ_STATUS_BASE_ADDR			0x1E000500
++#define LTQ_SYS1_BASE_ADDR			0x1C000800 
++
++#define SYS0_PLL1CR	0x0008
++#define SYS0_PLL1CR_PLLDIV   (0x3)
++#define SYS0_PLL1CR_PLLDIV_GET(val)   ((((val) & SYS0_PLL1CR_PLLDIV) >> 0) & 0x3) 
++
++#define SYS1_CLKSR	0x0000
++#define SYS1_FPICR	0x0014
++#define SYS1_CLKENR	0x0004
++#define SYS1_CLKCLR	0x0008
++#define SYS1_CLKENR_ASC0 (0x1 << 14)
++#define SYS1_CLKENR_ASC1 (0x1 << 15)
++#define SYS1_FPICR_FPIDIV   (0x1)
++#define SYS1_FPICR_FPIDIV_GET(val)   ((((val) & SYS1_FPICR_FPIDIV) >> 0) & 0x1) 
++
++#define SYS2_CLKSR	0x0000
++#define SYS2_CLKENR	0x0004
++#define SYS2_CLKCLR	0x0008 
++
++#define STATUS_FUSE_DEU		0x0000
++#define STATUS_FUSE_CPU		0x0004
++#define STATUS_FUSE_PLL		0x0008
++#define STATUS_CHIPID		0x000C
++#define STATUS_CONFIG		0x0010
++#define STATUS_CHIP_LOC		0x0014
++#define STATUS_FUSE_SPARE	0x0018 
++
++#define SVIP_FUSE		((u32 *)(KSEG1 + LTQ_STATUS_BASE_ADDR + STATUS_FUSE_DEU))
++#define SVIP_CHIPID		((u32 *)(KSEG1 + LTQ_STATUS_BASE_ADDR + STATUS_CHIPID))
++#define SVIP_CHIPTYPE		((u32 *)(KSEG1 + LTQ_STATUS_BASE_ADDR + STATUS_FUSE_DEU))
++#define SVIP_CHIPCONF		((u32 *)(KSEG1 + LTQ_STATUS_BASE_ADDR + STATUS_CONFIG))
++
++#define STATUS_CONFIG		0x0010
++#define STATUS_CHIPID_PART_NUMBER   (0xffff << 12)
++#define STATUS_CHIPID_PART_NUMBER_GET(val)   ((((val) & STATUS_CHIPID_PART_NUMBER) >> 12) & 0xffff)
++#define STATUS_CHIPID_VERSION   (0xf << 28)
++#define STATUS_CHIPID_VERSION_GET(val)   ((((val) & STATUS_CHIPID_VERSION) >> 28) & 0xf)
++#define STATUS_FUSE_DEU_DEU   (0x1)
++#define STATUS_FUSE_DEU_DEU_GET(val)   ((((val) & STATUS_FUSE_DEU_DEU) >> 0) & 0x1)
++#define STATUS_CONFIG_CLK_MODE   (0x1 << 1)
++#define STATUS_CONFIG_CLK_MODE_GET(val)   ((((val) & STATUS_CONFIG_CLK_MODE) >> 4) & 0x1) 
++
++#define LTQ_EBU_PCC_ISTAT   (LTQ_EBU_BASE + 0x00A0)
++
++/*
++ * during early_printk no ioremap possible at this early stage
++ * lets use KSEG1 instead
++ */
++
++#define LTQ_ASC_SIZE		0x100
++#define LTQ_EARLY_ASC		KSEG1ADDR(LTQ_ASC0_BASE_ADDR)
++
++/* WDT */
++#define LTQ_RST_CAUSE_WDTRST	0x0002
++
++/* SYSCTL - start/stop/restart/configure/... different parts of the Soc */
++#define SYSCTL_SYS0		0
++#define SYSCTL_SYS1		1
++#define SYSCTL_SYS2		2
++#define SYSCTL_NUM_OF_SYS	3
++
++extern void ltq_sysctl_activate(int module, unsigned int mask);
++extern void ltq_sysctl_deactivate(int module, unsigned int mask);
++extern void ltq_sysctl_clken(int module, unsigned int mask);
++extern void ltq_sysctl_clkdis(int module, unsigned int mask);
++extern void ltq_sysctl_reboot(int module, unsigned int mask);
++
++/* BOOT_SEL - find what boot media we have */
++
++#define BS_FLASH		0x1
++/*#define BS_SPI			0x4*/
++
++
++/* global register ranges */
++extern __iomem void *ltq_ebu_membase;
++extern __iomem void *ltq_sys1_membase;
++#define ltq_ebu_w32(x, y)	ltq_w32((x), ltq_ebu_membase + (y))
++#define ltq_ebu_r32(x)		ltq_r32(ltq_ebu_membase + (x))
++
++#define ltq_sys1_w32(x, y)	ltq_w32((x), ltq_sys1_membase + (y))
++#define ltq_sys1_r32(x)		ltq_r32(ltq_sys1_membase + (x))
++#define ltq_sys1_w32_mask(clear, set, reg)   \
++	ltq_sys1_w32((ltq_sys1_r32(reg) & ~(clear)) | (set), reg)
++
++#endif /* CONFIG_SOC_SVIP */
++#endif /* _LTQ_SVIP_H__ */
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/mps_reg.h b/arch/mips/include/asm/mach-lantiq/svip/mps_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/mps_reg.h
+@@ -0,0 +1,240 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __MPS_REG_H
++#define __MPS_REG_H
++
++#define mbs_r32(reg) ltq_r32(&mbs->reg)
++#define mbs_w32(val, reg) ltq_w32(val, &mbs->reg)
++#define mbs_w32_mask(clear, set, reg) ltq_w32_mask(clear, set, &mbs->reg)
++
++/** MBS register structure */
++struct svip_reg_mbs {
++	unsigned long reserved0[4];
++	unsigned long mbsr0; /* 0x0010 */
++	unsigned long mbsr1; /* 0x0014 */
++	unsigned long mbsr2; /* 0x0018 */
++	unsigned long mbsr3; /* 0x001c */
++	unsigned long mbsr4; /* 0x0020 */
++	unsigned long mbsr5; /* 0x0024 */
++	unsigned long mbsr6; /* 0x0028 */
++	unsigned long mbsr7; /* 0x002c */
++	unsigned long mbsr8; /* 0x0030 */
++	unsigned long mbsr9; /* 0x0034 */
++	unsigned long mbsr10; /* 0x0038 */
++	unsigned long mbsr11; /* 0x003c */
++	unsigned long mbsr12; /* 0x0040 */
++	unsigned long mbsr13; /* 0x0044 */
++	unsigned long mbsr14; /* 0x0048 */
++	unsigned long mbsr15; /* 0x004c */
++	unsigned long mbsr16; /* 0x0050 */
++	unsigned long mbsr17; /* 0x0054 */
++	unsigned long mbsr18; /* 0x0058 */
++	unsigned long mbsr19; /* 0x005c */
++	unsigned long mbsr20; /* 0x0060 */
++	unsigned long mbsr21; /* 0x0064 */
++	unsigned long mbsr22; /* 0x0068 */
++	unsigned long mbsr23; /* 0x006c */
++	unsigned long mbsr24; /* 0x0070 */
++	unsigned long mbsr25; /* 0x0074 */
++	unsigned long mbsr26; /* 0x0078 */
++	unsigned long mbsr27; /* 0x007c */
++	unsigned long mbsr28; /* 0x0080 */
++};
++
++/** MPS register structure */
++struct svip_reg_mps {
++	volatile unsigned long  mps_swirn0set;  /*  0x0000 */
++	volatile unsigned long  mps_swirn0en;  /*  0x0004 */
++	volatile unsigned long  mps_swirn0cr;  /*  0x0008 */
++	volatile unsigned long  mps_swirn0icr;  /*  0x000C */
++	volatile unsigned long  mps_swirn1set;  /*  0x0010 */
++	volatile unsigned long  mps_swirn1en;  /*  0x0014 */
++	volatile unsigned long  mps_swirn1cr;  /*  0x0018 */
++	volatile unsigned long  mps_swirn1icr;  /*  0x001C */
++	volatile unsigned long  mps_swirn2set;  /*  0x0020 */
++	volatile unsigned long  mps_swirn2en;  /*  0x0024 */
++	volatile unsigned long  mps_swirn2cr;  /*  0x0028 */
++	volatile unsigned long  mps_swirn2icr;  /*  0x002C */
++	volatile unsigned long  mps_swirn3set;  /*  0x0030 */
++	volatile unsigned long  mps_swirn3en;  /*  0x0034 */
++	volatile unsigned long  mps_swirn3cr;  /*  0x0038 */
++	volatile unsigned long  mps_swirn3icr;  /*  0x003C */
++	volatile unsigned long  mps_swirn4set;  /*  0x0040 */
++	volatile unsigned long  mps_swirn4en;  /*  0x0044 */
++	volatile unsigned long  mps_swirn4cr;  /*  0x0048 */
++	volatile unsigned long  mps_swirn4icr;  /*  0x004C */
++	volatile unsigned long  mps_swirn5set;  /*  0x0050 */
++	volatile unsigned long  mps_swirn5en;  /*  0x0054 */
++	volatile unsigned long  mps_swirn5cr;  /*  0x0058 */
++	volatile unsigned long  mps_swirn5icr;  /*  0x005C */
++	volatile unsigned long  mps_swirn6set;  /*  0x0060 */
++	volatile unsigned long  mps_swirn6en;  /*  0x0064 */
++	volatile unsigned long  mps_swirn6cr;  /*  0x0068 */
++	volatile unsigned long  mps_swirn6icr;  /*  0x006C */
++	volatile unsigned long  mps_swirn7set;  /*  0x0070 */
++	volatile unsigned long  mps_swirn7en;  /*  0x0074 */
++	volatile unsigned long  mps_swirn7cr;  /*  0x0078 */
++	volatile unsigned long  mps_swirn7icr;  /*  0x007C */
++	volatile unsigned long  mps_swirn8set;  /*  0x0080 */
++	volatile unsigned long  mps_swirn8en;  /*  0x0084 */
++	volatile unsigned long  mps_swirn8cr;  /*  0x0088 */
++	volatile unsigned long  mps_swirn8icr;  /*  0x008C */
++};
++
++/* Software Interrupt */
++#define IFX_MPS_SWIRN0SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0000))
++#define IFX_MPS_SWIRN0EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0004))
++#define IFX_MPS_SWIRN0CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0008))
++#define IFX_MPS_SWIRN0ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x000C))
++#define IFX_MPS_SWIRN1SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0010))
++#define IFX_MPS_SWIRN1EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0014))
++#define IFX_MPS_SWIRN1CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0018))
++#define IFX_MPS_SWIRN1ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x001C))
++#define IFX_MPS_SWIRN2SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0020))
++#define IFX_MPS_SWIRN2EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0024))
++#define IFX_MPS_SWIRN2CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0028))
++#define IFX_MPS_SWIRN2ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x002C))
++#define IFX_MPS_SWIRN3SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0030))
++#define IFX_MPS_SWIRN3EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0034))
++#define IFX_MPS_SWIRN3CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0038))
++#define IFX_MPS_SWIRN3ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x003C))
++#define IFX_MPS_SWIRN4SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0040))
++#define IFX_MPS_SWIRN4EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0044))
++#define IFX_MPS_SWIRN4CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0048))
++#define IFX_MPS_SWIRN4ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x004C))
++#define IFX_MPS_SWIRN5SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0050))
++#define IFX_MPS_SWIRN5EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0054))
++#define IFX_MPS_SWIRN5CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0058))
++#define IFX_MPS_SWIRN5ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x005C))
++#define IFX_MPS_SWIRN6SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0060))
++#define IFX_MPS_SWIRN6EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0064))
++#define IFX_MPS_SWIRN6CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0068))
++#define IFX_MPS_SWIRN6ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x006C))
++#define IFX_MPS_SWIRN7SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0070))
++#define IFX_MPS_SWIRN7EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0074))
++#define IFX_MPS_SWIRN7CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0078))
++#define IFX_MPS_SWIRN7ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x007C))
++#define IFX_MPS_SWIRN8SET   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0080))
++#define IFX_MPS_SWIRN8EN   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0084))
++#define IFX_MPS_SWIRN8ICR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x008C))
++#define IFX_MPS_SWIRN8CR   ((volatile u32 *)(KSEG1 + LTQ_SWINT_BASE_ADDR + 0x0088))
++
++/*******************************************************************************
++ * MPS_SWIRNSET Register
++ ******************************************************************************/
++
++/* Software Interrupt Request IR5 (5) */
++#define IFX_MPS_SWIRNSET_IR5   (0x1 << 5)
++#define IFX_MPS_SWIRNSET_IR5_VAL(val)   (((val) & 0x1) << 5)
++#define IFX_MPS_SWIRNSET_IR5_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR5) | (val) & 1) << 5)
++/* Software Interrupt Request IR4 (4) */
++#define IFX_MPS_SWIRNSET_IR4   (0x1 << 4)
++#define IFX_MPS_SWIRNSET_IR4_VAL(val)   (((val) & 0x1) << 4)
++#define IFX_MPS_SWIRNSET_IR4_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR4) | (val) & 1) << 4)
++/* Software Interrupt Request IR3 (3) */
++#define IFX_MPS_SWIRNSET_IR3   (0x1 << 3)
++#define IFX_MPS_SWIRNSET_IR3_VAL(val)   (((val) & 0x1) << 3)
++#define IFX_MPS_SWIRNSET_IR3_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR3) | (val) & 1) << 3)
++/* Software Interrupt Request IR2 (2) */
++#define IFX_MPS_SWIRNSET_IR2   (0x1 << 2)
++#define IFX_MPS_SWIRNSET_IR2_VAL(val)   (((val) & 0x1) << 2)
++#define IFX_MPS_SWIRNSET_IR2_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR2) | (val) & 1) << 2)
++/* Software Interrupt Request IR1 (1) */
++#define IFX_MPS_SWIRNSET_IR1   (0x1 << 1)
++#define IFX_MPS_SWIRNSET_IR1_VAL(val)   (((val) & 0x1) << 1)
++#define IFX_MPS_SWIRNSET_IR1_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR1) | (val) & 1) << 1)
++/* Software Interrupt Request IR0 (0) */
++#define IFX_MPS_SWIRNSET_IR0   (0x1)
++#define IFX_MPS_SWIRNSET_IR0_VAL(val)   (((val) & 0x1) << 0)
++#define IFX_MPS_SWIRNSET_IR0_SET(reg,val) (reg) = (((reg & ~IFX_MPS_SWIRNSET_IR0) | (val) & 1) << 0)
++
++/*******************************************************************************
++ * MPS_SWIRNEN Register
++ ******************************************************************************/
++
++/* Software Interrupt Request IR5 (5) */
++#define IFX_MPS_SWIRNEN_IR5   (0x1 << 5)
++#define IFX_MPS_SWIRNEN_IR5_VAL(val)   (((val) & 0x1) << 5)
++#define IFX_MPS_SWIRNEN_IR5_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR5) >> 5) & 0x1)
++#define IFX_MPS_SWIRNEN_IR5_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR5) | (((val) & 0x1) << 5))
++/* Software Interrupt Request IR4 (4) */
++#define IFX_MPS_SWIRNEN_IR4   (0x1 << 4)
++#define IFX_MPS_SWIRNEN_IR4_VAL(val)   (((val) & 0x1) << 4)
++#define IFX_MPS_SWIRNEN_IR4_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR4) >> 4) & 0x1)
++#define IFX_MPS_SWIRNEN_IR4_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR4) | (((val) & 0x1) << 4))
++/* Software Interrupt Request IR3 (3) */
++#define IFX_MPS_SWIRNEN_IR3   (0x1 << 3)
++#define IFX_MPS_SWIRNEN_IR3_VAL(val)   (((val) & 0x1) << 3)
++#define IFX_MPS_SWIRNEN_IR3_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR3) >> 3) & 0x1)
++#define IFX_MPS_SWIRNEN_IR3_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR3) | (((val) & 0x1) << 3))
++/* Software Interrupt Request IR2 (2) */
++#define IFX_MPS_SWIRNEN_IR2   (0x1 << 2)
++#define IFX_MPS_SWIRNEN_IR2_VAL(val)   (((val) & 0x1) << 2)
++#define IFX_MPS_SWIRNEN_IR2_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR2) >> 2) & 0x1)
++#define IFX_MPS_SWIRNEN_IR2_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR2) | (((val) & 0x1) << 2))
++/* Software Interrupt Request IR1 (1) */
++#define IFX_MPS_SWIRNEN_IR1   (0x1 << 1)
++#define IFX_MPS_SWIRNEN_IR1_VAL(val)   (((val) & 0x1) << 1)
++#define IFX_MPS_SWIRNEN_IR1_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR1) >> 1) & 0x1)
++#define IFX_MPS_SWIRNEN_IR1_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR1) | (((val) & 0x1) << 1))
++/* Software Interrupt Request IR0 (0) */
++#define IFX_MPS_SWIRNEN_IR0   (0x1)
++#define IFX_MPS_SWIRNEN_IR0_VAL(val)   (((val) & 0x1) << 0)
++#define IFX_MPS_SWIRNEN_IR0_GET(val)   ((((val) & IFX_MPS_SWIRNEN_IR0) >> 0) & 0x1)
++#define IFX_MPS_SWIRNEN_IR0_SET(reg,val) (reg) = ((reg & ~IFX_MPS_SWIRNEN_IR0) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * MPS_SWIRNICR Register
++ ******************************************************************************/
++
++/* Software Interrupt Request IR5 (5) */
++#define IFX_MPS_SWIRNICR_IR5   (0x1 << 5)
++#define IFX_MPS_SWIRNICR_IR5_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR5) >> 5) & 0x1)
++/* Software Interrupt Request IR4 (4) */
++#define IFX_MPS_SWIRNICR_IR4   (0x1 << 4)
++#define IFX_MPS_SWIRNICR_IR4_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR4) >> 4) & 0x1)
++/* Software Interrupt Request IR3 (3) */
++#define IFX_MPS_SWIRNICR_IR3   (0x1 << 3)
++#define IFX_MPS_SWIRNICR_IR3_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR3) >> 3) & 0x1)
++/* Software Interrupt Request IR2 (2) */
++#define IFX_MPS_SWIRNICR_IR2   (0x1 << 2)
++#define IFX_MPS_SWIRNICR_IR2_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR2) >> 2) & 0x1)
++/* Software Interrupt Request IR1 (1) */
++#define IFX_MPS_SWIRNICR_IR1   (0x1 << 1)
++#define IFX_MPS_SWIRNICR_IR1_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR1) >> 1) & 0x1)
++/* Software Interrupt Request IR0 (0) */
++#define IFX_MPS_SWIRNICR_IR0   (0x1)
++#define IFX_MPS_SWIRNICR_IR0_GET(val)   ((((val) & IFX_MPS_SWIRNICR_IR0) >> 0) & 0x1)
++
++/*******************************************************************************
++ * MPS_SWIRNCR Register
++ ******************************************************************************/
++
++/* Software Interrupt Request IR5 (5) */
++#define IFX_MPS_SWIRNCR_IR5   (0x1 << 5)
++#define IFX_MPS_SWIRNCR_IR5_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR5) >> 5) & 0x1)
++/* Software Interrupt Request IR4 (4) */
++#define IFX_MPS_SWIRNCR_IR4   (0x1 << 4)
++#define IFX_MPS_SWIRNCR_IR4_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR4) >> 4) & 0x1)
++/* Software Interrupt Request IR3 (3) */
++#define IFX_MPS_SWIRNCR_IR3   (0x1 << 3)
++#define IFX_MPS_SWIRNCR_IR3_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR3) >> 3) & 0x1)
++/* Software Interrupt Request IR2 (2) */
++#define IFX_MPS_SWIRNCR_IR2   (0x1 << 2)
++#define IFX_MPS_SWIRNCR_IR2_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR2) >> 2) & 0x1)
++/* Software Interrupt Request IR1 (1) */
++#define IFX_MPS_SWIRNCR_IR1   (0x1 << 1)
++#define IFX_MPS_SWIRNCR_IR1_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR1) >> 1) & 0x1)
++/* Software Interrupt Request IR0 (0) */
++#define IFX_MPS_SWIRNCR_IR0   (0x1)
++#define IFX_MPS_SWIRNCR_IR0_GET(val)   ((((val) & IFX_MPS_SWIRNCR_IR0) >> 0) & 0x1)
++
++#endif
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/sys0_reg.h b/arch/mips/include/asm/mach-lantiq/svip/sys0_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/sys0_reg.h
+@@ -0,0 +1,163 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __SYS0_REG_H
++#define __SYS0_REG_H
++
++#define sys0_r32(reg) ltq_r32(&sys0->reg)
++#define sys0_w32(val, reg) ltq_w32(val, &sys0->reg)
++#define sys0_w32_mask(clear, set, reg) ltq_w32_mask(clear, set, &sys0->reg)
++
++/** SYS0 register structure */
++struct svip_reg_sys0 {
++	unsigned long sr; /* 0x0000 */
++	unsigned long bcr; /* 0x0004 */
++	unsigned long pll1cr; /* 0x0008 */
++	unsigned long pll2cr; /* 0x000c */
++	unsigned long tscr; /* 0x0010 */
++	unsigned long phyclkr; /* 0x0014 */
++};
++
++/*******************************************************************************
++ * SYS0 Status Register
++ ******************************************************************************/
++
++/* Endian select pin (31) */
++#define SYS0_SR_ESEL   (0x1 << 31)
++#define SYS0_SR_ESEL_GET(val)   ((((val) & SYS0_SR_ESEL) >> 31) & 0x1)
++/* Boot mode pins (27:24) */
++#define SYS0_SR_BMODE   (0xf << 24)
++#define SYS0_SR_BMODE_GET(val)   ((((val) & SYS0_SR_BMODE) >> 24) & 0xf)
++/* PLL2 Lock (18) */
++#define SYS0_SR_PLL2LOCK   (0x1 << 18)
++#define SYS0_SR_PLL2LOCK_GET(val)   ((((val) & SYS0_SR_PLL2LOCK) >> 18) & 0x1)
++/* PLL1 Lock (17) */
++#define SYS0_SR_PLL1LOCK   (0x1 << 17)
++#define SYS0_SR_PLL1LOCK_GET(val)   ((((val) & SYS0_SR_PLL1LOCK) >> 17) & 0x1)
++/* Discrete Timing Oscillator Lock (16) */
++#define SYS0_SR_DTOLOCK   (0x1 << 16)
++#define SYS0_SR_DTOLOCK_GET(val)   ((((val) & SYS0_SR_DTOLOCK) >> 16) & 0x1)
++/* Hardware Reset Indication (1) */
++#define SYS0_SR_HRSTIN   (0x1 << 1)
++#define SYS0_SR_HRSTIN_VAL(val)   (((val) & 0x1) << 1)
++#define SYS0_SR_HRSTIN_GET(val)   ((((val) & SYS0_SR_HRSTIN) >> 1) & 0x1)
++#define SYS0_SR_HRSTIN_SET(reg,val) (reg) = ((reg & ~SYS0_SR_HRSTIN) | (((val) & 0x1) << 1))
++/* Power-on Reset Indication (0) */
++#define SYS0_SR_POR   (0x1 << 0)
++#define SYS0_SR_POR_VAL(val)   (((val) & 0x1) << 0)
++#define SYS0_SR_POR_GET(val)   ((((val) & SYS0_SR_POR) >> 0) & 0x1)
++#define SYS0_SR_POR_SET(reg,val) (reg) = ((reg & ~SYS0_SR_POR) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * SYS0 Boot Control Register
++ ******************************************************************************/
++
++/* Configuration of Boot Source for CPU5 (25) */
++#define SYS0_BCR_BMODECPU5   (0x1 << 25)
++#define SYS0_BCR_BMODECPU5_VAL(val)   (((val) & 0x1) << 25)
++#define SYS0_BCR_BMODECPU5_GET(val)   ((((val) & SYS0_BCR_BMODECPU5) >> 25) & 0x1)
++#define SYS0_BCR_BMODECPU5_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU5) | (((val) & 0x1) << 25))
++/* Configuration of Boot Source for CPU4 (24) */
++#define SYS0_BCR_BMODECPU4   (0x1 << 24)
++#define SYS0_BCR_BMODECPU4_VAL(val)   (((val) & 0x1) << 24)
++#define SYS0_BCR_BMODECPU4_GET(val)   ((((val) & SYS0_BCR_BMODECPU4) >> 24) & 0x1)
++#define SYS0_BCR_BMODECPU4_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU4) | (((val) & 0x1) << 24))
++/* Configuration of Boot Source for CPU3 (23) */
++#define SYS0_BCR_BMODECPU3   (0x1 << 23)
++#define SYS0_BCR_BMODECPU3_VAL(val)   (((val) & 0x1) << 23)
++#define SYS0_BCR_BMODECPU3_GET(val)   ((((val) & SYS0_BCR_BMODECPU3) >> 23) & 0x1)
++#define SYS0_BCR_BMODECPU3_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU3) | (((val) & 0x1) << 23))
++/* Configuration of Boot Source for CPU2 (22) */
++#define SYS0_BCR_BMODECPU2   (0x1 << 22)
++#define SYS0_BCR_BMODECPU2_VAL(val)   (((val) & 0x1) << 22)
++#define SYS0_BCR_BMODECPU2_GET(val)   ((((val) & SYS0_BCR_BMODECPU2) >> 22) & 0x1)
++#define SYS0_BCR_BMODECPU2_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU2) | (((val) & 0x1) << 22))
++/* Configuration of Boot Source for CPU1 (21) */
++#define SYS0_BCR_BMODECPU1   (0x1 << 21)
++#define SYS0_BCR_BMODECPU1_VAL(val)   (((val) & 0x1) << 21)
++#define SYS0_BCR_BMODECPU1_GET(val)   ((((val) & SYS0_BCR_BMODECPU1) >> 21) & 0x1)
++#define SYS0_BCR_BMODECPU1_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU1) | (((val) & 0x1) << 21))
++/* Configuration of Boot Source for CPU0 (20:16) */
++#define SYS0_BCR_BMODECPU0   (0x1f << 16)
++#define SYS0_BCR_BMODECPU0_VAL(val)   (((val) & 0x1f) << 16)
++#define SYS0_BCR_BMODECPU0_GET(val)   ((((val) & SYS0_BCR_BMODECPU0) >> 16) & 0x1f)
++#define SYS0_BCR_BMODECPU0_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_BMODECPU0) | (((val) & 0x1f) << 16))
++/* Configuration of Endianess for CPU5 (5) */
++#define SYS0_BCR_ESELCPU5   (0x1 << 5)
++#define SYS0_BCR_ESELCPU5_VAL(val)   (((val) & 0x1) << 5)
++#define SYS0_BCR_ESELCPU5_GET(val)   ((((val) & SYS0_BCR_ESELCPU5) >> 5) & 0x1)
++#define SYS0_BCR_ESELCPU5_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU5) | (((val) & 0x1) << 5))
++/* Configuration of Endianess for CPU4 (4) */
++#define SYS0_BCR_ESELCPU4   (0x1 << 4)
++#define SYS0_BCR_ESELCPU4_VAL(val)   (((val) & 0x1) << 4)
++#define SYS0_BCR_ESELCPU4_GET(val)   ((((val) & SYS0_BCR_ESELCPU4) >> 4) & 0x1)
++#define SYS0_BCR_ESELCPU4_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU4) | (((val) & 0x1) << 4))
++/* Configuration of Endianess for CPU3 (3) */
++#define SYS0_BCR_ESELCPU3   (0x1 << 3)
++#define SYS0_BCR_ESELCPU3_VAL(val)   (((val) & 0x1) << 3)
++#define SYS0_BCR_ESELCPU3_GET(val)   ((((val) & SYS0_BCR_ESELCPU3) >> 3) & 0x1)
++#define SYS0_BCR_ESELCPU3_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU3) | (((val) & 0x1) << 3))
++/* Configuration of Endianess for CPU2 (2) */
++#define SYS0_BCR_ESELCPU2   (0x1 << 2)
++#define SYS0_BCR_ESELCPU2_VAL(val)   (((val) & 0x1) << 2)
++#define SYS0_BCR_ESELCPU2_GET(val)   ((((val) & SYS0_BCR_ESELCPU2) >> 2) & 0x1)
++#define SYS0_BCR_ESELCPU2_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU2) | (((val) & 0x1) << 2))
++/* Configuration of Endianess for CPU1 (1) */
++#define SYS0_BCR_ESELCPU1   (0x1 << 1)
++#define SYS0_BCR_ESELCPU1_VAL(val)   (((val) & 0x1) << 1)
++#define SYS0_BCR_ESELCPU1_GET(val)   ((((val) & SYS0_BCR_ESELCPU1) >> 1) & 0x1)
++#define SYS0_BCR_ESELCPU1_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU1) | (((val) & 0x1) << 1))
++/* Configuration of Endianess for CPU0  (0) */
++#define SYS0_BCR_ESELCPU0   (0x1)
++#define SYS0_BCR_ESELCPU0_VAL(val)   (((val) & 0x1) << 0)
++#define SYS0_BCR_ESELCPU0_GET(val)   ((((val) & SYS0_BCR_ESELCPU0) >> 0) & 0x1)
++#define SYS0_BCR_ESELCPU0_SET(reg,val) (reg) = ((reg & ~SYS0_BCR_ESELCPU0) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * PLL1 Control Register
++ ******************************************************************************/
++
++/* PLL1 Bypass Enable (31) */
++#define SYS0_PLL1CR_OSCBYP   (0x1 << 31)
++#define SYS0_PLL1CR_OSCBYP_VAL(val)   (((val) & 0x1) << 31)
++#define SYS0_PLL1CR_OSCBYP_GET(val)   ((((val) & SYS0_PLL1CR_OSCBYP) >> 31) & 0x1)
++#define SYS0_PLL1CR_OSCBYP_SET(reg,val) (reg) = ((reg & ~SYS0_PLL1CR_OSCBYP) | (((val) & 0x1) << 31))
++/* PLL1 Divider Value (1:0) */
++#define SYS0_PLL1CR_PLLDIV   (0x3)
++#define SYS0_PLL1CR_PLLDIV_VAL(val)   (((val) & 0x3) << 0)
++#define SYS0_PLL1CR_PLLDIV_GET(val)   ((((val) & SYS0_PLL1CR_PLLDIV) >> 0) & 0x3)
++#define SYS0_PLL1CR_PLLDIV_SET(reg,val) (reg) = ((reg & ~SYS0_PLL1CR_PLLDIV) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * PLL2 Control Register
++ ******************************************************************************/
++
++/* PLL2 clear deepsleep (31) */
++#define SYS0_PLL2CR_CLRDS   (0x1 << 31)
++#define SYS0_PLL2CR_CLRDS_VAL(val)   (((val) & 0x1) << 31)
++#define SYS0_PLL2CR_CLRDS_GET(val)   ((((val) & SYS0_PLL2CR_CLRDS) >> 31) & 0x1)
++#define SYS0_PLL2CR_CLRDS_SET(reg,val) (reg) = ((reg & ~SYS0_PLL2CR_CLRDS) | (((val) & 0x1) << 31))
++/* PLL2 set deepsleep (30) */
++#define SYS0_PLL2CR_SETDS   (0x1 << 30)
++#define SYS0_PLL2CR_SETDS_VAL(val)   (((val) & 0x1) << 30)
++#define SYS0_PLL2CR_SETDS_GET(val)   ((((val) & SYS0_PLL2CR_SETDS) >> 30) & 0x1)
++#define SYS0_PLL2CR_SETDS_SET(reg,val) (reg) = ((reg & ~SYS0_PLL2CR_SETDS) | (((val) & 0x1) << 30))
++/* PLL2 Fractional division enable (16) */
++#define SYS0_PLL2CR_FRACTEN   (0x1 << 16)
++#define SYS0_PLL2CR_FRACTEN_VAL(val)   (((val) & 0x1) << 16)
++#define SYS0_PLL2CR_FRACTEN_GET(val)   ((((val) & SYS0_PLL2CR_FRACTEN) >> 16) & 0x1)
++#define SYS0_PLL2CR_FRACTEN_SET(reg,val) (reg) = ((reg & ~SYS0_PLL2CR_FRACTEN) | (((val) & 0x1) << 16))
++/* PLL2 Fractional division value (9:0) */
++#define SYS0_FRACTVAL   (0x3f)
++#define SYS0_FRACTVAL_VAL(val)   (((val) & 0x3f) << 0)
++#define SYS0_FRACTVAL_GET(val)   ((((val) & SYS0_FRACTVAL) >> 0) & 0x3f)
++#define SYS0_FRACTVAL_SET(reg,val) (reg) = ((reg & ~SYS0_FRACTVAL) | (((val) & 0x3f) << 0))
++
++#endif
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/sys1_reg.h b/arch/mips/include/asm/mach-lantiq/svip/sys1_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/sys1_reg.h
+@@ -0,0 +1,368 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __SYS1_REG_H
++#define __SYS1_REG_H
++
++#define sys1_r32(reg) ltq_r32(&sys1->reg)
++#define sys1_w32(val, reg) ltq_w32(val, &sys1->reg)
++#define sys1_w32_mask(clear, set, reg) ltq_w32_mask(clear, set, &sys1->reg)
++
++/** SYS1 register structure */
++struct svip_reg_sys1 {
++	unsigned long clksr; /* 0x0000 */
++	unsigned long clkenr; /* 0x0004 */
++	unsigned long clkclr; /* 0x0008 */
++	unsigned long reserved0[1];
++	unsigned long l2ccr; /* 0x0010 */
++	unsigned long fpicr; /* 0x0014 */
++	unsigned long wdtcr; /* 0x0018 */
++	unsigned long reserved1[1];
++	unsigned long cpucr[6]; /* 0x0020 */
++	unsigned long reserved2[2];
++	unsigned long rsr; /* 0x0040 */
++	unsigned long rreqr; /* 0x0044 */
++	unsigned long rrlsr; /* 0x0048 */
++	unsigned long rbtr; /* 0x004c */
++	unsigned long irncr; /* 0x0050 */
++	unsigned long irnicr; /* 0x0054 */
++	unsigned long irnen; /* 0x0058 */
++	unsigned long reserved3[1];
++	unsigned long cpursr[6]; /* 0x0060 */
++	unsigned long reserved4[2];
++	unsigned long cpusrssr[6]; /* 0x0080 */
++	unsigned long reserved5[2];
++	unsigned long cpuwrssr[6]; /* 0x00a0 */
++};
++
++/*******************************************************************************
++ * SYS1 Clock Status Register
++ ******************************************************************************/
++/* (r) Clock Enable for L2C */
++#define SYS1_CLKSR_L2C (0x1 << 31)
++/* (r) Clock Enable for DDR2 */
++#define SYS1_CLKSR_DDR2 (0x1 << 30)
++/* (r) Clock Enable for SMI2 */
++#define SYS1_CLKSR_SMI2 (0x1 << 29)
++/* (r) Clock Enable for SMI1 */
++#define SYS1_CLKSR_SMI1 (0x1 << 28)
++/* (r) Clock Enable for SMI0 */
++#define SYS1_CLKSR_SMI0 (0x1 << 27)
++/* (r) Clock Enable for FMI0 */
++#define SYS1_CLKSR_FMI0 (0x1 << 26)
++/* (r) Clock Enable for PORT0 */
++#define SYS1_CLKSR_PORT0 (0x1 << 0)
++/* (r) Clock Enable for PCM3 */
++#define SYS1_CLKSR_PCM3 (0x1 << 19)
++/* (r) Clock Enable for PCM2 */
++#define SYS1_CLKSR_PCM2 (0x1 << 18)
++/* (r) Clock Enable for PCM1 */
++#define SYS1_CLKSR_PCM1 (0x1 << 17)
++/* (r) Clock Enable for PCM0 */
++#define SYS1_CLKSR_PCM0 (0x1 << 16)
++/* (r) Clock Enable for ASC1 */
++#define SYS1_CLKSR_ASC1 (0x1 << 15)
++/* (r) Clock Enable for ASC0 */
++#define SYS1_CLKSR_ASC0 (0x1 << 14)
++/* (r) Clock Enable for SSC2 */
++#define SYS1_CLKSR_SSC2 (0x1 << 13)
++/* (r) Clock Enable for SSC1 */
++#define SYS1_CLKSR_SSC1 (0x1 << 12)
++/* (r) Clock Enable for SSC0 */
++#define SYS1_CLKSR_SSC0 (0x1 << 11)
++/* (r) Clock Enable for GPTC */
++#define SYS1_CLKSR_GPTC (0x1 << 10)
++/* (r) Clock Enable for DMA */
++#define SYS1_CLKSR_DMA (0x1 << 9)
++/* (r) Clock Enable for FSCT */
++#define SYS1_CLKSR_FSCT (0x1 << 8)
++/* (r) Clock Enable for ETHSW */
++#define SYS1_CLKSR_ETHSW (0x1 << 7)
++/* (r) Clock Enable for EBU */
++#define SYS1_CLKSR_EBU (0x1 << 6)
++/* (r) Clock Enable for TRNG */
++#define SYS1_CLKSR_TRNG (0x1 << 5)
++/* (r) Clock Enable for DEU */
++#define SYS1_CLKSR_DEU (0x1 << 4)
++/* (r) Clock Enable for PORT3 */
++#define SYS1_CLKSR_PORT3 (0x1 << 3)
++/* (r) Clock Enable for PORT2 */
++#define SYS1_CLKSR_PORT2 (0x1 << 2)
++/* (r) Clock Enable for PORT1 */
++#define SYS1_CLKSR_PORT1 (0x1 << 1)
++
++/*******************************************************************************
++ * SYS1 Clock Enable Register
++ ******************************************************************************/
++/* (w) Clock Enable Request for L2C */
++#define SYS1_CLKENR_L2C (0x1 << 31)
++/* (w) Clock Enable Request for DDR2 */
++#define SYS1_CLKENR_DDR2 (0x1 << 30)
++/* (w) Clock Enable Request for SMI2 */
++#define SYS1_CLKENR_SMI2 (0x1 << 29)
++/* (w) Clock Enable Request for SMI1 */
++#define SYS1_CLKENR_SMI1 (0x1 << 28)
++/* (w) Clock Enable Request for SMI0 */
++#define SYS1_CLKENR_SMI0 (0x1 << 27)
++/* (w) Clock Enable Request for FMI0 */
++#define SYS1_CLKENR_FMI0 (0x1 << 26)
++/* (w) Clock Enable Request for PORT0 */
++#define SYS1_CLKENR_PORT0 (0x1 << 0)
++/* (w) Clock Enable Request for PCM3 */
++#define SYS1_CLKENR_PCM3 (0x1 << 19)
++/* (w) Clock Enable Request for PCM2 */
++#define SYS1_CLKENR_PCM2 (0x1 << 18)
++/* (w) Clock Enable Request for PCM1 */
++#define SYS1_CLKENR_PCM1 (0x1 << 17)
++/* (w) Clock Enable Request for PCM0 */
++#define SYS1_CLKENR_PCM0 (0x1 << 16)
++/* (w) Clock Enable Request for ASC1 */
++#define SYS1_CLKENR_ASC1 (0x1 << 15)
++/* (w) Clock Enable Request for ASC0 */
++#define SYS1_CLKENR_ASC0 (0x1 << 14)
++/* (w) Clock Enable Request for SSC2 */
++#define SYS1_CLKENR_SSC2 (0x1 << 13)
++/* (w) Clock Enable Request for SSC1 */
++#define SYS1_CLKENR_SSC1 (0x1 << 12)
++/* (w) Clock Enable Request for SSC0 */
++#define SYS1_CLKENR_SSC0 (0x1 << 11)
++/* (w) Clock Enable Request for GPTC */
++#define SYS1_CLKENR_GPTC (0x1 << 10)
++/* (w) Clock Enable Request for DMA */
++#define SYS1_CLKENR_DMA (0x1 << 9)
++/* (w) Clock Enable Request for FSCT */
++#define SYS1_CLKENR_FSCT (0x1 << 8)
++/* (w) Clock Enable Request for ETHSW */
++#define SYS1_CLKENR_ETHSW (0x1 << 7)
++/* (w) Clock Enable Request for EBU */
++#define SYS1_CLKENR_EBU (0x1 << 6)
++/* (w) Clock Enable Request for TRNG */
++#define SYS1_CLKENR_TRNG (0x1 << 5)
++/* (w) Clock Enable Request for DEU */
++#define SYS1_CLKENR_DEU (0x1 << 4)
++/* (w) Clock Enable Request for PORT3 */
++#define SYS1_CLKENR_PORT3 (0x1 << 3)
++/* (w) Clock Enable Request for PORT2 */
++#define SYS1_CLKENR_PORT2 (0x1 << 2)
++/* (w) Clock Enable Request for PORT1 */
++#define SYS1_CLKENR_PORT1 (0x1 << 1)
++
++/*******************************************************************************
++ * SYS1 Clock Clear Register
++ ******************************************************************************/
++/* (w) Clock Disable Request for L2C */
++#define SYS1_CLKCLR_L2C (0x1 << 31)
++/* (w) Clock Disable Request for DDR2 */
++#define SYS1_CLKCLR_DDR2 (0x1 << 30)
++/* (w) Clock Disable Request for SMI2 */
++#define SYS1_CLKCLR_SMI2 (0x1 << 29)
++/* (w) Clock Disable Request for SMI1 */
++#define SYS1_CLKCLR_SMI1 (0x1 << 28)
++/* (w) Clock Disable Request for SMI0 */
++#define SYS1_CLKCLR_SMI0 (0x1 << 27)
++/* (w) Clock Disable Request for FMI0 */
++#define SYS1_CLKCLR_FMI0 (0x1 << 26)
++/* (w) Clock Disable Request for PORT0 */
++#define SYS1_CLKCLR_PORT0 (0x1 << 0)
++/* (w) Clock Disable Request for PCM3 */
++#define SYS1_CLKCLR_PCM3 (0x1 << 19)
++/* (w) Clock Disable Request for PCM2 */
++#define SYS1_CLKCLR_PCM2 (0x1 << 18)
++/* (w) Clock Disable Request for PCM1 */
++#define SYS1_CLKCLR_PCM1 (0x1 << 17)
++/* (w) Clock Disable Request for PCM0 */
++#define SYS1_CLKCLR_PCM0 (0x1 << 16)
++/* (w) Clock Disable Request for ASC1 */
++#define SYS1_CLKCLR_ASC1 (0x1 << 15)
++/* (w) Clock Disable Request for ASC0 */
++#define SYS1_CLKCLR_ASC0 (0x1 << 14)
++/* (w) Clock Disable Request for SSC2 */
++#define SYS1_CLKCLR_SSC2 (0x1 << 13)
++/* (w) Clock Disable Request for SSC1 */
++#define SYS1_CLKCLR_SSC1 (0x1 << 12)
++/* (w) Clock Disable Request for SSC0 */
++#define SYS1_CLKCLR_SSC0 (0x1 << 11)
++/* (w) Clock Disable Request for GPTC */
++#define SYS1_CLKCLR_GPTC (0x1 << 10)
++/* (w) Clock Disable Request for DMA */
++#define SYS1_CLKCLR_DMA (0x1 << 9)
++/* (w) Clock Disable Request for FSCT */
++#define SYS1_CLKCLR_FSCT (0x1 << 8)
++/* (w) Clock Disable Request for ETHSW */
++#define SYS1_CLKCLR_ETHSW (0x1 << 7)
++/* (w) Clock Disable Request for EBU */
++#define SYS1_CLKCLR_EBU (0x1 << 6)
++/* (w) Clock Disable Request for TRNG */
++#define SYS1_CLKCLR_TRNG (0x1 << 5)
++/* (w) Clock Disable Request for DEU */
++#define SYS1_CLKCLR_DEU (0x1 << 4)
++/* (w) Clock Disable Request for PORT3 */
++#define SYS1_CLKCLR_PORT3 (0x1 << 3)
++/* (w) Clock Disable Request for PORT2 */
++#define SYS1_CLKCLR_PORT2 (0x1 << 2)
++/* (w) Clock Disable Request for PORT1 */
++#define SYS1_CLKCLR_PORT1 (0x1 << 1)
++
++/*******************************************************************************
++ * SYS1 FPI Control Register
++ ******************************************************************************/
++
++/* FPI Bus Clock divider (0) */
++#define SYS1_FPICR_FPIDIV   (0x1)
++#define SYS1_FPICR_FPIDIV_VAL(val)   (((val) & 0x1) << 0)
++#define SYS1_FPICR_FPIDIV_GET(val)   ((((val) & SYS1_FPICR_FPIDIV) >> 0) & 0x1)
++#define SYS1_FPICR_FPIDIV_SET(reg,val) (reg) = ((reg & ~SYS1_FPICR_FPIDIV) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * SYS1 Clock Control Register for CPUn
++ ******************************************************************************/
++
++/* Enable bit for clock of CPUn (1) */
++#define SYS1_CPUCR_CPUCLKEN    (0x1 << 1)
++#define SYS1_CPUCR_CPUCLKEN_VAL(val)   (((val) & 0x1) << 1)
++#define SYS1_CPUCR_CPUCLKEN_GET(val)   ((((val) & SYS1_CPUCR_CPUCLKEN) >> 1) & 0x1)
++#define SYS1_CPUCR_CPUCLKEN_SET(reg,val) (reg) = ((reg & ~SYS1_CPUCR_CPUCLKEN) | (((val) & 0x1) << 1))
++/* Divider factor for clock of CPUn (0) */
++#define SYS1_CPUCR_CPUDIV    (0x1)
++#define SYS1_CPUCR_CPUDIV_VAL(val)   (((val) & 0x1) << 0)
++#define SYS1_CPUCR_CPUDIV_GET(val)   ((((val) & SYS1_CPUCR_CPUDIV) >> 0) & 0x1)
++#define SYS1_CPUCR_CPUDIV_SET(reg,val) (reg) = ((reg & ~SYS1_CPUCR_CPUDIV) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * SYS1 Reset Request Register
++ ******************************************************************************/
++
++/* HRSTOUT Reset Request (18) */
++#define SYS1_RREQ_HRSTOUT   (0x1 << 18)
++#define SYS1_RREQ_HRSTOUT_VAL(val)   (((val) & 0x1) << 18)
++#define SYS1_RREQ_HRSTOUT_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_HRSTOUT) | (((val) & 1) << 18))
++						    /* FBS0 Reset Request (17) */
++#define SYS1_RREQ_FBS0   (0x1 << 17)
++#define SYS1_RREQ_FBS0_VAL(val)   (((val) & 0x1) << 17)
++#define SYS1_RREQ_FBS0_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_FBS0) | (((val) & 1) << 17))
++						 /* SUBSYS Reset Request (16) */
++#define SYS1_RREQ_SUBSYS   (0x1 << 16)
++#define SYS1_RREQ_SUBSYS_VAL(val)   (((val) & 0x1) << 16)
++#define SYS1_RREQ_SUBSYS_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_SUBSYS) | (((val) & 1) << 16))
++						   /* Watchdog5 Reset Request (13) */
++#define SYS1_RREQ_WDT5   (0x1 << 13)
++#define SYS1_RREQ_WDT5_VAL(val)   (((val) & 0x1) << 13)
++#define SYS1_RREQ_WDT5_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT5) | (((val) & 1) << 13))
++						 /* Watchdog4 Reset Request (12) */
++#define SYS1_RREQ_WDT4   (0x1 << 12)
++#define SYS1_RREQ_WDT4_VAL(val)   (((val) & 0x1) << 12)
++#define SYS1_RREQ_WDT4_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT4) | (((val) & 1) << 12))
++						 /* Watchdog3 Reset Request (11) */
++#define SYS1_RREQ_WDT3   (0x1 << 11)
++#define SYS1_RREQ_WDT3_VAL(val)   (((val) & 0x1) << 11)
++#define SYS1_RREQ_WDT3_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT3) | (((val) & 1) << 11))
++						 /* Watchdog2 Reset Request (10) */
++#define SYS1_RREQ_WDT2   (0x1 << 10)
++#define SYS1_RREQ_WDT2_VAL(val)   (((val) & 0x1) << 10)
++#define SYS1_RREQ_WDT2_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT2) | (((val) & 1) << 10))
++						 /* Watchdog1 Reset Request (9) */
++#define SYS1_RREQ_WDT1   (0x1 << 9)
++#define SYS1_RREQ_WDT1_VAL(val)   (((val) & 0x1) << 9)
++#define SYS1_RREQ_WDT1_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT1) | (((val) & 1) << 9))
++						 /* Watchdog0 Reset Request (8) */
++#define SYS1_RREQ_WDT0   (0x1 << 8)
++#define SYS1_RREQ_WDT0_VAL(val)   (((val) & 0x1) << 8)
++#define SYS1_RREQ_WDT0_SET(reg,val) (reg) = (((reg & ~SYS1_RREQ_WDT0) | (((val) & 1) << 8))
++						 /* CPU5 Reset Request (5) */
++#define SYS1_RREQ_CPU5   (0x1 << 5)
++#define SYS1_RREQ_CPU5_VAL(val)   (((val) & 0x1) << 5)
++#define SYS1_RREQ_CPU5_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU5) | (((val) & 1) << 5))
++						 /* CPU4 Reset Request (4) */
++#define SYS1_RREQ_CPU4   (0x1 << 4)
++#define SYS1_RREQ_CPU4_VAL(val)   (((val) & 0x1) << 4)
++#define SYS1_RREQ_CPU4_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU4) | (((val) & 1) << 4))
++						 /* CPU3 Reset Request (3) */
++#define SYS1_RREQ_CPU3   (0x1 << 3)
++#define SYS1_RREQ_CPU3_VAL(val)   (((val) & 0x1) << 3)
++#define SYS1_RREQ_CPU3_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU3) | (((val) & 1) << 3))
++						 /* CPU2 Reset Request (2) */
++#define SYS1_RREQ_CPU2   (0x1 << 2)
++#define SYS1_RREQ_CPU2_VAL(val)   (((val) & 0x1) << 2)
++#define SYS1_RREQ_CPU2_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU2) | (((val) & 1) << 2))
++						 /* CPU1 Reset Request (1) */
++#define SYS1_RREQ_CPU1   (0x1 << 1)
++#define SYS1_RREQ_CPU1_VAL(val)   (((val) & 0x1) << 1)
++#define SYS1_RREQ_CPU1_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU1) | (((val) & 1) << 1))
++/* CPU0 Reset Request (0) */
++#define SYS1_RREQ_CPU0   (0x1)
++#define SYS1_RREQ_CPU0_VAL(val)   (((val) & 0x1) << 0)
++#define SYS1_RREQ_CPU0_SET(reg,val) (reg) = ((reg & ~SYS1_RREQ_CPU0) | (((val) & 1) << 0))
++
++/*******************************************************************************
++ * SYS1 Reset Release Register
++ ******************************************************************************/
++
++/* HRSTOUT Reset Release (18) */
++#define SYS1_RRLSR_HRSTOUT   (0x1 << 18)
++#define SYS1_RRLSR_HRSTOUT_VAL(val)   (((val) & 0x1) << 18)
++#define SYS1_RRLSR_HRSTOUT_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_HRSTOUT) | (((val) & 1) << 18))
++/* FBS0 Reset Release (17) */
++#define SYS1_RRLSR_FBS0   (0x1 << 17)
++#define SYS1_RRLSR_FBS0_VAL(val)   (((val) & 0x1) << 17)
++#define SYS1_RRLSR_FBS0_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_FBS0) | (((val) & 1) << 17))
++/* SUBSYS Reset Release (16) */
++#define SYS1_RRLSR_SUBSYS   (0x1 << 16)
++#define SYS1_RRLSR_SUBSYS_VAL(val)   (((val) & 0x1) << 16)
++#define SYS1_RRLSR_SUBSYS_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_SUBSYS) | (((val) & 1) << 16))
++/* Watchdog5 Reset Release (13) */
++#define SYS1_RRLSR_WDT5   (0x1 << 13)
++#define SYS1_RRLSR_WDT5_VAL(val)   (((val) & 0x1) << 13)
++#define SYS1_RRLSR_WDT5_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT5) | (((val) & 1) << 13))
++/* Watchdog4 Reset Release (12) */
++#define SYS1_RRLSR_WDT4   (0x1 << 12)
++#define SYS1_RRLSR_WDT4_VAL(val)   (((val) & 0x1) << 12)
++#define SYS1_RRLSR_WDT4_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT4) | (((val) & 1) << 12))
++/* Watchdog3 Reset Release (11) */
++#define SYS1_RRLSR_WDT3   (0x1 << 11)
++#define SYS1_RRLSR_WDT3_VAL(val)   (((val) & 0x1) << 11)
++#define SYS1_RRLSR_WDT3_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT3) | (((val) & 1) << 11))
++/* Watchdog2 Reset Release (10) */
++#define SYS1_RRLSR_WDT2   (0x1 << 10)
++#define SYS1_RRLSR_WDT2_VAL(val)   (((val) & 0x1) << 10)
++#define SYS1_RRLSR_WDT2_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT2) | (((val) & 1) << 10))
++/* Watchdog1 Reset Release (9) */
++#define SYS1_RRLSR_WDT1   (0x1 << 9)
++#define SYS1_RRLSR_WDT1_VAL(val)   (((val) & 0x1) << 9)
++#define SYS1_RRLSR_WDT1_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT1) | (((val) & 1) << 9))
++/* Watchdog0 Reset Release (8) */
++#define SYS1_RRLSR_WDT0   (0x1 << 8)
++#define SYS1_RRLSR_WDT0_VAL(val)   (((val) & 0x1) << 8)
++#define SYS1_RRLSR_WDT0_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_WDT0) | (((val) & 1) << 8))
++/* CPU5 Reset Release (5) */
++#define SYS1_RRLSR_CPU5   (0x1 << 5)
++#define SYS1_RRLSR_CPU5_VAL(val)   (((val) & 0x1) << 5)
++#define SYS1_RRLSR_CPU5_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU5) | (((val) & 1) << 5))
++/* CPU4 Reset Release (4) */
++#define SYS1_RRLSR_CPU4   (0x1 << 4)
++#define SYS1_RRLSR_CPU4_VAL(val)   (((val) & 0x1) << 4)
++#define SYS1_RRLSR_CPU4_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU4) | (((val) & 1) << 4))
++/* CPU3 Reset Release (3) */
++#define SYS1_RRLSR_CPU3   (0x1 << 3)
++#define SYS1_RRLSR_CPU3_VAL(val)   (((val) & 0x1) << 3)
++#define SYS1_RRLSR_CPU3_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU3) | (((val) & 1) << 3))
++/* CPU2 Reset Release (2) */
++#define SYS1_RRLSR_CPU2   (0x1 << 2)
++#define SYS1_RRLSR_CPU2_VAL(val)   (((val) & 0x1) << 2)
++#define SYS1_RRLSR_CPU2_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU2) | (((val) & 1) << 2))
++/* CPU1 Reset Release (1) */
++#define SYS1_RRLSR_CPU1   (0x1 << 1)
++#define SYS1_RRLSR_CPU1_VAL(val)   (((val) & 0x1) << 1)
++#define SYS1_RRLSR_CPU1_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU1) | (((val) & 1) << 1))
++/* CPU0 Reset Release (0) */
++#define SYS1_RRLSR_CPU0   (0x1)
++#define SYS1_RRLSR_CPU0_VAL(val)   (((val) & 0x1) << 0)
++#define SYS1_RRLSR_CPU0_SET(reg,val) (reg) = ((reg & ~SYS1_RRLSR_CPU0) | (((val) & 1) << 0))
++
++#endif
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/sys2_reg.h b/arch/mips/include/asm/mach-lantiq/svip/sys2_reg.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/sys2_reg.h
+@@ -0,0 +1,492 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __SYS2_REG_H
++#define __SYS2_REG_H
++
++#define sys2_r32(reg) ltq_r32(&sys2->reg)
++#define sys2_w32(val, reg) ltq_w32(val, &sys2->reg)
++#define sys2_w32_mask(clear, set, reg) ltq_w32_mask(clear, set, &sys2->reg)
++
++/** SYS2 register structure */
++struct svip_reg_sys2 {
++	volatile unsigned long  clksr;  /*  0x0000 */
++	volatile unsigned long  clkenr;  /*  0x0004 */
++	volatile unsigned long  clkclr;  /*  0x0008 */
++	volatile unsigned long  reserved0[1];
++	volatile unsigned long  rsr;  /*  0x0010 */
++	volatile unsigned long  rreqr;  /*  0x0014 */
++	volatile unsigned long  rrlsr;  /*  0x0018 */
++};
++
++/*******************************************************************************
++ * SYS2 Clock Status Register
++ ******************************************************************************/
++
++/* Clock Enable for PORT4 */
++#define SYS2_CLKSR_PORT4 (0x1 << 27)
++#define SYS2_CLKSR_PORT4_VAL(val) (((val) & 0x1) << 27)
++#define SYS2_CLKSR_PORT4_GET(val) (((val) & SYS2_CLKSR_PORT4) >> 27)
++/* Clock Enable for HWSYNC */
++#define SYS2_CLKSR_HWSYNC (0x1 << 26)
++#define SYS2_CLKSR_HWSYNC_VAL(val) (((val) &
++#define SYS2_CLKSR_HWSYNC_GET(val) (((val) & SYS2_CLKSR_HWSYNC) >> 26)
++					 /* Clock Enable for MBS */
++#define SYS2_CLKSR_MBS (0x1 << 25)
++#define SYS2_CLKSR_MBS_VAL(val) (((val) & 0x1) << 25)
++#define SYS2_CLKSR_MBS_GET(val) (((val) & SYS2_CLKSR_MBS) >> 25)
++					 /* Clock Enable for SWINT */
++#define SYS2_CLKSR_SWINT (0x1 << 24)
++#define SYS2_CLKSR_SWINT_VAL(val) (((val) & 0x1) << 24)
++#define SYS2_CLKSR_SWINT_GET(val) (((val) & SYS2_CLKSR_SWINT) >> 24)
++					 /* Clock Enable for HWACC3 */
++#define SYS2_CLKSR_HWACC3 (0x1 << 19)
++#define SYS2_CLKSR_HWACC3_VAL(val) (((val) &
++#define SYS2_CLKSR_HWACC3_GET(val) (((val) & SYS2_CLKSR_HWACC3) >> 19)
++					 /* Clock Enable for HWACC2 */
++#define SYS2_CLKSR_HWACC2 (0x1 << 18)
++#define SYS2_CLKSR_HWACC2_VAL(val) (((val) &
++#define SYS2_CLKSR_HWACC2_GET(val) (((val) & SYS2_CLKSR_HWACC2) >> 18)
++					 /* Clock Enable for HWACC1 */
++#define SYS2_CLKSR_HWACC1 (0x1 << 17)
++#define SYS2_CLKSR_HWACC1_VAL(val) (((val) &
++#define SYS2_CLKSR_HWACC1_GET(val) (((val) & SYS2_CLKSR_HWACC1) >> 17)
++					 /* Clock Enable for HWACC0 */
++#define SYS2_CLKSR_HWACC0 (0x1 << 16)
++#define SYS2_CLKSR_HWACC0_VAL(val) (((val) &
++#define SYS2_CLKSR_HWACC0_GET(val) (((val) & SYS2_CLKSR_HWACC0) >> 16)
++					 /* Clock Enable for SIF7 */
++#define SYS2_CLKSR_SIF7 (0x1 << 15)
++#define SYS2_CLKSR_SIF7_VAL(val) (((val) & 0x1) << 15)
++#define SYS2_CLKSR_SIF7_GET(val) (((val) & SYS2_CLKSR_SIF7) >> 15)
++					 /* Clock Enable for SIF6 */
++#define SYS2_CLKSR_SIF6 (0x1 << 14)
++#define SYS2_CLKSR_SIF6_VAL(val) (((val) & 0x1) << 14)
++#define SYS2_CLKSR_SIF6_GET(val) (((val) & SYS2_CLKSR_SIF6) >> 14)
++					 /* Clock Enable for SIF5 */
++#define SYS2_CLKSR_SIF5 (0x1 << 13)
++#define SYS2_CLKSR_SIF5_VAL(val) (((val) & 0x1) << 13)
++#define SYS2_CLKSR_SIF5_GET(val) (((val) & SYS2_CLKSR_SIF5) >> 13)
++					 /* Clock Enable for SIF4 */
++#define SYS2_CLKSR_SIF4 (0x1 << 12)
++#define SYS2_CLKSR_SIF4_VAL(val) (((val) & 0x1) << 12)
++#define SYS2_CLKSR_SIF4_GET(val) (((val) & SYS2_CLKSR_SIF4) >> 12)
++					 /* Clock Enable for SIF3 */
++#define SYS2_CLKSR_SIF3 (0x1 << 11)
++#define SYS2_CLKSR_SIF3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_CLKSR_SIF3_GET(val) (((val) & SYS2_CLKSR_SIF3) >> 11)
++/* Clock Enable for SIF2 */
++#define SYS2_CLKSR_SIF2 (0x1 << 10)
++#define SYS2_CLKSR_SIF2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_CLKSR_SIF2_GET(val) (((val) & SYS2_CLKSR_SIF2) >> 10)
++/* Clock Enable for SIF1 */
++#define SYS2_CLKSR_SIF1 (0x1 << 9)
++#define SYS2_CLKSR_SIF1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_CLKSR_SIF1_GET(val) (((val) & SYS2_CLKSR_SIF1) >> 9)
++/* Clock Enable for SIF0 */
++#define SYS2_CLKSR_SIF0 (0x1 << 8)
++#define SYS2_CLKSR_SIF0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_CLKSR_SIF0_GET(val) (((val) & SYS2_CLKSR_SIF0) >> 8)
++/* Clock Enable for DFEV7 */
++#define SYS2_CLKSR_DFEV7 (0x1 << 7)
++#define SYS2_CLKSR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_CLKSR_DFEV7_GET(val) (((val) & SYS2_CLKSR_DFEV7) >> 7)
++/* Clock Enable for DFEV6 */
++#define SYS2_CLKSR_DFEV6 (0x1 << 6)
++#define SYS2_CLKSR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_CLKSR_DFEV6_GET(val) (((val) & SYS2_CLKSR_DFEV6) >> 6)
++/* Clock Enable for DFEV5 */
++#define SYS2_CLKSR_DFEV5 (0x1 << 5)
++#define SYS2_CLKSR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_CLKSR_DFEV5_GET(val) (((val) & SYS2_CLKSR_DFEV5) >> 5)
++/* Clock Enable for DFEV4 */
++#define SYS2_CLKSR_DFEV4 (0x1 << 4)
++#define SYS2_CLKSR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_CLKSR_DFEV4_GET(val) (((val) & SYS2_CLKSR_DFEV4) >> 4)
++/* Clock Enable for DFEV3 */
++#define SYS2_CLKSR_DFEV3 (0x1 << 3)
++#define SYS2_CLKSR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_CLKSR_DFEV3_GET(val) (((val) & SYS2_CLKSR_DFEV3) >> 3)
++/* Clock Enable for DFEV2 */
++#define SYS2_CLKSR_DFEV2 (0x1 << 2)
++#define SYS2_CLKSR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_CLKSR_DFEV2_GET(val) (((val) & SYS2_CLKSR_DFEV2) >> 2)
++/* Clock Enable for DFEV1 */
++#define SYS2_CLKSR_DFEV1 (0x1 << 1)
++#define SYS2_CLKSR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_CLKSR_DFEV1_GET(val) (((val) & SYS2_CLKSR_DFEV1) >> 1)
++/* Clock Enable for DFEV0 */
++#define SYS2_CLKSR_DFEV0 (0x1)
++#define SYS2_CLKSR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_CLKSR_DFEV0_GET(val) ((val) & SYS2_CLKSR_DFEV0)
++
++/*******************************************************************************
++ * SYS2 Clock Enable Register
++ ******************************************************************************/
++
++/* Clock Enable Request for PORT4 */
++#define SYS2_CLKENR_PORT4 (0x1 << 27)
++#define SYS2_CLKENR_PORT4_VAL(val) (((val) & 0x1) << 27)
++#define SYS2_CLKENR_PORT4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_PORT4) | ((val & 0x1) << 27))
++/* Clock Enable Request for HWSYNC */
++#define SYS2_CLKENR_HWSYNC (0x1 << 26)
++#define SYS2_CLKENR_HWSYNC_VAL(val) (((val) & 0x1) << 26)
++#define SYS2_CLKENR_HWSYNC_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_HWSYNC) | ((val & 0x1) << 26))
++/* Clock Enable Request for MBS */
++#define SYS2_CLKENR_MBS (0x1 << 25)
++#define SYS2_CLKENR_MBS_VAL(val) (((val) & 0x1) << 25)
++#define SYS2_CLKENR_MBS_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_MBS) | ((val & 0x1) << 25))
++/* Clock Enable Request for SWINT */
++#define SYS2_CLKENR_SWINT (0x1 << 24)
++#define SYS2_CLKENR_SWINT_VAL(val) (((val) & 0x1) << 24)
++#define SYS2_CLKENR_SWINT_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SWINT) | ((val & 0x1) << 24))
++/* Clock Enable Request for HWACC3 */
++#define SYS2_CLKENR_HWACC3 (0x1 << 19)
++#define SYS2_CLKENR_HWACC3_VAL(val) (((val) & 0x1) << 19)
++#define SYS2_CLKENR_HWACC3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_HWACC3) | ((val & 0x1) << 19))
++/* Clock Enable Request for HWACC2 */
++#define SYS2_CLKENR_HWACC2 (0x1 << 18)
++#define SYS2_CLKENR_HWACC2_VAL(val) (((val) & 0x1) << 18)
++#define SYS2_CLKENR_HWACC2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_HWACC2) | ((val & 0x1) << 18))
++/* Clock Enable Request for HWACC1 */
++#define SYS2_CLKENR_HWACC1 (0x1 << 17)
++#define SYS2_CLKENR_HWACC1_VAL(val) (((val) & 0x1) << 17)
++#define SYS2_CLKENR_HWACC1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_HWACC1) | ((val & 0x1) << 17))
++/* Clock Enable Request for HWACC0 */
++#define SYS2_CLKENR_HWACC0 (0x1 << 16)
++#define SYS2_CLKENR_HWACC0_VAL(val) (((val) & 0x1) << 16)
++#define SYS2_CLKENR_HWACC0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_HWACC0) | ((val & 0x1) << 16))
++/* Clock Enable Request for SIF7 */
++#define SYS2_CLKENR_SIF7 (0x1 << 15)
++#define SYS2_CLKENR_SIF7_VAL(val) (((val) & 0x1) << 15)
++#define SYS2_CLKENR_SIF7_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF7) | ((val & 0x1) << 15))
++/* Clock Enable Request for SIF6 */
++#define SYS2_CLKENR_SIF6 (0x1 << 14)
++#define SYS2_CLKENR_SIF6_VAL(val) (((val) & 0x1) << 14)
++#define SYS2_CLKENR_SIF6_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF6) | ((val & 0x1) << 14))
++/* Clock Enable Request for SIF5 */
++#define SYS2_CLKENR_SIF5 (0x1 << 13)
++#define SYS2_CLKENR_SIF5_VAL(val) (((val) & 0x1) << 13)
++#define SYS2_CLKENR_SIF5_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF5) | ((val & 0x1) << 13))
++/* Clock Enable Request for SIF4 */
++#define SYS2_CLKENR_SIF4 (0x1 << 12)
++#define SYS2_CLKENR_SIF4_VAL(val) (((val) & 0x1) << 12)
++#define SYS2_CLKENR_SIF4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF4) | ((val & 0x1) << 12))
++/* Clock Enable Request for SIF3 */
++#define SYS2_CLKENR_SIF3 (0x1 << 11)
++#define SYS2_CLKENR_SIF3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_CLKENR_SIF3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF3) | ((val & 0x1) << 11))
++/* Clock Enable Request for SIF2 */
++#define SYS2_CLKENR_SIF2 (0x1 << 10)
++#define SYS2_CLKENR_SIF2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_CLKENR_SIF2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF2) | ((val & 0x1) << 10))
++/* Clock Enable Request for SIF1 */
++#define SYS2_CLKENR_SIF1 (0x1 << 9)
++#define SYS2_CLKENR_SIF1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_CLKENR_SIF1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF1) | ((val & 0x1) << 9))
++/* Clock Enable Request for SIF0 */
++#define SYS2_CLKENR_SIF0 (0x1 << 8)
++#define SYS2_CLKENR_SIF0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_CLKENR_SIF0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_SIF0) | ((val & 0x1) << 8))
++/* Clock Enable Request for DFEV7 */
++#define SYS2_CLKENR_DFEV7 (0x1 << 7)
++#define SYS2_CLKENR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_CLKENR_DFEV7_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV7) | ((val & 0x1) << 7))
++/* Clock Enable Request for DFEV6 */
++#define SYS2_CLKENR_DFEV6 (0x1 << 6)
++#define SYS2_CLKENR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_CLKENR_DFEV6_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV6) | ((val & 0x1) << 6))
++/* Clock Enable Request for DFEV5 */
++#define SYS2_CLKENR_DFEV5 (0x1 << 5)
++#define SYS2_CLKENR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_CLKENR_DFEV5_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV5) | ((val & 0x1) << 5))
++/* Clock Enable Request for DFEV4 */
++#define SYS2_CLKENR_DFEV4 (0x1 << 4)
++#define SYS2_CLKENR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_CLKENR_DFEV4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV4) | ((val & 0x1) << 4))
++/* Clock Enable Request for DFEV3 */
++#define SYS2_CLKENR_DFEV3 (0x1 << 3)
++#define SYS2_CLKENR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_CLKENR_DFEV3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV3) | ((val & 0x1) << 3))
++/* Clock Enable Request for DFEV2 */
++#define SYS2_CLKENR_DFEV2 (0x1 << 2)
++#define SYS2_CLKENR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_CLKENR_DFEV2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV2) | ((val & 0x1) << 2))
++/* Clock Enable Request for DFEV1 */
++#define SYS2_CLKENR_DFEV1 (0x1 << 1)
++#define SYS2_CLKENR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_CLKENR_DFEV1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV1) | ((val & 0x1) << 1))
++/* Clock Enable Request for DFEV0 */
++#define SYS2_CLKENR_DFEV0 (0x1)
++#define SYS2_CLKENR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_CLKENR_DFEV0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKENR_DFEV0) | ((val & 0x1)))
++
++/*******************************************************************************
++ * SYS2 Clock Clear Register
++ ******************************************************************************/
++
++/* Clock Disable Request for PORT4 */
++#define SYS2_CLKCLR_PORT4 (0x1 << 27)
++#define SYS2_CLKCLR_PORT4_VAL(val) (((val) & 0x1) << 27)
++#define SYS2_CLKCLR_PORT4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_PORT4) | ((val & 0x1) << 27))
++/* Clock Disable Request for HWSYNC */
++#define SYS2_CLKCLR_HWSYNC (0x1 << 26)
++#define SYS2_CLKCLR_HWSYNC_VAL(val) (((val) & 0x1) << 26)
++#define SYS2_CLKCLR_HWSYNC_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_HWSYNC) | ((val & 0x1) << 26))
++/* Clock Disable Request for MBS */
++#define SYS2_CLKCLR_MBS (0x1 << 25)
++#define SYS2_CLKCLR_MBS_VAL(val) (((val) & 0x1) << 25)
++#define SYS2_CLKCLR_MBS_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_MBS) | ((val & 0x1) << 25))
++/* Clock Disable Request for SWINT */
++#define SYS2_CLKCLR_SWINT (0x1 << 24)
++#define SYS2_CLKCLR_SWINT_VAL(val) (((val) & 0x1) << 24)
++#define SYS2_CLKCLR_SWINT_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SWINT) | ((val & 0x1) << 24))
++/* Clock Disable Request for HWACC3 */
++#define SYS2_CLKCLR_HWACC3 (0x1 << 19)
++#define SYS2_CLKCLR_HWACC3_VAL(val) (((val) & 0x1) << 19)
++#define SYS2_CLKCLR_HWACC3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_HWACC3) | ((val & 0x1) << 19))
++/* Clock Disable Request for HWACC2 */
++#define SYS2_CLKCLR_HWACC2 (0x1 << 18)
++#define SYS2_CLKCLR_HWACC2_VAL(val) (((val) & 0x1) << 18)
++#define SYS2_CLKCLR_HWACC2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_HWACC2) | ((val & 0x1) << 18))
++/* Clock Disable Request for HWACC1 */
++#define SYS2_CLKCLR_HWACC1 (0x1 << 17)
++#define SYS2_CLKCLR_HWACC1_VAL(val) (((val) & 0x1) << 17)
++#define SYS2_CLKCLR_HWACC1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_HWACC1) | ((val & 0x1) << 17))
++/* Clock Disable Request for HWACC0 */
++#define SYS2_CLKCLR_HWACC0 (0x1 << 16)
++#define SYS2_CLKCLR_HWACC0_VAL(val) (((val) & 0x1) << 16)
++#define SYS2_CLKCLR_HWACC0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_HWACC0) | ((val & 0x1) << 16))
++/* Clock Disable Request for SIF7 */
++#define SYS2_CLKCLR_SIF7 (0x1 << 15)
++#define SYS2_CLKCLR_SIF7_VAL(val) (((val) & 0x1) << 15)
++#define SYS2_CLKCLR_SIF7_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF7) | ((val & 0x1) << 15))
++/* Clock Disable Request for SIF6 */
++#define SYS2_CLKCLR_SIF6 (0x1 << 14)
++#define SYS2_CLKCLR_SIF6_VAL(val) (((val) & 0x1) << 14)
++#define SYS2_CLKCLR_SIF6_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF6) | ((val & 0x1) << 14))
++/* Clock Disable Request for SIF5 */
++#define SYS2_CLKCLR_SIF5 (0x1 << 13)
++#define SYS2_CLKCLR_SIF5_VAL(val) (((val) & 0x1) << 13)
++#define SYS2_CLKCLR_SIF5_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF5) | ((val & 0x1) << 13))
++/* Clock Disable Request for SIF4 */
++#define SYS2_CLKCLR_SIF4 (0x1 << 12)
++#define SYS2_CLKCLR_SIF4_VAL(val) (((val) & 0x1) << 12)
++#define SYS2_CLKCLR_SIF4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF4) | ((val & 0x1) << 12))
++/* Clock Disable Request for SIF3 */
++#define SYS2_CLKCLR_SIF3 (0x1 << 11)
++#define SYS2_CLKCLR_SIF3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_CLKCLR_SIF3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF3) | ((val & 0x1) << 11))
++/* Clock Disable Request for SIF2 */
++#define SYS2_CLKCLR_SIF2 (0x1 << 10)
++#define SYS2_CLKCLR_SIF2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_CLKCLR_SIF2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF2) | ((val & 0x1) << 10))
++/* Clock Disable Request for SIF1 */
++#define SYS2_CLKCLR_SIF1 (0x1 << 9)
++#define SYS2_CLKCLR_SIF1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_CLKCLR_SIF1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF1) | ((val & 0x1) << 9))
++/* Clock Disable Request for SIF0 */
++#define SYS2_CLKCLR_SIF0 (0x1 << 8)
++#define SYS2_CLKCLR_SIF0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_CLKCLR_SIF0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_SIF0) | ((val & 0x1) << 8))
++/* Clock Disable Request for DFEV7 */
++#define SYS2_CLKCLR_DFEV7 (0x1 << 7)
++#define SYS2_CLKCLR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_CLKCLR_DFEV7_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV7) | ((val & 0x1) << 7))
++/* Clock Disable Request for DFEV6 */
++#define SYS2_CLKCLR_DFEV6 (0x1 << 6)
++#define SYS2_CLKCLR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_CLKCLR_DFEV6_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV6) | ((val & 0x1) << 6))
++/* Clock Disable Request for DFEV5 */
++#define SYS2_CLKCLR_DFEV5 (0x1 << 5)
++#define SYS2_CLKCLR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_CLKCLR_DFEV5_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV5) | ((val & 0x1) << 5))
++/* Clock Disable Request for DFEV4 */
++#define SYS2_CLKCLR_DFEV4 (0x1 << 4)
++#define SYS2_CLKCLR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_CLKCLR_DFEV4_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV4) | ((val & 0x1) << 4))
++/* Clock Disable Request for DFEV3 */
++#define SYS2_CLKCLR_DFEV3 (0x1 << 3)
++#define SYS2_CLKCLR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_CLKCLR_DFEV3_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV3) | ((val & 0x1) << 3))
++/* Clock Disable Request for DFEV2 */
++#define SYS2_CLKCLR_DFEV2 (0x1 << 2)
++#define SYS2_CLKCLR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_CLKCLR_DFEV2_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV2) | ((val & 0x1) << 2))
++/* Clock Disable Request for DFEV1 */
++#define SYS2_CLKCLR_DFEV1 (0x1 << 1)
++#define SYS2_CLKCLR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_CLKCLR_DFEV1_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV1) | ((val & 0x1) << 1))
++/* Clock Disable Request for DFEV0 */
++#define SYS2_CLKCLR_DFEV0 (0x1)
++#define SYS2_CLKCLR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_CLKCLR_DFEV0_SET (reg,val) (reg) = ((reg & ~SYS2_CLKCLR_DFEV0) | ((val & 0x1)))
++
++/*******************************************************************************
++ * SYS2 Reset Status Register
++ ******************************************************************************/
++
++/* HWACC3 Reset */
++#define SYS2_RSR_HWACC3 (0x1 << 11)
++#define SYS2_RSR_HWACC3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_RSR_HWACC3_GET(val) (((val) & SYS2_RSR_HWACC3) >> 11)
++/* HWACC2 Reset */
++#define SYS2_RSR_HWACC2 (0x1 << 10)
++#define SYS2_RSR_HWACC2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_RSR_HWACC2_GET(val) (((val) & SYS2_RSR_HWACC2) >> 10)
++/* HWACC1 Reset */
++#define SYS2_RSR_HWACC1 (0x1 << 9)
++#define SYS2_RSR_HWACC1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_RSR_HWACC1_GET(val) (((val) & SYS2_RSR_HWACC1) >> 9)
++/* HWACC0 Reset */
++#define SYS2_RSR_HWACC0 (0x1 << 8)
++#define SYS2_RSR_HWACC0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_RSR_HWACC0_GET(val) (((val) & SYS2_RSR_HWACC0) >> 8)
++/* DFEV7 Reset */
++#define SYS2_RSR_DFEV7 (0x1 << 7)
++#define SYS2_RSR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_RSR_DFEV7_GET(val) (((val) & SYS2_RSR_DFEV7) >> 7)
++/* DFEV6 Reset */
++#define SYS2_RSR_DFEV6 (0x1 << 6)
++#define SYS2_RSR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_RSR_DFEV6_GET(val) (((val) & SYS2_RSR_DFEV6) >> 6)
++/* DFEV5 Reset */
++#define SYS2_RSR_DFEV5 (0x1 << 5)
++#define SYS2_RSR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_RSR_DFEV5_GET(val) (((val) & SYS2_RSR_DFEV5) >> 5)
++/* DFEV4 Reset */
++#define SYS2_RSR_DFEV4 (0x1 << 4)
++#define SYS2_RSR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_RSR_DFEV4_GET(val) (((val) & SYS2_RSR_DFEV4) >> 4)
++/* DFEV3 Reset */
++#define SYS2_RSR_DFEV3 (0x1 << 3)
++#define SYS2_RSR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_RSR_DFEV3_GET(val) (((val) & SYS2_RSR_DFEV3) >> 3)
++/* DFEV2 Reset */
++#define SYS2_RSR_DFEV2 (0x1 << 2)
++#define SYS2_RSR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_RSR_DFEV2_GET(val) (((val) & SYS2_RSR_DFEV2) >> 2)
++/* DFEV1 Reset */
++#define SYS2_RSR_DFEV1 (0x1 << 1)
++#define SYS2_RSR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_RSR_DFEV1_GET(val) (((val) & SYS2_RSR_DFEV1) >> 1)
++/* DFEV0 Reset */
++#define SYS2_RSR_DFEV0 (0x1)
++#define SYS2_RSR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_RSR_DFEV0_GET(val) ((val) & SYS2_RSR_DFEV0)
++
++/******************************************************************************
++ * SYS2 Reset Request Register
++ ******************************************************************************/
++
++/* HWACC3 Reset Request */
++#define SYS2_RREQR_HWACC3 (0x1 << 11)
++#define SYS2_RREQR_HWACC3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_RREQR_HWACC3_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_HWACC3) | ((val & 0x1) << 11))
++/* HWACC2 Reset Request */
++#define SYS2_RREQR_HWACC2 (0x1 << 10)
++#define SYS2_RREQR_HWACC2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_RREQR_HWACC2_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_HWACC2) | ((val & 0x1) << 10))
++/* HWACC1 Reset Request */
++#define SYS2_RREQR_HWACC1 (0x1 << 9)
++#define SYS2_RREQR_HWACC1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_RREQR_HWACC1_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_HWACC1) | ((val & 0x1) << 9))
++/* HWACC0 Reset Request */
++#define SYS2_RREQR_HWACC0 (0x1 << 8)
++#define SYS2_RREQR_HWACC0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_RREQR_HWACC0_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_HWACC0) | ((val & 0x1) << 8))
++/* DFEV7 Reset Request */
++#define SYS2_RREQR_DFEV7 (0x1 << 7)
++#define SYS2_RREQR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_RREQR_DFEV7_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV7) | ((val & 0x1) << 7))
++/* DFEV6 Reset Request */
++#define SYS2_RREQR_DFEV6 (0x1 << 6)
++#define SYS2_RREQR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_RREQR_DFEV6_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV6) | ((val & 0x1) << 6))
++/* DFEV5 Reset Request */
++#define SYS2_RREQR_DFEV5 (0x1 << 5)
++#define SYS2_RREQR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_RREQR_DFEV5_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV5) | ((val & 0x1) << 5))
++/* DFEV4 Reset Request */
++#define SYS2_RREQR_DFEV4 (0x1 << 4)
++#define SYS2_RREQR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_RREQR_DFEV4_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV4) | ((val & 0x1) << 4))
++/* DFEV3 Reset Request */
++#define SYS2_RREQR_DFEV3 (0x1 << 3)
++#define SYS2_RREQR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_RREQR_DFEV3_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV3) | ((val & 0x1) << 3))
++/* DFEV2 Reset Request */
++#define SYS2_RREQR_DFEV2 (0x1 << 2)
++#define SYS2_RREQR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_RREQR_DFEV2_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV2) | ((val & 0x1) << 2))
++/* DFEV1 Reset Request */
++#define SYS2_RREQR_DFEV1 (0x1 << 1)
++#define SYS2_RREQR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_RREQR_DFEV1_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV1) | ((val & 0x1) << 1))
++/* DFEV0 Reset Request */
++#define SYS2_RREQR_DFEV0 (0x1)
++#define SYS2_RREQR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_RREQR_DFEV0_SET (reg,val) (reg) = ((reg & ~SYS2_RREQR_DFEV0) | ((val & 0x1)))
++
++/*******************************************************************************
++ * SYS2 Reset Release Register
++ ******************************************************************************/
++
++/* HWACC3 Reset Release */
++#define SYS2_RRLSR_HWACC3 (0x1 << 11)
++#define SYS2_RRLSR_HWACC3_VAL(val) (((val) & 0x1) << 11)
++#define SYS2_RRLSR_HWACC3_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_HWACC3) | ((val & 0x1) << 11))
++/* HWACC2 Reset Release */
++#define SYS2_RRLSR_HWACC2 (0x1 << 10)
++#define SYS2_RRLSR_HWACC2_VAL(val) (((val) & 0x1) << 10)
++#define SYS2_RRLSR_HWACC2_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_HWACC2) | ((val & 0x1) << 10))
++/* HWACC1 Reset Release */
++#define SYS2_RRLSR_HWACC1 (0x1 << 9)
++#define SYS2_RRLSR_HWACC1_VAL(val) (((val) & 0x1) << 9)
++#define SYS2_RRLSR_HWACC1_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_HWACC1) | ((val & 0x1) << 9))
++/* HWACC0 Reset Release */
++#define SYS2_RRLSR_HWACC0 (0x1 << 8)
++#define SYS2_RRLSR_HWACC0_VAL(val) (((val) & 0x1) << 8)
++#define SYS2_RRLSR_HWACC0_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_HWACC0) | ((val & 0x1) << 8))
++/* DFEV7 Reset Release */
++#define SYS2_RRLSR_DFEV7 (0x1 << 7)
++#define SYS2_RRLSR_DFEV7_VAL(val) (((val) & 0x1) << 7)
++#define SYS2_RRLSR_DFEV7_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV7) | ((val & 0x1) << 7))
++/* DFEV6 Reset Release */
++#define SYS2_RRLSR_DFEV6 (0x1 << 6)
++#define SYS2_RRLSR_DFEV6_VAL(val) (((val) & 0x1) << 6)
++#define SYS2_RRLSR_DFEV6_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV6) | ((val & 0x1) << 6))
++/* DFEV5 Reset Release */
++#define SYS2_RRLSR_DFEV5 (0x1 << 5)
++#define SYS2_RRLSR_DFEV5_VAL(val) (((val) & 0x1) << 5)
++#define SYS2_RRLSR_DFEV5_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV5) | ((val & 0x1) << 5))
++/* DFEV4 Reset Release */
++#define SYS2_RRLSR_DFEV4 (0x1 << 4)
++#define SYS2_RRLSR_DFEV4_VAL(val) (((val) & 0x1) << 4)
++#define SYS2_RRLSR_DFEV4_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV4) | ((val & 0x1) << 4))
++/* DFEV3 Reset Release */
++#define SYS2_RRLSR_DFEV3 (0x1 << 3)
++#define SYS2_RRLSR_DFEV3_VAL(val) (((val) & 0x1) << 3)
++#define SYS2_RRLSR_DFEV3_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV3) | ((val & 0x1) << 3))
++/* DFEV2 Reset Release */
++#define SYS2_RRLSR_DFEV2 (0x1 << 2)
++#define SYS2_RRLSR_DFEV2_VAL(val) (((val) & 0x1) << 2)
++#define SYS2_RRLSR_DFEV2_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV2) | ((val & 0x1) << 2))
++/* DFEV1 Reset Release */
++#define SYS2_RRLSR_DFEV1 (0x1 << 1)
++#define SYS2_RRLSR_DFEV1_VAL(val) (((val) & 0x1) << 1)
++#define SYS2_RRLSR_DFEV1_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV1) | ((val & 0x1) << 1))
++/* DFEV0 Reset Release */
++#define SYS2_RRLSR_DFEV0 (0x1)
++#define SYS2_RRLSR_DFEV0_VAL(val) (((val) & 0x1))
++#define SYS2_RRLSR_DFEV0_SET (reg,val) (reg) = ((reg & ~SYS2_RRLSR_DFEV0) | ((val & 0x1)))
++
++#endif /* __SYS2_H */
++
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/sysctrl.h b/arch/mips/include/asm/mach-lantiq/svip/sysctrl.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/sysctrl.h
+@@ -0,0 +1,25 @@
++/*
++ * This program is free software; you can redistribute it and/or
++ * modify it under the terms of the GNU General Public License as
++ * published by the Free Software Foundation; either version 2 of
++ * the License, or (at your option) any later version.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program; if not, write to the Free Software
++ * Foundation, Inc., 59 Temple Place, Suite 330, Boston,
++ * MA 02111-1307 USA
++ *
++ * Copyright (C) 2010 Thomas Langer, Lantiq Deutschland
++ */
++
++#ifndef __SVIP_SYSCTRL_H
++#define __SVIP_SYSCTRL_H
++
++#include <svip/lantiq_soc.h>
++
++#endif /* __SVIP_SYSCTRL_H */
+diff --git a/arch/mips/lantiq/Kconfig b/arch/mips/lantiq/Kconfig
+--- a/arch/mips/lantiq/Kconfig
++++ b/arch/mips/lantiq/Kconfig
+@@ -24,6 +24,11 @@ config SOC_FALCON
+ 	bool "FALCON"
+ 	select PINCTRL_FALCON
+ 
++config SOC_SVIP
++	bool "SVIP"
++	select PINCTRL_SVIP
++	select SYS_SUPPORTS_LITTLE_ENDIAN
++
+ endchoice
+ 
+ choice
+diff --git a/arch/mips/lantiq/Makefile b/arch/mips/lantiq/Makefile
+--- a/arch/mips/lantiq/Makefile
++++ b/arch/mips/lantiq/Makefile
+@@ -11,3 +11,4 @@ obj-$(CONFIG_VPE_SOFTDOG) += softdog_vpe
+ 
+ obj-$(CONFIG_SOC_TYPE_XWAY) += xway/
+ obj-$(CONFIG_SOC_FALCON) += falcon/
++obj-$(CONFIG_SOC_SVIP) += svip/
+diff --git a/arch/mips/lantiq/Platform b/arch/mips/lantiq/Platform
+--- a/arch/mips/lantiq/Platform
++++ b/arch/mips/lantiq/Platform
+@@ -7,3 +7,4 @@ cflags-$(CONFIG_LANTIQ)		+= -I$(srctree)
+ load-$(CONFIG_LANTIQ)		= 0xffffffff80002000
+ cflags-$(CONFIG_SOC_TYPE_XWAY)	+= -I$(srctree)/arch/mips/include/asm/mach-lantiq/xway
+ cflags-$(CONFIG_SOC_FALCON)	+= -I$(srctree)/arch/mips/include/asm/mach-lantiq/falcon
++cflags-$(CONFIG_SOC_SVIP)	+= -I$(srctree)/arch/mips/include/asm/mach-lantiq/svip
+diff --git a/arch/mips/lantiq/irq.c b/arch/mips/lantiq/irq.c
+--- a/arch/mips/lantiq/irq.c
++++ b/arch/mips/lantiq/irq.c
+@@ -21,6 +21,7 @@
+ 
+ #include <lantiq_soc.h>
+ #include <irq.h>
++#include <svip/ebu_reg.h>
+ 
+ /* register definitions - internal irqs */
+ #define LTQ_ICU_IM0_ISR		0x0000
+@@ -38,7 +39,7 @@
+ #define LTQ_EIU_EXIN_INEN	0x000C
+ 
+ /* number of external interrupts */
+-#define MAX_EIU			6
++#define MAX_EIU			17
+ 
+ /* the performance counter */
+ #define LTQ_PERF_IRQ		(INT_NUM_IM4_IRL0 + 31)
+diff --git a/arch/mips/lantiq/svip/Makefile b/arch/mips/lantiq/svip/Makefile
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/Makefile
+@@ -0,0 +1,1 @@
++obj-y := prom.o reset.o sysctrl.o
+diff --git a/arch/mips/lantiq/svip/prom.c b/arch/mips/lantiq/svip/prom.c
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/prom.c
+@@ -0,0 +1,86 @@
++/*
++ * This program is free software; you can redistribute it and/or modify it
++ * under the terms of the GNU General Public License version 2 as published
++ * by the Free Software Foundation.
++ *
++ * Copyright (C) 2012 Thomas Langer <thomas.langer@lantiq.com>
++ * Copyright (C) 2012 John Crispin <blogic@openwrt.org>
++ */
++
++#include <linux/kernel.h>
++#include <asm/cacheflush.h>
++#include <asm/traps.h>
++#include <asm/io.h>
++#include <asm/bootinfo.h>
++
++#include <lantiq_soc.h>
++
++#include "../prom.h"
++
++#define SOC_SVIP	"SVIP"
++#define SOC_SVIP_D	"SVIP-D"
++
++#define COMP_SVIP	"lantiq,svip"
++
++/* reset, nmi and ejtag exception vectors */
++/*
++#define BOOT_REG_BASE	(KSEG1 | 0x1F200000)
++#define BOOT_RVEC	(BOOT_REG_BASE | 0x00)
++#define BOOT_NVEC	(BOOT_REG_BASE | 0x04)
++#define BOOT_EVEC	(BOOT_REG_BASE | 0x08)
++*/
++/*
++void __init ltq_soc_nmi_setup(void)
++{
++
++	extern void (*nmi_handler)(void);
++
++	ltq_w32((unsigned long)&nmi_handler, (void *)BOOT_NVEC);
++}
++*/
++/*
++void __init ltq_soc_ejtag_setup(void)
++{
++	extern void (*ejtag_debug_handler)(void);
++
++	ltq_w32((unsigned long)&ejtag_debug_handler, (void *)BOOT_EVEC);
++}
++*/
++void __init ltq_soc_detect(struct ltq_soc_info *i)
++{
++	u32 fuse, chipid;
++	struct cpuinfo_mips *c = &current_cpu_data;
++	/* disable L2 cache, because it is in use by FW !*/
++	c->scache.flags |= MIPS_CACHE_NOT_PRESENT;
++
++	chipid = ltq_r32(SVIP_CHIPID);
++	fuse = ltq_r32(SVIP_FUSE);
++	i->partnum = STATUS_CHIPID_PART_NUMBER_GET(chipid);
++	i->rev = STATUS_CHIPID_VERSION_GET(chipid);
++	i->compatible = COMP_SVIP;
++	i->type = SOC_TYPE_SVIP;
++	sprintf(i->rev_type, "1.%d", i->rev);
++	switch (i->partnum) {
++	case SOC_ID_SVIP:
++	case SOC_ID_SVIPx:
++		if (STATUS_FUSE_DEU_DEU_GET(fuse))
++			i->name = SOC_SVIP_D;
++		else
++			i->name = SOC_SVIP;
++		break;
++	default:
++		printk(KERN_ERR "unknown partnum : 0x%08X\n", i->partnum);
++		unreachable();
++		break;
++	}
++/*
++	board_nmi_handler_setup = ltq_soc_nmi_setup;
++	board_ejtag_handler_setup = ltq_soc_ejtag_setup;
++*/
++}
++
++unsigned int* ltq_get_cp1_base(void)
++{
++	return (unsigned int *)KSEG1ADDR(boot_mem_map.map[0].size);
++}
++EXPORT_SYMBOL(ltq_get_cp1_base);
+diff --git a/arch/mips/lantiq/svip/reset.c b/arch/mips/lantiq/svip/reset.c
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/reset.c
+@@ -0,0 +1,67 @@
++/*
++ * This program is free software; you can redistribute it and/or modify it
++ * under the terms of the GNU General Public License version 2 as published
++ * by the Free Software Foundation.
++ *
++ * Copyright (C) 2012 Thomas Langer <thomas.langer@lantiq.com>
++ * Copyright (C) 2012 John Crispin <blogic@openwrt.org>
++ */
++
++#include <linux/init.h>
++#include <linux/io.h>
++#include <linux/pm.h>
++#include <asm/reboot.h>
++#include <linux/export.h>
++
++#include <lantiq_soc.h>
++
++static struct svip_reg_sys1 *const sys1 = (struct svip_reg_sys1 *)LTQ_SYS1_BASE_ADDR;
++/*
++static struct svip_reg_ebu *const ebu = (struct svip_reg_ebu *)LTQ_EBU_BASE;
++*/
++
++
++/* to avoid problems with the gcc "__builtin_unreachable", change definition here! */
++#undef unreachable
++#define unreachable()	do { } while(1)
++
++/* allow platform code to find out what surce we booted from */
++unsigned char ltq_boot_select(void)
++{
++	return 0/* ltq_sys1_r32(SYS1_BM) & BM_MASK */;
++}
++
++/* allow the watchdog driver to find out what the boot reason was */
++int ltq_reset_cause(void)
++{
++	return 0 /*ltq_sys1_r32(SYS1_CPU0RS) & CPU0RS_MASK*/;
++} 
++
++static void machine_restart(char *command)
++{
++	printk(KERN_NOTICE "System restart not implemented yet\n");
++}
++
++static void machine_halt(void)
++{
++	printk(KERN_NOTICE "System halted.\n");
++	local_irq_disable();
++	unreachable();
++}
++
++static void machine_power_off(void)
++{
++	printk(KERN_NOTICE "Please turn off the power now.\n");
++	local_irq_disable();
++	unreachable();
++}
++
++static int __init mips_reboot_setup(void)
++{
++	_machine_restart = machine_restart;
++	_machine_halt = machine_halt;
++	pm_power_off = machine_power_off;
++	return 0;
++}
++
++arch_initcall(mips_reboot_setup);
+diff --git a/arch/mips/lantiq/svip/sysctrl.c b/arch/mips/lantiq/svip/sysctrl.c
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/sysctrl.c
+@@ -0,0 +1,238 @@
++/*
++ * This program is free software; you can redistribute it and/or modify it
++ * under the terms of the GNU General Public License version 2 as published
++ * by the Free Software Foundation.
++ *
++ * Copyright (C) 2011 Thomas Langer <thomas.langer@lantiq.com>
++ * Copyright (C) 2011 John Crispin <blogic@openwrt.org>
++ */
++
++#include <linux/ioport.h>
++#include <linux/export.h>
++#include <linux/clkdev.h>
++#include <linux/of_address.h>
++#include <linux/spinlock.h>
++#include <svip/sysctrl.h>
++#include <asm/delay.h>
++
++#include <lantiq_soc.h>
++/*
++static struct svip_reg_status *const status =
++(struct svip_reg_status *)LTQ_STATUS_BASE_ADDR;
++static struct svip_reg_sys0 *const sys0 = (struct svip_reg_sys0 *)LTQ_SYS0_BASE_ADDR;
++static struct svip_reg_sys1 *const sys1 = (struct svip_reg_sys1 *)LTQ_SYS1_BASE_ADDR;
++*/
++#include "../clk.h"
++
++#define sysctl_w32(m, x, y)	ltq_w32((x), sysctl_membase[m] + (y))
++#define sysctl_r32(m, x)	ltq_r32(sysctl_membase[m] + (x))
++#define sysctl_w32_mask(m, clear, set, reg)	\
++		sysctl_w32(m, (sysctl_r32(m, reg) & ~(clear)) | (set), reg)
++
++#define status_w32(x, y)	ltq_w32((x), status_membase + (y))
++#define status_r32(x)		ltq_r32(status_membase + (x))
++
++static void __iomem *sysctl_membase[SYSCTL_NUM_OF_SYS], *status_membase;
++void __iomem *ltq_sys1_membase, *ltq_ebu_membase;
++
++static inline void sysctl_wait(struct clk *clk,
++		unsigned int test, unsigned int reg)
++{
++	int err = 1000000;
++
++	do {} while (--err && ((sysctl_r32(clk->module, reg)
++					& clk->bits) != test));
++	if (!err)
++		pr_err("module de/activation failed %d %08X %08X %08X\n",
++			clk->module, clk->bits, test,
++			sysctl_r32(clk->module, reg) & clk->bits);
++}
++
++static int sysctl_activate(struct clk *clk)
++{
++	if (clk->module == SYSCTL_SYS1 || clk->module == SYSCTL_SYS2) {
++		sysctl_w32(clk->module, clk->bits, SYS1_CLKENR);
++		sysctl_wait(clk, clk->bits, SYS1_CLKSR);
++	}
++	else
++		pr_err("sysctl_activate: only SYSCTL_SYS1 and SYSCTL_SYS2 are supported");
++	return 0;
++}
++
++static void sysctl_deactivate(struct clk *clk)
++{
++	if (clk->module == SYSCTL_SYS1 || clk->module == SYSCTL_SYS2) {
++		sysctl_w32(clk->module, clk->bits, SYS1_CLKCLR);
++		sysctl_wait(clk, 0, SYS1_CLKSR);
++	}
++	else
++		pr_err("sysctl_deactivate: only SYSCTL_SYS1 and SYSCTL_SYS2 are supported");
++}
++
++static int sysctl_clken(struct clk *clk)
++{
++	if (clk->module == SYSCTL_SYS1 || clk->module == SYSCTL_SYS2) {
++		sysctl_w32(clk->module, clk->bits, SYS1_CLKENR);
++		sysctl_wait(clk, clk->bits, SYS1_CLKSR);
++	}
++	else
++		pr_err("sysctl_clken: only SYSCTL_SYS1 and SYSCTL_SYS2 are supported");
++	return 0;
++}
++
++static void sysctl_clkdis(struct clk *clk)
++{
++	if (clk->module == SYSCTL_SYS1 || clk->module == SYSCTL_SYS2) {
++		sysctl_w32(clk->module, clk->bits, SYS1_CLKCLR);
++		sysctl_wait(clk, 0, SYS1_CLKSR);
++	}
++	else
++		pr_err("sysctl_clkdis: only SYSCTL_SYS1 and SYSCTL_SYS2 are supported");
++}
++
++static void sysctl_reboot(struct clk *clk)
++{
++/*
++	unsigned int act;
++	unsigned int bits;
++
++	act = sysctl_r32(clk->module, SYSCTL_ACT);
++	bits = ~act & clk->bits;
++	if (bits != 0) {
++		sysctl_w32(clk->module, bits, SYSCTL_CLKEN);
++		sysctl_w32(clk->module, bits, SYSCTL_ACT);
++		sysctl_wait(clk, bits, SYSCTL_ACTS);
++	}
++	sysctl_w32(clk->module, act & clk->bits, SYSCTL_RBT);
++	sysctl_wait(clk, clk->bits, SYSCTL_ACTS);
++*/
++}
++
++static inline void clkdev_add_sys(const char *dev, unsigned int module,
++					unsigned int bits)
++{
++	struct clk *clk = kzalloc(sizeof(struct clk), GFP_KERNEL);
++
++	clk->cl.dev_id = dev;
++	clk->cl.con_id = NULL;
++	clk->cl.clk = clk;
++	clk->module = module;
++	clk->bits = bits;
++	clk->activate = sysctl_activate;
++	clk->deactivate = sysctl_deactivate;
++	clk->enable = sysctl_clken;
++	clk->disable = sysctl_clkdis;
++	clk->reboot = sysctl_reboot;
++	clkdev_add(&clk->cl);
++}
++
++static unsigned int svip_cpu_hz(void)
++{
++	/* Magic BootROM speed location... */
++	if ((*(u32 *)0x9fc07ff0) == 1)
++		return *(u32 *)0x9fc07ff4;
++
++	if (STATUS_CONFIG_CLK_MODE_GET(status_r32(STATUS_CONFIG)) == 1) {
++		/* xT16 Switch-IP clock mode */
++		return 393216000;
++	} else {
++		switch (SYS0_PLL1CR_PLLDIV_GET(sysctl_r32(SYSCTL_SYS0,SYS0_PLL1CR))) {
++		case 3:
++			return 475000000;
++		case 2:
++			return 450000000;
++		case 1:
++			return 425000000;
++		default:
++			return 400000000;
++		}
++	}
++}
++
++static unsigned int svip_fpi_hz(void)
++{
++	u32 fbs0_div[2] = {4, 8};
++	u32 div;
++
++	div = SYS1_FPICR_FPIDIV_GET(sysctl_r32(SYSCTL_SYS1,SYS1_FPICR));
++	return svip_cpu_hz()/fbs0_div[div];
++}
++
++static unsigned int svip_io_region_clock(void)
++{
++	return 200000000; /* 200 MHz */
++}
++
++void __init ltq_soc_init(void)
++{
++	struct device_node *np_status =
++		of_find_compatible_node(NULL, NULL, "lantiq,status-svip");
++	struct device_node *np_ebu =
++		of_find_compatible_node(NULL, NULL, "lantiq,ebu-svip");
++	struct device_node *np_sys0 =
++		of_find_compatible_node(NULL, NULL, "lantiq,sys0-svip");
++	struct device_node *np_sys1 =
++		of_find_compatible_node(NULL, NULL, "lantiq,sys1-svip");
++	struct device_node *np_sys2 =
++		of_find_compatible_node(NULL, NULL, "lantiq,sys2-svip");
++	struct resource res_status, res_ebu, res_sys[SYSCTL_NUM_OF_SYS];
++	int i;
++
++	/* check if all the core register ranges are available */
++	if (!np_status || !np_ebu || !np_sys0 || !np_sys1 || !np_sys2)
++		panic("Failed to load core nodes from devicetree");
++
++	if (of_address_to_resource(np_status, 0, &res_status) ||
++			of_address_to_resource(np_ebu, 0, &res_ebu) ||
++			of_address_to_resource(np_sys0, 0, &res_sys[SYSCTL_SYS0]) ||
++			of_address_to_resource(np_sys1, 0, &res_sys[SYSCTL_SYS1]) ||
++			of_address_to_resource(np_sys2, 0, &res_sys[SYSCTL_SYS2]))
++		panic("Failed to get core resources");
++
++	if ((request_mem_region(res_status.start, resource_size(&res_status),
++				res_status.name) < 0) ||
++		(request_mem_region(res_ebu.start, resource_size(&res_ebu),
++				res_ebu.name) < 0) ||
++		(request_mem_region(res_sys[SYSCTL_SYS0].start,
++				resource_size(&res_sys[SYSCTL_SYS0]),
++				res_sys[SYSCTL_SYS0].name) < 0) ||
++		(request_mem_region(res_sys[SYSCTL_SYS1].start,
++				resource_size(&res_sys[SYSCTL_SYS1]),
++				res_sys[SYSCTL_SYS1].name) < 0) ||
++		(request_mem_region(res_sys[SYSCTL_SYS2].start,
++				resource_size(&res_sys[SYSCTL_SYS2]),
++				res_sys[SYSCTL_SYS2].name) < 0))
++		pr_err("Failed to request core resources");
++
++	status_membase = ioremap_nocache(res_status.start,
++					resource_size(&res_status));
++	ltq_ebu_membase = ioremap_nocache(res_ebu.start,
++					resource_size(&res_ebu));
++
++	if (!status_membase || !ltq_ebu_membase)
++		panic("Failed to remap core resources");
++
++	for (i = 0; i < SYSCTL_NUM_OF_SYS; i++) {
++		sysctl_membase[i] = ioremap_nocache(res_sys[i].start,
++						resource_size(&res_sys[i]));
++		if (!sysctl_membase[i])
++			panic("Failed to remap sysctrl resources");
++	}
++	ltq_sys1_membase = sysctl_membase[SYSCTL_SYS1];
++
++	clkdev_add_static(svip_cpu_hz(), svip_fpi_hz(),
++		svip_io_region_clock(), 0);
++	clkdev_add_sys("14100100.serial", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
++	clkdev_add_sys("14100200.serial", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
++}
++
++/*
++ * for compatibility to external drivers from Lantiq
++ * see arch/mips/include/asm/mach-lantiq/svip/sysctrl.h
++ */
++void ltq_sysctl_chipid_get(unsigned int *chipid)
++{
++	if (chipid)
++		*chipid = ltq_r32(SVIP_CHIPID);
++}
++EXPORT_SYMBOL(ltq_sysctl_chipid_get);
diff --git a/target/linux/lantiq/patches-3.10/2201-svip-add-nand.patch b/target/linux/lantiq/patches-3.10/2201-svip-add-nand.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2201-svip-add-nand.patch
@@ -0,0 +1,229 @@
+# HG changeset patch
+# Parent c924add7eea811231c2ec88a90f7ba68d05bae61
+[mq]: 2201-svip-add-nand.patch
+
+diff --git a/drivers/mtd/nand/Kconfig b/drivers/mtd/nand/Kconfig
+--- a/drivers/mtd/nand/Kconfig
++++ b/drivers/mtd/nand/Kconfig
+@@ -552,4 +552,12 @@ config MTD_NAND_FALCON
+ 	  Enables support for NAND Flash chips on Lantiq FALC-ON SoCs. NAND is
+ 	  attached to the External Bus Unit (EBU).
+ 
++config MTD_NAND_SVIP
++	tristate "Support for NAND on Lantiq SVIP SoC"
++	depends on LANTIQ && SOC_SVIP
++	select MTD_NAND_PLATFORM
++	help
++	  Enables support for NAND Flash chips on Lantiq SVIP SoCs. NAND is
++	  attached to the External Bus Unit (EBU).
++
+ endif # MTD_NAND
+diff --git a/drivers/mtd/nand/Makefile b/drivers/mtd/nand/Makefile
+--- a/drivers/mtd/nand/Makefile
++++ b/drivers/mtd/nand/Makefile
+@@ -51,5 +51,6 @@ obj-$(CONFIG_MTD_NAND_GPMI_NAND)	+= gpmi
+ obj-$(CONFIG_MTD_NAND_XWAY)		+= xway_nand.o
+ obj-$(CONFIG_MTD_NAND_BCM47XXNFLASH)	+= bcm47xxnflash/
+ obj-$(CONFIG_MTD_NAND_FALCON)		+= falcon_nand.o
++obj-$(CONFIG_MTD_NAND_SVIP)		+= svip_nand.o
+ 
+ nand-objs := nand_base.o nand_bbt.o
+diff --git a/drivers/mtd/nand/svip_nand.c b/drivers/mtd/nand/svip_nand.c
+new file mode 100644
+--- /dev/null
++++ b/drivers/mtd/nand/svip_nand.c
+@@ -0,0 +1,194 @@
++/*
++ *  This program is free software; you can redistribute it and/or modify it
++ *  under the terms of the GNU General Public License version 2 as published
++ *  by the Free Software Foundation.
++ *
++ *  Copyright © 2014 Dmitrijs Belijs <dmitrijs.belijs@lantiq.com>
++ */
++
++#include <linux/mtd/nand.h>
++#include <linux/mtd/nand_ecc.h>
++#include <linux/of_gpio.h>
++#include <linux/of_platform.h>
++
++#include <lantiq_soc.h>
++
++#define EBU_ADDR_SEL_0		0x0020
++#define EBU_CON_0		0x0060
++#define EBU_NAND_CON		0x00B0
++#define EBU_NAND_WAIT		0x00B4
++#define EBU_NAND_ECC0		0x00B8
++
++#define NAND_WAIT_RD		0x1
++#define NAND_NOPS		150
++#define NAND_CLE_BIT		(1 << 3)
++#define NAND_ALE_BIT		(1 << 2)
++
++#define EBU_ADDR_SEL_0_MASK_VAL(val)   (((val) & 0xf) << 4)
++#define EBU_ADDR_SEL_0_REGEN_VAL(val)   (((val) & 0x1) << 0)
++#define EBU_CON_0_ADSWP_VAL(val)   (((val) & 0x1) << 30)
++#define EBU_CON_0_SETUP_VAL(val)   (((val) & 0x1) << 22)
++#define EBU_CON_0_BCGEN_VAL(val)   (((val) & 0x3) << 12)
++#define EBU_CON_0_WAITWRC_VAL(val)   (((val) & 0x7) << 8)
++#define EBU_CON_0_WAITRDC_VAL(val)   (((val) & 0x3) << 6)
++#define EBU_CON_0_HOLDC_VAL(val)   (((val) & 0x3) << 4)
++#define EBU_CON_0_CMULT_VAL(val)   (((val) & 0x3) << 0)
++#define EBU_NAND_CON_LAT_EN_VAL(val)   (((val) & 0x3f) << 18)
++#define EBU_NAND_CON_PRE_P_VAL(val)   (((val) & 0x1) << 7)
++#define EBU_NAND_CON_WP_P_VAL(val)   (((val) & 0x1) << 6)
++#define EBU_NAND_CON_SE_P_VAL(val)   (((val) & 0x1) << 5)
++#define EBU_NAND_CON_CS_P_VAL(val)   (((val) & 0x1) << 4)
++#define EBU_NAND_CON_NANDMODE_VAL(val)   (((val) & 0x1) << 0)
++#define EBU_NAND_CON_ECC_ON_VAL(val)   (((val) & 0x1) << 31)
++
++#define GET_BITS(x, msb, lsb)           (((x) & ((1 << ((msb) + 1)) - 1)) >> (lsb))
++
++static int f_ecc_write = 0;
++
++static void svip_cmd_ctrl(struct mtd_info *mtd, int cmd, unsigned int ctrl)
++{
++	struct nand_chip *this = mtd->priv;
++	if (ctrl & NAND_CTRL_CHANGE) {
++		unsigned long nandaddr = (unsigned long)this->IO_ADDR_W;
++		/* coming here means to change either the enable state or the address for
++		controlling ALE or CLE */
++		/* NAND_NCE: Select the chip by setting nCE to low. This is done in CON register */
++		if (ctrl & NAND_NCE)
++			ltq_ebu_w32_mask(0, EBU_NAND_CON_NANDMODE_VAL(1), EBU_NAND_CON);
++		else
++			ltq_ebu_w32_mask(EBU_NAND_CON_NANDMODE_VAL(1), 0, EBU_NAND_CON);
++		nandaddr &= ~(NAND_CLE_BIT | NAND_ALE_BIT);
++		nandaddr |= (ctrl & NAND_CLE) << 2 | (ctrl & NAND_ALE);
++		this->IO_ADDR_W = (void __iomem *)nandaddr;
++	}
++	if (cmd != NAND_CMD_NONE)
++		writeb(cmd, this->IO_ADDR_W);
++}
++
++static int svip_dev_ready(struct mtd_info *mtd)
++{
++	return ltq_ebu_r32(EBU_NAND_WAIT) & NAND_WAIT_RD;
++}
++
++static void svip_read_buf(struct mtd_info *mtd, u_char *buf, int len)
++{
++	int i, j;
++	struct nand_chip *this = mtd->priv;
++
++	for (i=0; i<len; i++){
++		buf[i] = readb(this->IO_ADDR_R);
++		for (j=0; j<NAND_NOPS; j++) {
++		/*insert some NOPs to let the FW still handle interrupts, 
++		otherwise the NAND access to the EBU would block 
++		the other cores too long when they also access the EBU */
++			asm("nop");
++		}
++	}
++}
++
++static void svip_write_buf(struct mtd_info *mtd, const u_char *buf, int len)
++{
++	int i, j;
++	struct nand_chip *this = mtd->priv;
++
++	for (i=0; i<len; i++){
++		writeb(buf[i], this->IO_ADDR_W);
++		for (j=0; j<NAND_NOPS; j++) {
++		/*insert some NOPs to let the FW still handle interrupts, 
++		otherwise the NAND access to the EBU would block 
++		the other cores too long when they also access the EBU */
++			asm("nop"); 
++		}
++	}
++}
++
++static int svip_calculate_ecc(struct mtd_info *mtd, const u_char *dat, u_char *ecc_code)
++{
++	if ( f_ecc_write ) {
++		nand_calculate_ecc(mtd, dat, ecc_code);
++	}
++	else {
++		unsigned int hwecc_result_val = ltq_ebu_r32(EBU_NAND_ECC0);
++		ecc_code[0] = GET_BITS(hwecc_result_val, 7, 0);
++		ecc_code[1] = GET_BITS(hwecc_result_val, 15, 8);
++		ecc_code[2] = GET_BITS(hwecc_result_val, 23, 16);
++		ltq_ebu_w32_mask(EBU_NAND_CON_ECC_ON_VAL(1), 0, EBU_NAND_CON);
++	}
++	return 0;
++}
++
++static void svip_enable_hwecc(struct mtd_info *mtd, int mode)
++{
++	switch ( mode ) {
++	case NAND_ECC_READ:
++		f_ecc_write = 0;
++		ltq_ebu_w32(0, EBU_NAND_ECC0);
++		ltq_ebu_w32_mask(0, EBU_NAND_CON_ECC_ON_VAL(1), EBU_NAND_CON);
++		break;
++	case NAND_ECC_WRITE:
++		f_ecc_write = 1;
++		break;
++	}
++}
++
++static int svip_nand_probe(struct platform_device *pdev)
++{
++	struct nand_chip *this = platform_get_drvdata(pdev);
++	unsigned long nandaddr = (unsigned long) this->IO_ADDR_W;
++
++	/* setup the EBU to run in NAND mode on our base addr */
++	ltq_ebu_w32(CPHYSADDR(nandaddr)
++		| EBU_ADDR_SEL_0_MASK_VAL(0xF) | EBU_ADDR_SEL_0_REGEN_VAL(1), EBU_ADDR_SEL_0);
++	ltq_ebu_w32(EBU_CON_0_ADSWP_VAL(1) | EBU_CON_0_SETUP_VAL(1) | EBU_CON_0_BCGEN_VAL(0x01)
++			| EBU_CON_0_WAITWRC_VAL(1) | EBU_CON_0_WAITRDC_VAL(1) | EBU_CON_0_HOLDC_VAL(1)
++			| EBU_CON_0_CMULT_VAL(0x01), EBU_CON_0);
++	ltq_ebu_w32(EBU_NAND_CON_LAT_EN_VAL(0x38)
++			| EBU_NAND_CON_PRE_P_VAL(1) | EBU_NAND_CON_WP_P_VAL(1)
++			| EBU_NAND_CON_SE_P_VAL(1) | EBU_NAND_CON_CS_P_VAL(1), EBU_NAND_CON);
++	this->ecc.calculate = svip_calculate_ecc;
++	this->ecc.mode = NAND_ECC_SOFT;
++	this->ecc.hwctl = svip_enable_hwecc;
++	return 0;
++}
++
++/* allow users to override the partition in DT using the cmdline */
++static const char *part_probes[] = { "cmdlinepart", "ofpart", NULL };
++
++static struct platform_nand_data svip_nand_data = {
++	.chip = {
++		.nr_chips		= 1,
++		.chip_delay		= 30,
++		.part_probe_types	= part_probes,
++	},
++	.ctrl = {
++		.probe		= svip_nand_probe,
++		.cmd_ctrl	= svip_cmd_ctrl,
++		.dev_ready	= svip_dev_ready,
++		.read_buf	= svip_read_buf,
++		.write_buf	= svip_write_buf,
++	}
++};
++
++/*
++ * Try to find the node inside the DT. If it is available attach out
++ * platform_nand_data
++ */
++static int __init svip_register_nand(void)
++{
++	struct device_node *node;
++	struct platform_device *pdev;
++
++	node = of_find_compatible_node(NULL, NULL, "lantiq,nand-svip");
++	if (!node) {
++		return -ENOENT;
++	}
++	pdev = of_find_device_by_node(node);
++	if (!pdev) {
++		return -EINVAL;
++	}
++	pdev->dev.platform_data = &svip_nand_data;
++	of_node_put(node);
++	return 0;
++}
++
++subsys_initcall(svip_register_nand);
diff --git a/target/linux/lantiq/patches-3.10/2203-svip-add-reboot.patch b/target/linux/lantiq/patches-3.10/2203-svip-add-reboot.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2203-svip-add-reboot.patch
@@ -0,0 +1,114 @@
+# HG changeset patch
+# Parent ad0dcc4edfa66e049ee841a695b8f97e5f74ff03
+[mq]: 2203-svip-add-reboot.patch
+
+diff --git a/arch/mips/lantiq/svip/reset.c b/arch/mips/lantiq/svip/reset.c
+--- a/arch/mips/lantiq/svip/reset.c
++++ b/arch/mips/lantiq/svip/reset.c
+@@ -12,34 +12,65 @@
+ #include <linux/pm.h>
+ #include <asm/reboot.h>
+ #include <linux/export.h>
+-
++#include <linux/of.h>
++#include <linux/of_address.h>
+ #include <lantiq_soc.h>
+ 
+ static struct svip_reg_sys1 *const sys1 = (struct svip_reg_sys1 *)LTQ_SYS1_BASE_ADDR;
+-/*
+-static struct svip_reg_ebu *const ebu = (struct svip_reg_ebu *)LTQ_EBU_BASE;
+-*/
++static unsigned long svip_board_reset_reg_addr = 0;
++static unsigned long svip_board_reset_value = 0;
+ 
++#define EBU_ADDR_SEL_2	0x0028
++#define EBU_CON_2	0x0068
++#define L2_SPRAM_BASE 0x1F1E8000
++#define SYS1_RREQR 0x0044
++#define SYS1_RRLSR 0x0048
++#define SYS1_CLKCLR 0x0008
++#define SYS1_RBTR 0x004C
+ 
+ /* to avoid problems with the gcc "__builtin_unreachable", change definition here! */
+ #undef unreachable
+ #define unreachable()	do { } while(1)
+ 
+-/* allow platform code to find out what surce we booted from */
++/* allow platform code to find out what source we booted from */
+ unsigned char ltq_boot_select(void)
+ {
+-	return 0/* ltq_sys1_r32(SYS1_BM) & BM_MASK */;
++	return 0;
+ }
+ 
+ /* allow the watchdog driver to find out what the boot reason was */
+ int ltq_reset_cause(void)
+ {
+-	return 0 /*ltq_sys1_r32(SYS1_CPU0RS) & CPU0RS_MASK*/;
++	return 0;
+ } 
+ 
+-static void machine_restart(char *command)
++static void machine_restart_toggle_pin(char *command)
+ {
+-	printk(KERN_NOTICE "System restart not implemented yet\n");
++	local_irq_disable();
++	ltq_ebu_w32(0x120000f1, EBU_ADDR_SEL_2);
++#ifdef CONFIG_CPU_LITTLE_ENDIAN
++	ltq_ebu_w32(0x4027ff, EBU_CON_2);
++#else
++	ltq_ebu_w32(0x404027ff, EBU_CON_2);
++#endif
++	if (svip_board_reset_reg_addr)
++		/* We just use the CPLD function to reset the entire system as a
++		workaround for the switch reset problem described in the errata
++		*/
++		iowrite8((u8) svip_board_reset_value,
++			(void __iomem *) (KSEG1 | svip_board_reset_reg_addr));
++}
++
++static void machine_restart_default(char *command)
++{
++	/* if no device tree found */
++	printk(KERN_WARNING "SVIP: default restart not good for eth switch\n");
++	ltq_w32(0, (void *)(KSEG1 | L2_SPRAM_BASE));
++	ltq_sys1_w32(0x00043F3E, SYS1_RREQR);/* reset all except PER, SUBSYS and CPU0 */
++	ltq_sys1_w32(0x00000100, SYS1_RRLSR);/* release WDT0 reset */
++	ltq_sys1_w32(~0x0c000040, SYS1_CLKCLR);/* restore reset value for clock enables */
++	ltq_sys1_w32(0x00030001, SYS1_RBTR);/* reset SUBSYS (incl. DDR2) and CPU0 */
++	unreachable();
+ }
+ 
+ static void machine_halt(void)
+@@ -56,9 +87,30 @@ static void machine_power_off(void)
+ 	unreachable();
+ }
+ 
++static int reset_property_exists(void)
++{
++	const u32 *property;
++	int len;
++	struct device_node *dt_reset =
++		of_find_compatible_node(NULL, NULL, "lantiq,svip-reset");
++	if (dt_reset) {
++		property = of_get_property(dt_reset, "svip-toggle-reset-pin", &len);
++		if (property && len == (2 * sizeof(u32))) {
++			svip_board_reset_reg_addr = (unsigned long) be32_to_cpup(property);
++			svip_board_reset_value = (unsigned long) be32_to_cpup(property + 1);
++			return 1;
++		}
++	}
++	printk(KERN_WARNING "SVIP: default restart not good for eth switch\n");
++	return 0;
++}
++
+ static int __init mips_reboot_setup(void)
+ {
+-	_machine_restart = machine_restart;
++	if (reset_property_exists())
++		_machine_restart = machine_restart_toggle_pin;
++	else
++		_machine_restart = machine_restart_default;
+ 	_machine_halt = machine_halt;
+ 	pm_power_off = machine_power_off;
+ 	return 0;
diff --git a/target/linux/lantiq/patches-3.10/2210-svip-add-dma.patch b/target/linux/lantiq/patches-3.10/2210-svip-add-dma.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2210-svip-add-dma.patch
@@ -0,0 +1,3120 @@
+# HG changeset patch
+# Parent b7ff0a3038f18a6a0e934c84fe21bfa5ce45f888
+
+diff --git a/arch/mips/include/asm/mach-lantiq/lantiq.h b/arch/mips/include/asm/mach-lantiq/lantiq.h
+--- a/arch/mips/include/asm/mach-lantiq/lantiq.h
++++ b/arch/mips/include/asm/mach-lantiq/lantiq.h
+@@ -49,6 +49,8 @@ extern unsigned char ltq_boot_select(voi
+ /* find out what caused the last cpu reset */
+ extern int ltq_reset_cause(void);
+ 
++extern unsigned int ltq_get_soc_type(void);
++
+ #define IOPORT_RESOURCE_START	0x10000000
+ #define IOPORT_RESOURCE_END	0xffffffff
+ #define IOMEM_RESOURCE_START	0x10000000
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/dma.h b/arch/mips/include/asm/mach-lantiq/svip/dma.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/dma.h
+@@ -0,0 +1,578 @@
++/*
++ *  This program is free software; you can redistribute it and/or modify it
++ *  under the terms of the GNU General Public License version 2 as published
++ *  by the Free Software Foundation.
++ *
++ *  Copyright (C) 2009~1012 Reddy <Reddy.Mallikarjun@lantiq.com>
++ *  Copyright (C) 2013 Lei Chuanhua <chuanhua.lei@lantiq.com>
++ */
++
++#ifndef _LANTIQ_LTQ_DMA_H__
++#define _LANTIQ_LTQ_DMA_H__
++/*!
++  \file ltq_dma.h
++  \ingroup LTQ_DMA_CORE
++  \brief LTQ Central DMA module register definition
++*/
++
++/** Register definitions */
++/** Clock control register */
++#define DMA_CLC			0x00
++
++/** Module ID register */
++#define DMA_ID			0x08
++
++/** DMA control register, global */
++#define DMA_CTRL		0x10
++/** Global Software reset */
++#define DMA_CTRL_RST		0x00000001
++/**Descriptor read back (supported for VR9)*/
++#define DMA_CTRL_DRB		0x00000100
++/** Descriptor read back (supported for VR9) */
++#define DMA_CTRL_MBRSTCNT_MASK	0x03FF0000
++/** MUltiple Burst Arbitration (supported for VR9) */
++#define DMA_CTRL_MBRSTARB	0x40000000
++/** Packet Arbitration (supported for VR9) */
++#define DMA_CTRL_PKTARB		0x80000000
++/** channel polling register*/
++
++#define DMA_CPOLL		0x14
++/**Enable Descriptor polling*/
++#define DMA_CPOLL_EN		0x80000000
++/**Enable Descriptor polling*/
++#define DMA_CPOLL_CNT_MASK	0x0000FFF0
++#define DMA_CPOLL_CNT_VAL(val)	(((val) & 0xFFF) << 4)
++
++/** Channel Related registers */
++/** channel select register */
++#define DMA_CS0			0x18
++#define DMA_CS			DMA_CS0
++#define DMA_CS_MASK		0x0000001F
++#define DMA_CS_VAL_GET(val)	(((val) & 0x1F))
++#define DMA_CS_VAL_SET(val)	(((val) & 0x1F))
++
++/** channel control register */
++#define DMA_CCTRL0		0x1C
++#define DMA_CCTRL		DMA_CCTRL0
++/** Channel On/ Off */
++#define DMA_CCTRL_ON		0x00000001
++/** Channel reset */
++#define DMA_CCTRL_RST		0x00000002
++/** direction of the channel*/
++#define DMA_CCTRL_DIR		0x00000100
++/** channel weight for Tx direction */
++#define DMA_CCTRL_TXWGT_MASK			0x00030000
++#define DMA_CCTRL_TXWGT_VAL(val)	(((val) & 0x3) << 16)
++#define DMA_CCTRL_TXWGT_GET(val)	\
++	((((val) & DMA_CCTRL_TXWGT_MASK) >> 16) & 0x3)
++/** Packet drop enabled */
++#define DMA_CCTRL_PDEN			0x00800000
++#define DMA_CCTRL_PDEN_GET(val)		\
++	((((val) & DMA_CCTRL_PDEN) >> 23) & 0x1)
++/* Peripheral to Peripheral copy enable*/
++#define DMA_CCTRL_P2PCPY		0x01000000
++#define DMA_CCTRL_P2PCPY_GET(val)	\
++	((((val) & DMA_CCTRL_P2PCPY) >> 24) & 0x1)
++/** loopback enable */
++#define DMA_CCTRL_LBEN			0x02000000
++/** Loopback channel number mask*/
++#define DMA_CCTRL_LBCHNR_MASK		0xFC000000
++#define DMA_CCTRL_LBCHNR_SET(val)	(((val) & 0x3F) << 26)
++#define DMA_CCTRL_LBCHNR_GET(val)	\
++	((((val) & DMA_CCTRL_LBCHNR_MASK) >> 26) & 0x3F)
++
++/** Channel descriptor base address register */
++#define DMA_CDBA0		0x20
++#define DMA_CDBA		DMA_CDBA0
++
++/** Channel descriptor length register */
++#define DMA_CDLEN0		0x24
++#define DMA_CDLEN		DMA_CDLEN0
++#define DMA_CDLEN_MASK		0x000000FF
++#define DMA_CDLEN_VAL_SET(val)	(((val) & 0xFF))
++#define DMA_CDLEN_VAL_GET(val)	(((val) & 0xFF))
++
++/** Channel interrupt status register */
++#define DMA_CIS0		0x28
++#define DMA_CIS			DMA_CIS0
++
++/* Channel Interrupt Status Register  */
++/** End of packet interrupt */
++#define DMA_CIS_EOP		0x00000002
++/** Descriptor Under-Run Interrupt  */
++#define DMA_CIS_DUR		0x00000004
++/** Descriptor Complete Interrupt  */
++#define DMA_CIS_DESCPT		0x00000008
++/** Channel Off Interrupt  */
++#define DMA_CIS_CHOFF		0x00000010
++/** SAI Read Error Interrupt */
++#define DMA_CIS_RDERR		0x00000020
++/** all interrupts */
++#define DMA_CIS_ALL	(DMA_CIS_EOP | DMA_CIS_DUR | DMA_CIS_DESCPT \
++			| DMA_CIS_CHOFF | DMA_CIS_RDERR)
++
++/** Channel interrupt enable register */
++#define DMA_CIE0		0x2C
++#define DMA_CIE			DMA_CIE0
++
++/**  Channel Interrupt Enable Register */
++/** End of packet interrupt enable */
++#define DMA_CIE_EOP		0x00000002
++/** Descriptor Under-Run Interrupt enable  */
++#define DMA_CIE_DUR		0x00000004
++/** Descriptor Complete Interrupt  enable*/
++#define DMA_CIE_DESCPT		0x00000008
++/** Channel Off Interrupt enable */
++#define DMA_CIE_CHOFF		0x00000010
++/** SAI Read Error Interrupt enable*/
++#define DMA_CIE_RDERR		0x00000020
++
++/** enable all interrupts */
++#define DMA_CIE_ALL	(DMA_CIE_EOP | DMA_CIE_DUR | DMA_CIE_DESCPT \
++			| DMA_CIE_CHOFF | DMA_CIE_RDERR)
++
++/** Channel Global buffer length register (not supported for Danube)*/
++#define DMA_CGBL		0x30
++#define DMA_CGBL_MASK		0x0000FFFF
++#define DMA_CGBL_SET_VAL(val)	(((val) & 0xFFFF))
++#define DMA_CGBL_GET_VAL(val)	(((val) & 0xFFFF))
++
++/** DMA Current Descriptor Pointer Register (Supports only VR9)*/
++#define DMA_CDPTNRD		0x34
++
++/** Port Related registers */
++/** Port select registers */
++#define DMA_PS0			0x40
++#define DMA_PS			DMA_PS0
++/** Port select mask*/
++#define DMA_PS_PS_MASK		0x00000007
++#define DMA_PS_VAL_SET(val)	(((val) & 0x7))
++#define DMA_PS_VAL_GET(val)	(((val) & 0x7))
++
++#define DMA_PCTRL0		0x44
++#define DMA_PCTRL		DMA_PCTRL0
++/** Port Control bit mask*/
++/** General purpose control, only used to memcopy port*/
++#define DMA_PCTRL_GPC			0x00010000
++#define DMA_PCTRL_GPC_VAL_SET(val)	(((val) & 0x1) << 16)
++#define DMA_PCTRL_GPC_VAL_GET(val)	(((val) & DMA_PCTRL_GPC) >> 16)
++
++/** Port Endianness for Tranmit Direction  */
++#define DMA_PCTRL_TXENDI_SET_VAL(val)	(((val) & 0x3) << 10)
++/** Port Endianness for Receive Direction  */
++#define DMA_PCTRL_RXENDI_SET_VAL(val)	(((val) & 0x3) << 8)
++/** Port Burst Length for Transmit Direction */
++#define DMA_PCTRL_TXBL_SET_VAL(val)	(((val) & 0x3) << 4)
++/* Port Burst Length for Receive Direction */
++#define DMA_PCTRL_RXBL_SET_VAL(val)	(((val) & 0x3) << 2)
++/* Port Weight for Transmit Direction  */
++#define DMA_PCTRL_TXWGT_SET_VAL(val)	(((val) & 0x7) << 12)
++/* Port packet drop enabled*/
++#define DMA_PCTRL_PDEN_SET_VAL(val)	(((val) & 0x1) << 6)
++
++/** Interrupt Related registers */
++/** Interrupt node enable register */
++#define DMA_IRNEN		0xf4
++/** Interrupt node control register */
++#define DMA_IRNCR		0xf8
++/** Interrupt capture register */
++#define DMA_IRNICR		0xfc
++
++/** DMA descriptor bitfields */
++/** Descriptor owner bit indication*/
++/** Descriptor is owned by DMA */
++#define DMA_OWN			1
++/** Descriptor is owned by CPU */
++#define CPU_OWN			0
++/** Descriprot Complete */
++#define DMA_DESC_CPT_SET	0x40000000
++/** Start of Packet */
++#define DMA_DESC_SOP_SET	0x20000000
++/** End of Packet */
++#define DMA_DESC_EOP_SET	0x10000000
++
++/** Descriptor byte offset */
++#define DMA_DESC_BYTE_OFFSET		23
++/** Receive Descriptor byte offset */
++#define DMA_RX_DESC_BYTE_OFFSET(val)    \
++	((val & 0x3) << DMA_DESC_BYTE_OFFSET)
++/** Transmit Descriptor byte offset */
++#define DMA_TX_DESC_BYTE_OFFSET(val)    \
++	((val & 0x1f) << DMA_DESC_BYTE_OFFSET)
++
++/**
++** Default Transmit/Receive endiannes type
++** B0_B1_B2_B3 No byte switching
++*/
++#define DMA_DEFAULT_TX_ENDIANNESS	IFX_DMA_ENDIAN_TYPE3
++#define DMA_DEFAULT_RX_ENDIANNESS	IFX_DMA_ENDIAN_TYPE3
++/** channel weight value*/
++#define DMA_CH_DEFAULT_WEIGHT		100
++/** Default Port Transmit weight value */
++#define DMA_TX_PORT_DEFAULT_WEIGHT	1
++/** Default Port Transmit weight value */
++#define DMA_TX_CHAN_DEFAULT_WEIGHT	1
++/** Default packet drop enabled for port */
++#define DMA_DEF_PORT_BASED_PKT_DROP_EN	0
++/** Default packet drop enabled for channel   */
++#define DMA_DEF_CHAN_BASED_PKT_DROP_EN	0
++/** Default Transmit burst length  */
++#define DMA_PORT_DEFAULT_TX_BURST_LEN	IFX_DMA_BURSTL_2
++/** Default Receive burst length  */
++#define DMA_PORT_DEFAULT_RX_BURST_LEN	IFX_DMA_BURSTL_2
++
++/** default enabled interrupts */
++#define DMA_CIE_DEFAULT			(DMA_CIE_DESCPT | DMA_CIE_EOP)
++/** disable all interrupts */
++#define DMA_CIE_DISABLE_ALL		0
++/** default channel polling interval value*/
++#define DMA_DEFAULT_POLL_VALUE		4
++
++#ifdef CONFIG_CPU_LITTLE_ENDIAN
++struct rx_desc {
++	union {
++		struct {
++			volatile unsigned int data_length:16;
++			volatile unsigned int reserved16_22:7;
++			volatile unsigned int byte_offset:2;
++			volatile unsigned int reserve_25_27:3;
++			volatile unsigned int eop:1;
++			volatile unsigned int sop:1;
++			volatile unsigned int C:1;
++			volatile unsigned int OWN:1;
++		} field;
++		volatile unsigned int word;
++	} status;
++	volatile unsigned int data_pointer;
++};
++
++struct tx_desc {
++	union {
++		struct {
++			volatile unsigned int data_length:16;
++			volatile unsigned int reserved:7;
++			volatile unsigned int byte_offset:5;
++			volatile unsigned int eop:1;
++			volatile unsigned int sop:1;
++			volatile unsigned int C:1;
++			volatile unsigned int OWN:1;
++		} field;
++		volatile unsigned int word;
++	} status;
++	volatile unsigned int data_pointer; /* Descriptor data pointer */
++};
++#else
++struct rx_desc {
++	union {
++		struct {
++			volatile unsigned int OWN:1;
++			volatile unsigned int C:1;
++			volatile unsigned int sop:1;
++			volatile unsigned int eop:1;
++			volatile unsigned int reserve_25_27:3;
++			volatile unsigned int byte_offset:2;
++			volatile unsigned int reserve16_22:7;
++			volatile unsigned int data_length:16;
++		} field;
++		volatile unsigned int word;
++	} status;
++	volatile unsigned int data_pointer; /* Descriptor data pointer */
++};
++
++struct tx_desc {
++	union {
++		struct {
++			volatile unsigned int OWN:1;
++			volatile unsigned int C:1;
++			volatile unsigned int sop:1;
++			volatile unsigned int eop:1;
++			volatile unsigned int byte_offset:5;
++			volatile unsigned int reserved:7;
++			volatile unsigned int data_length:16;
++		} field;
++		volatile unsigned int word;
++	} status;
++	volatile unsigned int data_pointer; /* Descriptor data pointer */
++};
++
++#endif /*CONFIG_CPU_LITTLE_ENDIAN  */
++
++/*!
++  \defgroup LTQ_DMA_CORE UEIP Project - Central DMA core driver
++  \brief UEIP Project - Central DMA core Module, supports LTQ CPE
++  \ platforms(Danube/ASE/ARx/VRx/GRX).
++ */
++
++/*!
++  \defgroup LTQ_DMA_DRV_API External APIs
++  \ingroup LTQ_DMA_CORE
++  \brief External APIs definitions for other modules.
++ */
++
++/*!
++  \defgroup LTQ_DMA_DRV_STRUCTURE Driver Structures
++  \ingroup LTQ_DMA_CORE
++  \brief Definitions/Structures of LTQ dma core module.
++ */
++
++/*!
++  \file lantiq_dma.h
++  \ingroup LTQ_DMA_CORE
++  \brief Header file for LTQ Central DMA core driver
++ */
++
++#define MAX_DMA_DEVICE_NUM              9
++
++#define MAX_DMA_CHANNEL_NUM             24
++
++#define DMA_DEV_NAME_LEN                8
++
++/** Config the Num of descriptors from Kernel configurations
++* or else if will take default number of  descriptors per channel
++*/
++
++#define MAX_DMA_DESC_NUM               255
++
++
++/*!
++  \addtogroup LTQ_DMA_DRV_STRUCTURE
++ */
++/*@{*/
++
++/*! \enum  dma_psuedeo_interrupts_t
++* \brief DMA pseudo interrupts.
++    These interrupts are generated by dma core driver to sync with client
++    drivers to handle the data between the clinet and core driver.
++*/
++typedef enum {
++	RCV_INT = 1,		/*!< Receive psuedo interrupt */
++	TX_BUF_FULL_INT = 2,	/*!< Tx channel descriptors full interrupt */
++	TRANSMIT_CPT_INT = 4, /*!< Tx channel descriptors available interrupt */
++} dma_psuedeo_interrupts_t;
++
++/*! \enum ifx_dma_channel_onoff_t
++ \brief dma channel is on/ off.
++*/
++typedef enum {
++	IFX_DMA_CH_OFF = 0,	/*!< DMA channel is OFF */
++	IFX_DMA_CH_ON = 1,	/*!< DMA channel is ON */
++} ifx_dma_channel_onoff_t;
++
++/*! \enum ifx_dma_class_t
++ \brief dma channel class value.
++*/
++typedef enum {
++	IFX_DMA_CLASS_0 = 0,
++	IFX_DMA_CLASS_1,
++	IFX_DMA_CLASS_2,
++	IFX_DMA_CLASS_3,
++	IFX_DMA_CLASS_4,
++	IFX_DMA_CLASS_5,
++	IFX_DMA_CLASS_6,
++	IFX_DMA_CLASS_7,
++} ifx_dma_class_t;
++
++/*! \enum ifx_dma_endian_t
++ \brief DMA endiannes type.
++*/
++typedef enum {
++	IFX_DMA_ENDIAN_TYPE0 = 0,	/*!< No byte Swapping */
++	IFX_DMA_ENDIAN_TYPE1,	/*!< Byte Swap(B0B1B2B3 => B1B0B3B2) */
++	IFX_DMA_ENDIAN_TYPE2,	/*!< Word Swap (B0B1B2B3 => B2B3B0B1) */
++	IFX_DMA_ENDIAN_TYPE3,	/*!< DWord Swap (B0B1B2B3 => B3B2B1B0) */
++} ifx_dma_endian_t;
++
++enum {
++    /** 2 DWORDS */
++	IFX_DMA_BURSTL_2 = 1,
++    /** 4 DWORDS */
++	IFX_DMA_BURSTL_4 = 2,
++    /** 8 DWORDS */
++	IFX_DMA_BURSTL_8 = 3,
++};
++
++/*! \enum ifx_dma_burst_len_t
++ \brief DMA Burst length.
++*/
++typedef enum {
++	DMA_BURSTL_2DW = 2,	/*!< 2 DWORD DMA burst length */
++	DMA_BURSTL_4DW = 4,	/*!< 4 DWORD DMA burst length */
++	DMA_BURSTL_8DW = 8,	/*!< 8 DWORD DMA burst length
++				(not supported by all peripherals) */
++} ifx_dma_burst_len_t;
++
++/*! \typedef _dma_arbitration_info
++ \brief Parameter Structure to used to configure DMA arbitration
++ based on packet or burst also Descriptor read back enabled/disabled
++ (Supported only VR9)
++ Used by reference dma_device_info
++*/
++typedef struct dma_arbitration_info {
++	__u32 packet_arbitration; /*!< enabled/disabled packet arbitration */
++
++	__u32 multiple_burst_arbitration; /*!< Enabled/Disabled Multi burst */
++	 /*!< Counter of the Multi burst arbitration(Num of bursts that served
++	  * before the arbitration of another peri port) */
++	unsigned int multiple_burst_counter;
++	__u32 desc_read_back;	/*!< enabled/disabled Descriptor read back */
++} _dma_arbitration_info;
++
++/*! \typedef _dma_channel_info
++ \brief The parameter structure is used to configure the DMA channel info
++ when the peripheral driver need to register with DMA core device driver.
++*/
++typedef struct dma_channel_info {
++	int rel_chan_no;	/*!< Relative channel number */
++	int dir;		/*!< Direction of channel */
++	int irq;		/*!< DMA channel IRQ number */
++	unsigned int desc_base;	/*!< Channel descriptor base address */
++	int desc_len;		/*!< Num of descriptors per channel */
++	int curr_desc;		/*!< Current Descriptor number */
++	int prev_desc;		/*!< Previous Descriptor number */
++	int byte_offset;	/*!< Byte offset */
++	int desc_handle;	/*!< Descriptor handled flag
++				( to handle Rx Descriptor by client driver) */
++	int weight;		/*!< WFQ present weight value for DMA channel */
++	int default_weight;	/*!< WFQ default weight value to handle in
++				driver for DMA channel */
++	int tx_channel_weight;	/*!< Config the Tx DMA channel weight value */
++	ifx_dma_class_t class_value;	/*!< Config the DMA class value */
++	int packet_size;	/*!< Size of the packet length */
++	int channel_packet_drop_enable;	/*!< Config channel based packet drop
++					(supported only VR9) */
++	/*!< Channel based packet drop counter */
++	int channel_packet_drop_counter;
++	int peri_to_peri;	/*!< Config Peripheral to Peripheral
++				(not supported Danube) */
++	int global_buffer_len;	/*!< Config global buffer length, valid only
++				when enabled peri_to_peri) */
++	int loopback_enable;	/*!< Config Loop back between the DMA channels
++				(Supported only VRx) */
++	int loopback_channel_number;	/*!< Config the loopback Channel number
++					(supported only VRx) */
++	int req_irq_to_free;	/*!< Release the DMA channel IRQ, which was
++				already requested */
++	int dur;		/*!< Flag for Descriptor underrun interrupt */
++	spinlock_t irq_lock;	/*!< spin lock */
++	ifx_dma_channel_onoff_t control;	/*!< Channel on/off flag */
++	void *opt[MAX_DMA_DESC_NUM];	/*!< Optional info */
++	void *dma_dev;		/*!< Pointing to the devices */
++	void (*open) (struct dma_channel_info *pCh);	/*!< DMA channel ON */
++	void (*close) (struct dma_channel_info *pCh);	/*!< DMA channel OFF */
++	void (*reset) (struct dma_channel_info *pCh); /*!< Reset DMA channel */
++	/*!< Enable channel interrupt */
++	void (*enable_irq) (struct dma_channel_info *pCh);
++	/*!< Disable channel interrupt */
++	void (*disable_irq) (struct dma_channel_info *pCh);
++} _dma_channel_info;
++
++/*! \typedef _dma_device_info
++ \brief The parameter structure is used to configure the DMA Peripheral ports
++ info when the peripheral driver need to register with DMA core device driver.
++*/
++typedef struct dma_device_info {
++	char device_name[DMA_DEV_NAME_LEN];	/*!< Peripheral Device name */
++	int port_reserved;	/*!< Reserve the device by client driver */
++	int port_num;		/*!< Port number */
++	ifx_dma_burst_len_t tx_burst_len; /*!< Configure the Tx burst length */
++	ifx_dma_burst_len_t rx_burst_len; /*!< Conigure the Rx burst length */
++	int port_tx_weight;	/*!< Configure the Port based weight value */
++	int port_packet_drop_enable;	/*!< Packet drop Enabled/Disabled */
++	int port_packet_drop_counter;	/*!< Packet drop counter */
++	int mem_port_control;	/*!< Configure the mem port control,
++				only used Memory Ports */
++	ifx_dma_endian_t tx_endianness_mode; /*!< Configure TX Endiannes */
++	ifx_dma_endian_t rx_endianness_mode;/*!< Configure RX Endiannes */
++	int current_tx_chan;	/*!< Current Tx channel of the device */
++	int current_rx_chan;	/*!< Current Rx channel of the device */
++	int num_tx_chan;	/*!< Config the num of Tx channels for device */
++	int num_rx_chan;	/*!< Config the num of Rx channels for device */
++	int max_rx_chan_num;	/*!< Max number of Rx channels supported */
++	int max_tx_chan_num;	/*!< Max number of Tx channels supported */
++	spinlock_t irq_lock;	/*!< spin lock */
++	_dma_arbitration_info arbitration_info;	/*!< arbitration config */
++	_dma_channel_info * tx_chan[MAX_DMA_CHANNEL_NUM]; /*!< Max TX channel */
++	_dma_channel_info * rx_chan[MAX_DMA_CHANNEL_NUM]; /*!< Max RX channel */
++	u8 *(*buffer_alloc) (int len, int *offset, void **opt);
++	int (*buffer_free) (u8 *dataptr, void *opt); /*!< Buffer free */
++	/*!< DMA pseudo interrupt handler */
++	int (*intr_handler) (struct dma_device_info *info, int status);
++	/*!< activate the polling  (Used when NAPI enables) */
++	void (*activate_poll) (struct dma_device_info *dma_dev);
++	/*!< Deactivate the polling (used when NAPI enabled) */
++	void (*inactivate_poll) (struct dma_device_info *dma_dev);
++	void *priv;	/*!< Pointer to the device private structure */
++} _dma_device_info;
++
++/* @} */
++/** Reserve the dma device port
++*   This function should call before the dma_device_register */
++extern _dma_device_info *dma_device_reserve(char *dev_name);
++
++/** Unreseve the dma device port
++*   This function will called after the dma_device_unregister */
++extern int dma_device_release(_dma_device_info *dev);
++
++/** Register with DMA device driver.
++This function should call after dma_device_reserve function.
++*   This function register with dma device driver to handle dma functionality.
++*   Should provide the required configuration info during the register with
++*   dma device.
++*   if not provide config info, then take default values. */
++extern int dma_device_register(_dma_device_info *info);
++
++/** Unregister with DMA core driver
++*   This function unregister with dma core driver. Once it unregister there is
++*   no DMA handling with client driver.*/
++extern int dma_device_unregister(_dma_device_info *info);
++
++/** Read data packet from DMA Rx channel.
++*   This function gets the data from the current rx descriptor of the DMA
++*   channel and send to the client driver.
++*   This functions is called when the client driver gets a pseudo DMA interrupt
++*   (RCV_INT). Handle with care when call this function as well as
++* dma_device_desc_setup function.*/
++extern int dma_device_read(struct dma_device_info *info, u8 **dataptr,
++			void **opt);
++
++/** Write data Packet to DMA Tx channel.
++*   This function gets the data packet from the client driver and send over on
++*   DMA channel.*/
++extern int dma_device_write(struct dma_device_info *info, u8 *dataptr,
++			int len, void *opt);
++
++/** Setup the DMA channel descriptor.
++*   This function setup the descriptor of the DMA channel used by client driver.
++*   The client driver will take care the buffer allocation and do proper
++*   checking of buffer for DMA burst alignment.Handle with care when call this
++*   function as well as dma_device_read function */
++extern int dma_device_desc_setup(_dma_device_info *dma_dev, char *buf,
++				size_t len);
++
++/** Clear the interrupt status flag
++*   This function used to exit from DMA tasklet(tasklet don't need to run again
++*   and again ) This is also used to avoid multiple psuedo interrupt (RCV_INT)
++*   per packet.*/
++extern int dma_device_clear_int(_dma_device_info *dma_dev, int dir);
++
++/**Clear the descriptor status word from the client driver once receive
++    a pseudo interrupt(RCV_INT) from the DMA module to avoid duplicate
++    interrupts from tasklet.
++*/
++extern int dma_device_clear_desc_status_word(_dma_device_info *dma_dev,
++					int dir);
++
++/** Poll the DMA device channel descriptors
++*   This function polls the interrupts status in polling mode. */
++extern int dma_device_poll(struct dma_device_info *info, int work_to_do,
++			int *work_done);
++
++/** setup the dma channel class value
++*   This function setup the class of service value for DMA channel.*/
++extern void dma_device_setup_class_val(_dma_channel_info *pCh, int cls);
++
++/** poll DMA ownership bit to ensure that rx transactions are complete
++*   to prevent descriptor errors */
++extern void poll_dma_ownership_bit(_dma_device_info *dma_dev);
++
++#endif /* _LANTIQ_LTQ_DMA_H__ */
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+--- a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
++++ b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+@@ -16,14 +16,58 @@
+ 
+ /* Chip IDs */
+ #define SOC_ID_SVIP		0x169
++#define SOC_ID_SVIPx		0x160
+ 
+ /* SoC Types */
+-#define SOC_TYPE_SVIP		0x01
++#define SOC_TYPE_DANUBE		0x01
++#define SOC_TYPE_TWINPASS	0x02
++#define SOC_TYPE_AR9		0x03
++#define SOC_TYPE_VR9		0x04 /* v1.1 */
++#define SOC_TYPE_VR9_2		0x05 /* v1.2 */
++#define SOC_TYPE_AMAZON_SE	0x06
++#define SOC_TYPE_AR10		0x07
++#define SOC_TYPE_GRX390		0x08
++#define SOC_TYPE_VRX220		0x09
++#define SOC_TYPE_SVIP		0x0A
++
++static inline int ltq_is_ase(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_AMAZON_SE);
++}
++
++static inline int ltq_is_danube(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_DANUBE);
++}
++
++static inline int ltq_is_ar9(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_AR9);
++}
++
++static inline int ltq_is_vr9(void)
++{
++	int type = ltq_get_soc_type();
++	if ((type == SOC_TYPE_VR9) || (type == SOC_TYPE_VR9_2))
++		return 1;
++	else
++		return 0;
++}
++
++static inline int ltq_is_ar10(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_AR10);
++}
++
++static inline int ltq_is_grx390(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_GRX390);
++}
+ 
+ #define LTQ_EBU_BASE				0x14102000
+ #define LTQ_ASC0_BASE_ADDR			0x14100100
+ #define LTQ_ASC1_BASE_ADDR			0x14100200
+-#define LTQ_STATUS_BASE_ADDR			0x1E000500
++#define LTQ_STATUS_BASE_ADDR		0x1E000500
+ #define LTQ_SYS1_BASE_ADDR			0x1C000800 
+ 
+ #define SYS0_PLL1CR	0x0008
+@@ -34,6 +78,7 @@
+ #define SYS1_FPICR	0x0014
+ #define SYS1_CLKENR	0x0004
+ #define SYS1_CLKCLR	0x0008
++#define SYS1_CLKENR_DMA (0x1 << 9)
+ #define SYS1_CLKENR_ASC0 (0x1 << 14)
+ #define SYS1_CLKENR_ASC1 (0x1 << 15)
+ #define SYS1_FPICR_FPIDIV   (0x1)
+diff --git a/arch/mips/lantiq/prom.c b/arch/mips/lantiq/prom.c
+--- a/arch/mips/lantiq/prom.c
++++ b/arch/mips/lantiq/prom.c
+@@ -32,6 +32,13 @@ unsigned long physical_memsize = 0L;
+  */
+ static struct ltq_soc_info soc_info;
+ 
++
++unsigned int ltq_get_soc_type(void)
++{
++	return soc_info.type;
++}
++EXPORT_SYMBOL(ltq_get_soc_type);
++
+ const char *get_system_type(void)
+ {
+ 	return soc_info.sys_type;
+diff --git a/arch/mips/lantiq/svip/Makefile b/arch/mips/lantiq/svip/Makefile
+--- a/arch/mips/lantiq/svip/Makefile
++++ b/arch/mips/lantiq/svip/Makefile
+@@ -1,1 +1,1 @@
+-obj-y := prom.o reset.o sysctrl.o
++obj-y := prom.o reset.o sysctrl.o dma.o
+diff --git a/arch/mips/lantiq/svip/dma.c b/arch/mips/lantiq/svip/dma.c
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/dma.c
+@@ -0,0 +1,2407 @@
++/*
++ *  This program is free software; you can redistribute it and/or modify it
++ *  under the terms of the GNU General Public License version 2 as published
++ *  by the Free Software Foundation.
++ *
++ *  Copyright (C) 2009~1012 Reddy <Reddy.Mallikarjun@lantiq.com>
++ *  Copyright (C) 2013 Lei Chuanhua <chuanhua.lei@lantiq.com>
++ */
++/*!
++  \file dma.c
++  \ingroup LTQ_DMA_CORE
++  \brief DMA driver main source file.
++*/
++#include <linux/init.h>
++#include <linux/platform_device.h>
++#include <linux/io.h>
++#include <linux/module.h>
++#include <linux/clk.h>
++#include <linux/slab.h>
++#include <linux/fs.h>
++#include <linux/errno.h>
++#include <linux/proc_fs.h>
++#include <linux/interrupt.h>
++#include <linux/delay.h>
++#include <linux/errno.h>
++#include <linux/percpu.h>
++#include <linux/of.h>
++#include <linux/seq_file.h>
++
++#include <lantiq.h>
++#include <lantiq_soc.h>
++#include <irq.h>
++#include <dma.h>
++
++#define ltq_dma_r32(x)			ltq_r32(ltq_dma_membase + (x))
++#define ltq_dma_w32(x, y)		ltq_w32(x, ltq_dma_membase + (y))
++#define ltq_dma_w32_mask(x, y, z)	ltq_w32_mask(x, y, \
++						ltq_dma_membase + (z))
++
++static void __iomem *ltq_dma_membase;
++
++#define DMA_CH0_INT		INT_NUM_IM2_IRL0
++#define DMA_CH1_INT		(INT_NUM_IM2_IRL0 + 1)
++#define DMA_CH2_INT		(INT_NUM_IM2_IRL0 + 2)
++#define DMA_CH3_INT		(INT_NUM_IM2_IRL0 + 3)
++#define DMA_CH4_INT		(INT_NUM_IM2_IRL0 + 4)
++#define DMA_CH5_INT		(INT_NUM_IM2_IRL0 + 5)
++#define DMA_CH6_INT		(INT_NUM_IM2_IRL0 + 6)
++#define DMA_CH7_INT		(INT_NUM_IM2_IRL0 + 7)
++#define DMA_CH8_INT		(INT_NUM_IM2_IRL0 + 8)
++#define DMA_CH9_INT		(INT_NUM_IM2_IRL0 + 9)
++#define DMA_CH10_INT		(INT_NUM_IM2_IRL0 + 10)
++#define DMA_CH11_INT		(INT_NUM_IM2_IRL0 + 11)
++#define DMA_CH12_INT		(INT_NUM_IM2_IRL0 + 25)
++#define DMA_CH13_INT		(INT_NUM_IM2_IRL0 + 26)
++#define DMA_CH14_INT		(INT_NUM_IM2_IRL0 + 27)
++#define DMA_CH15_INT		(INT_NUM_IM2_IRL0 + 28)
++#define DMA_CH16_INT		(INT_NUM_IM2_IRL0 + 29)
++#define DMA_CH17_INT		(INT_NUM_IM1_IRL0 + 30)
++#define DMA_CH18_INT		(INT_NUM_IM2_IRL0 + 16)
++#define DMA_CH19_INT		(INT_NUM_IM2_IRL0 + 21)
++#define DMA_CH20_INT		INT_NUM_IM4_IRL0
++#define DMA_CH21_INT		(INT_NUM_IM4_IRL0 + 1)
++#define DMA_CH22_INT		(INT_NUM_IM4_IRL0 + 2)
++#define DMA_CH23_INT		(INT_NUM_IM4_IRL0 + 3)
++#define DMA_CH24_INT		(INT_NUM_IM4_IRL0 + 4)
++#define DMA_CH25_INT		(INT_NUM_IM4_IRL0 + 5)
++#define DMA_CH26_INT		(INT_NUM_IM4_IRL0 + 6)
++#define DMA_CH27_INT		(INT_NUM_IM4_IRL0 + 7)
++
++/** Default channel budget */
++#define DMA_INT_BUDGET			20
++
++#define CN_GET(X)		(int)((X) - dma_chan)
++
++/*! \enum ifx_dma_chan_dir_t
++ \brief DMA Rx/Tx channel
++*/
++enum ifx_dma_chan_dir {
++	IFX_DMA_RX_CH = 0,  /*!< Rx channel */
++	IFX_DMA_TX_CH = 1,  /*!< Tx channel */
++};
++
++/*! \typedef _dma_chan_map
++ \brief The parameter structure is used to dma channel map
++*/
++struct ifx_dma_chan_map {
++	int  port_num;	/*!< Port number */
++	char dev_name[DMA_DEV_NAME_LEN]; /*!< Device Name */
++	enum ifx_dma_chan_dir	dir; /*!< Direction of the DMA channel */
++	int  pri;		/*!< Class value */
++	int  irq;		/*!< DMA channel irq number */
++	int  rel_chan_no;	/*!< Relative channel number */
++};
++
++struct dma_sched_status {
++	volatile u32 dma_int_status;
++	volatile int dma_in_process;
++#ifdef CONFIG_NAPI_ENABLED
++	volatile u32 dma_poll_int_status;
++#endif
++};
++
++static struct proc_dir_entry *g_dma_dir;
++static int dma_max_ports;
++static int dma_max_chans;
++static int dma_desc_num;
++static int dma_hw_poll;
++static int dma_pkt_arb_enable;
++
++static u64 *g_desc_list;
++static char dma_device_name[MAX_DMA_DEVICE_NUM][DMA_DEV_NAME_LEN];
++static struct ifx_dma_chan_map ifx_default_dma_map[MAX_DMA_CHANNEL_NUM];
++struct ifx_dma_chan_map *chan_map = ifx_default_dma_map;
++static u64 *g_desc_list_backup;
++static _dma_device_info dma_devs[MAX_DMA_DEVICE_NUM];
++static _dma_channel_info dma_chan[MAX_DMA_CHANNEL_NUM];
++
++static DEFINE_SPINLOCK(g_dma_spinlock);
++static DEFINE_PER_CPU(struct dma_sched_status, dma_sched_stats);
++
++static void do_dma_tasklet(unsigned long);
++static struct tasklet_struct dma_tasklet[NR_CPUS];
++
++/** Set the channel number */
++static void select_chan(int cn)
++{
++	ltq_dma_w32(cn, DMA_CS);
++}
++
++/** Select the channel ON */
++static void enable_chan(int cn)
++{
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32_mask(0, DMA_CCTRL_ON, DMA_CCTRL);
++}
++
++/** Select the channel OFF */
++static void disable_chan(int cn)
++{
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++}
++
++static unsigned char *common_buffer_alloc(int len, int *byte_offset, void **opt)
++{
++	unsigned char *buffer = kmalloc(len, GFP_ATOMIC);
++	*byte_offset = 0;
++	return buffer;
++}
++
++static int common_buffer_free(unsigned char *dataptr, void *opt)
++{
++	kfree(dataptr);
++	return 0;
++}
++
++/** Enable the DMA channel interrupt */
++static void __enable_ch_irq(int cn, int dir)
++{
++	unsigned int val = DMA_CIE_EOP;
++
++	if (dir == IFX_DMA_RX_CH) {
++		if (dma_hw_poll)
++			val |= DMA_CIE_DESCPT;
++		else
++			val |= DMA_CIE_DUR;
++
++	}
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32(val, DMA_CIE);
++	ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++	/*ltq_dma_w32((1 << cn), DMA_IRNCR); */
++}
++
++static void enable_ch_irq(_dma_channel_info *pch)
++{
++	unsigned long flags;
++	int cn = CN_GET(pch);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	__enable_ch_irq(cn, pch->dir);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++
++/** Dsiable the DMA channel Interrupt */
++static void __disable_ch_irq(int cn)
++{
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	stats->dma_int_status &= ~(1 << cn);
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32(DMA_CIE_DISABLE_ALL, DMA_CIE);
++	ltq_dma_w32_mask((1 << cn), 0, DMA_IRNEN);
++}
++
++static void disable_ch_irq(_dma_channel_info *pch)
++{
++	unsigned long flags;
++	int cn = CN_GET(pch);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	__disable_ch_irq(cn);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++
++/**  Set the DMA channl ON*/
++static void open_chan(_dma_channel_info *pch)
++{
++	unsigned long flags;
++	int cn = CN_GET(pch);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	enable_chan(cn);
++	if (pch->dir == IFX_DMA_RX_CH)
++		__enable_ch_irq(cn, pch->dir);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++
++/**  Set the DMA channl OFF*/
++static void close_chan(_dma_channel_info *pch)
++{
++	int j;
++	unsigned long flags;
++	int cn = CN_GET(pch);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++	for (j = 10000; (ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_ON) && j > 0; j--)
++			;
++	__disable_ch_irq(cn);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	if (j == 0)
++		pr_info("cn %d: can not be turned off, DMA_CCTRL %08x\n",
++			cn, ltq_dma_r32(DMA_CCTRL));
++}
++
++/** Reset the dma channel */
++static void reset_chan(_dma_channel_info *pch)
++{
++	int i;
++	unsigned long flags;
++	u32 prev_cn;
++	int cn = CN_GET(pch);
++
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	stats->dma_int_status &= ~(1 << cn);
++	prev_cn = ltq_dma_r32(DMA_CS);
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32_mask(0, DMA_CCTRL_RST, DMA_CCTRL);
++	while ((ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_RST))
++		;
++	ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++	ltq_dma_w32(((u32)CPHYSADDR(pch->desc_base)), DMA_CDBA);
++	ltq_dma_w32(pch->desc_len, DMA_CDLEN);
++	pch->curr_desc = 0;
++	pch->prev_desc = 0;
++	ltq_dma_w32(prev_cn, DMA_CS);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	/* clear descriptor memory, so that OWN bit is set to CPU */
++	/* memset((u32*)pch->desc_base, 0,
++	 pch->desc_len * sizeof (struct tx_desc)); */
++	if (pch->dir == IFX_DMA_TX_CH) {
++		for (i = 0; i < pch->desc_len; i++)
++			((struct tx_desc *) pch->desc_base)[i].
++			status.field.OWN = CPU_OWN;
++	} else {
++		for (i = 0; i < pch->desc_len; i++) {
++			((struct rx_desc *) pch->desc_base)[i].
++				status.field.OWN = CPU_OWN;
++			((struct rx_desc *) pch->desc_base)[i].
++				status.field.C = 1;
++		}
++	}
++	/* TODO: Reset DMA descriptors (free buffer, reset owner bit, allocate
++	 * new buffer) */
++}
++
++/* Handle the Rx channel interrupt scheduler */
++static void rx_chan_intr_handler(int cn)
++{
++	unsigned long flags;
++	unsigned int prev_cn, cis;
++	struct rx_desc *rx_desc_p;
++	_dma_device_info *pdev = (_dma_device_info *) dma_chan[cn].dma_dev;
++	_dma_channel_info *pch = &dma_chan[cn];
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	/* Handle command complete interrupt */
++	rx_desc_p = (struct rx_desc *) pch->desc_base + pch->curr_desc;
++	if (rx_desc_p->status.field.OWN == CPU_OWN
++		&& rx_desc_p->status.field.C) {
++		/* Everything is correct, then we inform the upper layer */
++		pdev->current_rx_chan = pch->rel_chan_no;
++		if (pdev->intr_handler)
++			pdev->intr_handler(pdev, RCV_INT);
++
++		pch->weight--;
++		/* Clear interrupt status bits once we sendout the psuedo
++		 * interrupt to client driver */
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		prev_cn = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(cn, DMA_CS);
++		ltq_dma_w32_mask(0, DMA_CIS_ALL, DMA_CIS);
++		ltq_dma_w32(prev_cn, DMA_CS);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	} else {
++		/* Read channel interrupt status bit to determine if we should
++		 * wait or move forward */
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		/* Clear the DMA Node status register bit to remove the dummy
++		 * interrupts */
++		ltq_dma_w32((1 << cn), DMA_IRNCR);
++		prev_cn = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(cn, DMA_CS);
++		cis = ltq_dma_r32(DMA_CIS);
++		/*
++		 * In most cases, interrupt bit has been cleared in the first
++		 * branch, it will hit the following. However, in some corner
++		 * cases, the above branch doesn't happen, so it indicates that
++		 * interrupt is received actually, but OWN bit and
++		 * Complete bit not update yet, so go back to DMA tasklet and
++		 * let DMA tasklet wait <polling>. This is the most efficient
++		 * way. NB, only EOP and Descript Complete interrupt enabled
++		 * in Channel Interrupt Enable Register.
++		 */
++		if ((cis & ltq_dma_r32(DMA_CIE)) == 0) {
++			if (rx_desc_p->status.field.OWN != CPU_OWN)
++				stats->dma_int_status &= ~(1 << cn);
++			/*
++			 * Enable this channel interrupt again after processing
++			 * all packets available Placing this line at the last
++			 * line will avoid interrupt coming again in heavy load
++			 * to great extent
++			 */
++			ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++		}
++		ltq_dma_w32(prev_cn, DMA_CS);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		if (!dma_hw_poll) {
++			if (cis & DMA_CIS_DUR)
++				pch->dur = 1;
++		}
++	}
++
++}
++
++/* Handle the Tx channel interrupt scheduler */
++static void tx_chan_intr_handler(int cn)
++{
++	unsigned long flags;
++	unsigned int prev_cn, cis;
++	_dma_device_info *pdev = (_dma_device_info *) dma_chan[cn].dma_dev;
++	_dma_channel_info *pch = &dma_chan[cn];
++	struct tx_desc *tx_desc_p =
++		(struct tx_desc *) pch->desc_base + pch->curr_desc;
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	prev_cn = ltq_dma_r32(DMA_CS);
++	ltq_dma_w32(cn, DMA_CS);
++	cis = ltq_dma_r32(DMA_CIS);
++	ltq_dma_w32(cis | DMA_CIS_ALL, DMA_CIS);
++	ltq_dma_w32(prev_cn, DMA_CS);
++
++	pdev->current_tx_chan = pch->rel_chan_no;
++	/* DMA Descriptor update by Hardware is not sync with DMA interrupt
++	 * (EOP/Complete). To ensure descriptor is available before sending
++	 * psudo interrupt to the client drivers. */
++	if (!(tx_desc_p->status.field.OWN == DMA_OWN)) {
++		if ((cis & DMA_CIS_EOP) && pdev->intr_handler) {
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++			pdev->intr_handler(pdev, TRANSMIT_CPT_INT);
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			stats->dma_int_status &= ~(1 << cn);
++		}
++	}
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++
++/* Handle the Rx channel interrupt handler*/
++void rx_chan_handler(int cn)
++{
++	unsigned long flags;
++	unsigned int prev_cn;
++	struct rx_desc *rx_desc_p;
++	_dma_device_info *pdev =
++		(_dma_device_info *) dma_chan[cn].dma_dev;
++	_dma_channel_info *pch = &dma_chan[cn];
++
++	/* Handle command complete interrupt */
++	rx_desc_p = (struct rx_desc *) pch->desc_base + pch->prev_desc;
++	if ((rx_desc_p->status.field.OWN == CPU_OWN)
++		&& rx_desc_p->status.field.C) {
++		/* Everything is correct, then we inform the upper layer */
++		pdev->current_rx_chan = pch->rel_chan_no;
++		if (pdev->intr_handler)
++			pdev->intr_handler(pdev, RCV_INT);
++
++		/* Clear interrupt status bits once we sendout the psuedo
++		* interrupt to client driver */
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		prev_cn = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(cn, DMA_CS);
++		ltq_dma_w32_mask(0, DMA_CIS_ALL, DMA_CIS);
++		ltq_dma_w32(prev_cn, DMA_CS);
++		/*
++		 * Enable this channel interrupt again after processing all
++		 * packets available Placing this line at the last line will
++		 * avoid interrupt coming again in heavy load to great extent
++		 */
++		ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		pch->prev_desc = (pch->prev_desc + 1) % (pch->desc_len);
++	} else {
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		/*
++		 * Enable this channel interrupt again after processing all
++		 * packets available Placing this line at the last line will
++		 * avoid interrupt coming again in heavy load to great extent
++		 */
++		ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	}
++}
++
++/** Trigger when taklet schedule calls*/
++static void do_dma_tasklet(unsigned long cpu)
++{
++	unsigned long flags;
++	int i, cn = 0, budget = DMA_INT_BUDGET, weight = 0;
++	struct dma_sched_status *stats = &per_cpu(dma_sched_stats, cpu);
++
++	while (stats->dma_int_status) {
++		if (budget-- < 0) {
++			tasklet_schedule(&dma_tasklet[cpu]);
++			return;
++		}
++		cn = -1;
++		weight = 0;
++		/* WFQ algorithm to select the channel */
++		for (i = 0; i < dma_max_chans; i++) {
++			if ((stats->dma_int_status & (1 << i))
++				&& dma_chan[i].weight > 0) {
++				if (dma_chan[i].weight > weight) {
++					cn = i;
++					weight = dma_chan[cn].weight;
++				}
++			}
++		}
++		if (cn >= 0) {
++			if (chan_map[cn].dir == IFX_DMA_RX_CH)
++				rx_chan_intr_handler(cn);
++			else
++				tx_chan_intr_handler(cn);
++
++		} else {
++			/* Reset the default weight vallue for all channels */
++			for (i = 0; i < dma_max_chans; i++) {
++				dma_chan[i].weight =
++					dma_chan[i].default_weight;
++			}
++		}
++	}
++	/*
++	 * Sanity check, check if there is new packet coming during this small
++	 * gap However, at the same time, the following may cause interrupts is
++	 * coming again on the same channel, because of rescheduling.
++	 */
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	stats->dma_in_process = 0;
++	if (stats->dma_int_status) {
++		stats->dma_in_process = 1;
++		tasklet_schedule(&dma_tasklet[cpu]);
++	}
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++
++/**
++ * This function works as the following,
++ * 1) Valid check
++ * 2) Disable interrupt on this DMA channel, so that no further interrupts are
++ *  coming
++ *    when DMA tasklet is running. However, it still allows other DMA channel
++ *    interrupt to come in.
++ * 3) Record interrupt on this DMA channel, and dispatch it to DMA tasklet later
++ * 4) If NAPI is enabled, submit to polling process instead of DMA tasklet.
++ * 5) Check if DMA tasklet is running, if not, invoke it so that context switch
++ *is minimized
++ */
++static irqreturn_t dma_interrupt(int irq, void *dev_id)
++{
++	int cn;
++	unsigned long flags;
++	_dma_channel_info *pch;
++	int cpu = smp_processor_id();
++	struct dma_sched_status *stats = &per_cpu(dma_sched_stats, cpu);
++
++	pch = (_dma_channel_info *) dev_id;
++	cn = CN_GET(pch);
++
++	if (cn < 0 || cn > (dma_max_chans - 1)) {
++		pr_err("%s: dma_interrupt irq=%d cn=%d\n",
++			__func__, irq, cn);
++		return IRQ_NONE;
++	}
++	/*
++	 * Disable interrupt on this channel, later tasklet will enable it after
++	 * processing all available packets on this channel.
++	 */
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	ltq_dma_w32_mask((1 << cn), 0, DMA_IRNEN);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	if (ltq_is_ase()) {
++		/* SDIO channels direct call interrupt routine,
++		  without tasklet schedule */
++		if (cn == 6) {
++			rx_chan_handler(cn);
++			return IRQ_HANDLED;
++		}
++		if (cn == 7) {
++			tx_chan_intr_handler(cn);
++			return IRQ_HANDLED;
++		}
++	}
++	/* Record this channel interrupt for tasklet */
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	stats->dma_int_status |= (1 << cn);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++
++#ifdef CONFIG_NAPI_ENABLED
++	if (pch->dir == IFX_DMA_RX_CH) {
++		_dma_device_info *pdev = (_dma_device_info *) pch->dma_dev;
++		if (pdev->activate_poll) {
++			/* Handled by polling rather than tasklet */
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			stats->dma_int_status &= ~(1 << cn);
++			stats->dma_poll_int_status |= (1 << cn);
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++			if (pdev->activate_poll)
++				pdev->activate_poll(pdev);
++			return IRQ_RETVAL(1);
++		}
++	}
++#endif /* CONFIG_NAPI_ENABLED */
++	/* if not in process, then invoke the tasklet */
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	if (!stats->dma_in_process) {
++		stats->dma_in_process = 1;
++		tasklet_schedule(&dma_tasklet[cpu]);
++	}
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	return IRQ_HANDLED;
++}
++
++/*!
++  \fn _dma_device_info* dma_device_reserve(char* dev_name)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Reserve the dma device port
++
++  This function should call before the dma_device_register
++  It is reserved the dma device port if the port is supported and send
++  the dma device info pointer
++  \param    dev_name --> The supported port device name.
++  \return on success send dma devcie pointer else NULL
++*/
++_dma_device_info *dma_device_reserve(char *dev_name)
++{
++	int i;
++
++	for (i = 0; i < dma_max_ports; i++) {
++		if (strcmp(dev_name, dma_devs[i].device_name) == 0) {
++			if (dma_devs[i].port_reserved)
++				return NULL;
++			dma_devs[i].port_reserved = 1;
++		#ifdef CONFIG_NAPI_ENABLED
++			dma_devs[i].activate_poll = NULL;
++			dma_devs[i].inactivate_poll = NULL;
++		#endif /* CONFIG_NAPI_ENABLED */
++			return &dma_devs[i];
++		}
++	}
++	return NULL;
++}
++EXPORT_SYMBOL(dma_device_reserve);
++
++/*!
++  \fn int dma_device_release(_dma_device_info* dev)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Unreseve the dma device port
++
++  This function will called after the dma_device_unregister
++  This fucnction unreseve the port.
++
++  \param    dev --> pointer to DMA device structure
++  \return 0
++*/
++int dma_device_release(_dma_device_info *dev)
++{
++	int i;
++	for (i = 0; i < dma_max_ports; i++) {
++		if (strcmp(dma_devs[i].device_name, dev->device_name) == 0) {
++			if (dev->port_reserved) {
++				dev->port_reserved = 0;
++				return 0;
++			}
++		}
++	}
++	pr_err("%s: Device Port released failed: %s\n",
++		__func__, dev->device_name);
++	return -EIO;
++}
++EXPORT_SYMBOL(dma_device_release);
++
++static inline int dma_class_value_set(int value)
++{
++	if (ltq_is_vr9())
++		return (value & 0x7) << 9;
++	else
++		return (value & 0x3) << 9;
++}
++
++static inline int dma_class_mask_get(void)
++{
++	if (ltq_is_vr9())
++		return 0x00000E00;
++	else
++		return 0x00000600;
++}
++
++/*!
++  \fn int dma_device_register(_dma_device_info* dev)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Register with DMA device driver.
++
++  This function registers with dma device driver to handle dma functionality.
++  Should provide the configuration info during the register with dma device.
++  if not provide channel/port configuration info, then take default values.
++  This function should call after dma_device_reserve function.
++  This function configure the Tx/Rx channel info as well as device port info
++
++  \param    dev --> pointer to DMA device structure
++  \return 0/-EINVAL
++*/
++int dma_device_register(_dma_device_info *dev)
++{
++	unsigned long flags;
++	int result = 0, i, j, cn = 0, byte_offset = 0, txbl, rxbl;
++	unsigned int reg_val;
++	unsigned char *buffer;
++	_dma_device_info *pdev;
++	_dma_channel_info *pch;
++	struct rx_desc *rx_desc_p;
++	struct tx_desc *tx_desc_p;
++	int port_no = dev->port_num;
++
++	if (port_no < 0 || port_no > dma_max_ports)
++		pr_err("%s: Wrong port number(%d)!!!\n", __func__, port_no);
++	/* burst length Configration */
++	switch (dev->tx_burst_len) {
++	case 8:
++		txbl = IFX_DMA_BURSTL_8;
++		break;
++	case 4:
++		txbl = IFX_DMA_BURSTL_4;
++		break;
++	default:
++		txbl = DMA_PORT_DEFAULT_TX_BURST_LEN;
++	}
++	switch (dev->rx_burst_len) {
++	case 8:
++		rxbl = IFX_DMA_BURSTL_8;
++		break;
++	case 4:
++		rxbl = IFX_DMA_BURSTL_4;
++		break;
++	default:
++		rxbl = DMA_PORT_DEFAULT_RX_BURST_LEN;
++	}
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	ltq_dma_w32(port_no, DMA_PS);
++	reg_val = DMA_PCTRL_TXWGT_SET_VAL(dev->port_tx_weight)
++		| DMA_PCTRL_TXENDI_SET_VAL(dev->tx_endianness_mode)
++		| DMA_PCTRL_RXENDI_SET_VAL(dev->rx_endianness_mode)
++		| DMA_PCTRL_PDEN_SET_VAL(dev->port_packet_drop_enable)
++		| DMA_PCTRL_TXBL_SET_VAL(txbl)
++		| DMA_PCTRL_RXBL_SET_VAL(rxbl);
++	if (dev->mem_port_control)
++		reg_val |= DMA_PCTRL_GPC;
++	else
++		reg_val &= ~DMA_PCTRL_GPC;
++
++	ltq_dma_w32(reg_val, DMA_PCTRL);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	/* Tx channel register */
++	for (i = 0; i < dev->max_tx_chan_num; i++) {
++		pch = dev->tx_chan[i];
++		if (pch->control == IFX_DMA_CH_ON) {
++			cn = CN_GET(pch);
++			if (pch->desc_len > dma_desc_num) {
++				pr_info("%s Tx Channel %d descriptor length %d out of range <1~%d>\n",
++					__func__, cn, pch->desc_len,
++					dma_desc_num);
++				result = -EINVAL;
++				goto done;
++			}
++			for (j = 0; j < pch->desc_len; j++) {
++				tx_desc_p =
++					(struct tx_desc *) pch->desc_base + j;
++				memset(tx_desc_p, 0, sizeof(struct tx_desc));
++			}
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			ltq_dma_w32(cn, DMA_CS);
++
++			/* check if the descriptor base is changed */
++			if (ltq_dma_r32(DMA_CDBA) !=
++				(u32)CPHYSADDR(pch->desc_base)) {
++				ltq_dma_w32(((u32)CPHYSADDR(pch->desc_base)),
++					DMA_CDBA);
++			}
++			/* Check if the descriptor length is changed */
++			if (ltq_dma_r32(DMA_CDLEN) != pch->desc_len)
++				ltq_dma_w32(pch->desc_len, DMA_CDLEN);
++
++			ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++			udelay(20);
++			ltq_dma_w32_mask(0, DMA_CCTRL_RST, DMA_CCTRL);
++			while ((ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_RST))
++				;
++			ltq_dma_w32((1 << cn), DMA_IRNCR);
++			ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++			reg_val = ltq_dma_r32(DMA_CCTRL);
++			reg_val &= ~DMA_CCTRL_TXWGT_MASK;
++			reg_val |= DMA_CCTRL_TXWGT_VAL(pch->tx_channel_weight);
++			if (pch->channel_packet_drop_enable)
++				reg_val |= DMA_CCTRL_PDEN;
++			else
++				reg_val &= ~DMA_CCTRL_PDEN;
++			if (pch->class_value) {
++				reg_val &= ~dma_class_mask_get();
++				reg_val |=
++					dma_class_value_set(pch->class_value);
++			}
++			if (pch->peri_to_peri) {
++				reg_val |= DMA_CCTRL_P2PCPY;
++				if (pch->global_buffer_len)
++					ltq_dma_w32(pch->global_buffer_len,
++						DMA_CGBL);
++			} else
++				reg_val &= ~DMA_CCTRL_P2PCPY;
++
++			if (pch->loopback_enable) {
++				reg_val |= DMA_CCTRL_LBEN;
++				if (pch->loopback_channel_number) {
++					reg_val &= ~DMA_CCTRL_LBCHNR_MASK;
++					reg_val |= DMA_CCTRL_LBCHNR_SET(
++						pch->loopback_channel_number);
++				}
++			} else
++				reg_val &= ~DMA_CCTRL_LBEN;
++			ltq_dma_w32(reg_val, DMA_CCTRL);
++			if (pch->req_irq_to_free == dma_chan[cn].irq) {
++				/* free_irq(dma_chan[cn].irq,
++					(void*)&dma_chan[cn]); */
++			} else
++				enable_irq(dma_chan[cn].irq);
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		}
++	}
++	/* RX channel register */
++	for (i = 0; i < dev->max_rx_chan_num; i++) {
++		pch = dev->rx_chan[i];
++		if (pch->control == IFX_DMA_CH_ON) {
++			cn = CN_GET(pch);
++
++			if (pch->desc_len > dma_desc_num) {
++				pr_info("%s Rx Channel %d descriptor length %d out of range <1~%d>\n",
++					__func__, cn, pch->desc_len,
++					dma_desc_num);
++				result = -EINVAL;
++				goto done;
++			}
++			for (j = 0; j < pch->desc_len; j++) {
++				rx_desc_p =
++					(struct rx_desc *) pch->desc_base + j;
++				pdev = (_dma_device_info *) (pch->dma_dev);
++				buffer = pdev->buffer_alloc(
++					pch->packet_size, &byte_offset,
++					(void *)&(pch->opt[j]));
++				if (!buffer)
++					break;
++				dma_cache_inv((unsigned long) buffer,
++					pch->packet_size);
++				rx_desc_p->data_pointer =
++					(u32) CPHYSADDR((u32) buffer);
++				rx_desc_p->status.word = 0;
++				rx_desc_p->status.field.byte_offset =
++					byte_offset;
++				rx_desc_p->status.field.OWN = DMA_OWN;
++				rx_desc_p->status.field.data_length =
++					pch->packet_size;
++			}
++
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			ltq_dma_w32(cn, DMA_CS);
++			/* Check if the descriptor base is changed */
++			if (ltq_dma_r32(DMA_CDBA) !=
++				(u32)CPHYSADDR(pch->desc_base)) {
++				ltq_dma_w32(((u32)CPHYSADDR(pch->desc_base)),
++					DMA_CDBA);
++			}
++			/* Check if the descriptor length is changed */
++			if (ltq_dma_r32(DMA_CDLEN) != pch->desc_len)
++				ltq_dma_w32(((pch->desc_len)), DMA_CDLEN);
++
++			ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++			udelay(20);
++			ltq_dma_w32_mask(0, DMA_CCTRL_RST, DMA_CCTRL);
++			while ((ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_RST))
++				;
++
++			/* XXX, should enable all interrupts? */
++			ltq_dma_w32(DMA_CIE_DEFAULT, DMA_CIE);
++			ltq_dma_w32((1 << cn), DMA_IRNCR);
++			ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++			reg_val = ltq_dma_r32(DMA_CCTRL);
++			/*
++			reg_val &= ~DMA_CCTRL_TXWGT_MASK;
++			reg_val |= DMA_CCTRL_TXWGT_VAL(pch->tx_channel_weight);
++			 */
++			if (pch->channel_packet_drop_enable)
++				reg_val |= DMA_CCTRL_PDEN;
++			else
++				reg_val &= ~DMA_CCTRL_PDEN;
++			if (pch->class_value) {
++				reg_val &= ~dma_class_mask_get();
++				reg_val |=
++					dma_class_value_set(pch->class_value);
++			}
++			if (pch->loopback_enable) {
++				reg_val |= DMA_CCTRL_LBEN;
++				if (pch->loopback_channel_number) {
++					reg_val &= ~DMA_CCTRL_LBCHNR_MASK;
++					reg_val |= DMA_CCTRL_LBCHNR_SET(pch->
++						loopback_channel_number);
++				}
++			} else
++				reg_val &= ~DMA_CCTRL_LBEN;
++			ltq_dma_w32(reg_val, DMA_CCTRL);
++			if (pch->req_irq_to_free == dma_chan[cn].irq) {
++				/* free_irq(dma_chan[cn].irq,
++					(void*)&dma_chan[cn]); */
++			} else
++				enable_irq(dma_chan[cn].irq);
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		}
++	}
++done:
++	return result;
++}
++EXPORT_SYMBOL(dma_device_register);
++
++/*!
++  \fn int dma_device_unregister(_dma_device_info* dev)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Unregister with DMA core driver
++
++  This function unregisters with dma core driver. Once it unregisters
++  there is no
++  DMA handling with client driver.
++
++  \param    dev --> pointer to DMA device structure
++  \return 0
++*/
++int dma_device_unregister(_dma_device_info *dev)
++{
++	unsigned long flags;
++	int result = 0, i, j, cn;
++	unsigned int reg_val;
++	_dma_channel_info *pch;
++	struct rx_desc *rx_desc_p;
++	struct tx_desc *tx_desc_p;
++	int port_no = dev->port_num;
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	if (port_no < 0 || port_no > dma_max_ports)
++		pr_err("%s: Wrong port number(%d)!!!\n", __func__, port_no);
++	for (i = 0; i < dev->max_tx_chan_num; i++) {
++		pch = dev->tx_chan[i];
++		if (pch->control == IFX_DMA_CH_ON) {
++			cn = CN_GET(pch);
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			ltq_dma_w32(cn, DMA_CS);
++			pch->curr_desc = 0;
++			pch->prev_desc = 0;
++			pch->control = IFX_DMA_CH_OFF;
++
++			/* XXX, Should dislabe all interrupts here */
++			ltq_dma_w32(0, DMA_CIE);
++
++			/* Disable this channel interrupt */
++			ltq_dma_w32_mask((1 << cn), 0, DMA_IRNEN);
++			ltq_dma_w32((1 << cn), DMA_IRNCR);
++			ltq_dma_w32_mask(DMA_CCTRL_ON, 0, DMA_CCTRL);
++			for (j = 10000; (ltq_dma_r32(DMA_CCTRL) &
++				DMA_CCTRL_ON) && j > 0; j--)
++				;
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++			if (j == 0)
++				pr_info("Port %d TX CH %d: can not be turned off, DMA_CCTRL %08x\n",
++					port_no, i, ltq_dma_r32(DMA_CCTRL));
++			for (j = 0; j < pch->desc_len; j++) {
++				if (pch->peri_to_peri)
++					break;
++
++				tx_desc_p =
++					(struct tx_desc *) pch->desc_base + j;
++				if (((tx_desc_p->status.field.OWN == CPU_OWN)
++					&& tx_desc_p->status.field.C)
++					|| (tx_desc_p->status.field.OWN ==
++					DMA_OWN
++					&& tx_desc_p->status.field.
++					data_length > 0)) {
++					dev->buffer_free(
++					(u8 *)__va(tx_desc_p->data_pointer),
++						(void *) pch->opt[j]);
++				}
++				tx_desc_p->status.field.OWN = CPU_OWN;
++				memset(tx_desc_p, 0, sizeof(struct tx_desc));
++			}
++			if (pch->req_irq_to_free != dma_chan[cn].irq)
++				disable_irq(dma_chan[cn].irq);
++			pch->desc_base = (u32) g_desc_list +
++				cn * dma_desc_num * 8;
++			pch->desc_len = dma_desc_num;
++
++			pch->channel_packet_drop_enable =
++				DMA_DEF_CHAN_BASED_PKT_DROP_EN;
++			pch->peri_to_peri = 0;
++			pch->loopback_enable = 0;
++			pch->req_irq_to_free = -1;
++		}
++		/* XXX, Should free buffer that is not transferred by dma */
++
++	}
++	for (i = 0; i < dev->max_rx_chan_num; i++) {
++		pch = dev->rx_chan[i];
++		cn = CN_GET(pch);
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		stats->dma_int_status &= ~(1 << cn);
++		pch->curr_desc = 0;
++		pch->prev_desc = 0;
++		pch->control = IFX_DMA_CH_OFF;
++		ltq_dma_w32(cn, DMA_CS);
++		if (ltq_dma_r32(DMA_CS) != cn) {
++			pr_info(":%d:%s: *DMA_CS (%d) != cn (%d)\n",
++				__LINE__, __func__, ltq_dma_r32(DMA_CS), cn);
++		}
++		/* XXX, should disable all interrupts? */
++		ltq_dma_w32(0, DMA_CIE);
++		/* Disable this channel interrupt */
++		ltq_dma_w32_mask((1 << cn), 0, DMA_IRNEN);
++		ltq_dma_w32((1 << cn), DMA_IRNCR);
++		ltq_dma_w32_mask(DMA_CCTRL_ON, 0 , DMA_CCTRL);
++		for (j = 10000; (ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_ON)
++			&& j > 0; j--)
++			;
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		if (j == 0)
++			pr_info("Port %d RX CH %d: can not be turned off, DMA_CCTRL %08x\n",
++				port_no, i, ltq_dma_r32(DMA_CCTRL));
++		for (j = 0; j < pch->desc_len; j++) {
++			rx_desc_p = (struct rx_desc *) pch->desc_base + j;
++			if (pch->loopback_enable ||
++				(rx_desc_p->status.field.OWN == CPU_OWN
++				&& rx_desc_p->status.field.C)
++				|| (rx_desc_p->status.field.OWN == DMA_OWN
++				&& rx_desc_p->status.field.data_length > 0)) {
++				dev->buffer_free((u8 *)
++					__va(rx_desc_p->data_pointer),
++					(void *) pch->opt[j]);
++			}
++		}
++		if (pch->req_irq_to_free != dma_chan[cn].irq)
++			disable_irq(dma_chan[cn].irq);
++		pch->desc_base = (u32) g_desc_list + cn * dma_desc_num * 8;
++		pch->desc_len = dma_desc_num;
++
++		pch->channel_packet_drop_enable =
++			DMA_DEF_CHAN_BASED_PKT_DROP_EN;
++		pch->peri_to_peri = 0;
++		pch->loopback_enable = 0;
++		pch->req_irq_to_free = -1;
++	}
++	/*  Port configuration stuff  */
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	ltq_dma_w32(port_no, DMA_PS);
++	reg_val = DMA_PCTRL_TXWGT_SET_VAL(DMA_TX_PORT_DEFAULT_WEIGHT)
++		| DMA_PCTRL_TXENDI_SET_VAL(DMA_DEFAULT_TX_ENDIANNESS)
++		| DMA_PCTRL_RXENDI_SET_VAL(DMA_DEFAULT_RX_ENDIANNESS)
++		| DMA_PCTRL_PDEN_SET_VAL(DMA_DEF_PORT_BASED_PKT_DROP_EN)
++		| DMA_PCTRL_TXBL_SET_VAL(DMA_PORT_DEFAULT_TX_BURST_LEN)
++		| DMA_PCTRL_RXBL_SET_VAL(DMA_PORT_DEFAULT_RX_BURST_LEN);
++	reg_val &= ~DMA_PCTRL_GPC;
++	ltq_dma_w32(reg_val, DMA_PCTRL);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	return result;
++}
++EXPORT_SYMBOL(dma_device_unregister);
++
++/*!
++  \fn int dma_device_desc_setup(_dma_device_info *dma_dev, char *buf,
++   size_t len)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Setup the DMA channel descriptor.
++
++  This function setup the descriptor of the DMA channel used by client driver.
++  The client driver will handle the buffer allocation and
++  do proper checking of buffer for DMA burst alignment.
++  Handle with care when call this function as well as dma_device_read function
++  (in both function the current descriptor is incrementing)
++
++  \param    dma_dev --> pointer to DMA device structure
++  \param    buf     --> Pointer to the Databuffer
++  \param    len     --> number of bytes.
++  \return 0
++*/
++int dma_device_desc_setup(_dma_device_info *dma_dev, char *buf,
++			size_t len)
++{
++	int byte_offset = 0;
++	struct rx_desc *rx_desc_p;
++
++	_dma_channel_info *pch =
++		dma_dev->rx_chan[dma_dev->current_rx_chan];
++
++	rx_desc_p = (struct rx_desc *) pch->desc_base + pch->curr_desc;
++	if (!(rx_desc_p->status.field.OWN == CPU_OWN)) {
++		pr_err("%s: Err!!!(Already setup the descriptor)\n", __func__);
++		return -EBUSY;
++	}
++	dma_cache_inv((unsigned long) buf, len);
++	pch->opt[pch->curr_desc] = NULL;
++	rx_desc_p->data_pointer = (u32) CPHYSADDR((u32) buf);
++	rx_desc_p->status.word =
++		(DMA_OWN << 31) | ((byte_offset) << 23) | len;
++	smp_wmb();
++	if (!dma_hw_poll) {
++		int prev_cn;
++		unsigned long flags;
++
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		prev_cn = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(CN_GET(pch), DMA_CS);
++		/* trigger descriptor fetching by clearing DUR interrupt */
++		ltq_dma_w32(DMA_CIS_DUR, DMA_CIS);
++		ltq_dma_w32(prev_cn, DMA_CS);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	}
++	/* Increase descriptor index and process wrap around
++	   Handle it with care when use the dma_device_desc_setup fucntion* */
++	if (!pch->desc_handle) {
++		pch->curr_desc++;
++		if (pch->curr_desc == pch->desc_len)
++			pch->curr_desc = 0;
++	}
++	return 0;
++}
++EXPORT_SYMBOL(dma_device_desc_setup);
++
++/*!
++  \fn int dma_device_clear_desc_status_word(_dma_device_info *dma_dev,
++  int dir)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Clear the descriptor status word
++
++  Clear the status word of the descriptor from the client driver once receive
++  a pseudo interrupt(RCV_INT)
++  from the DMA module to avoid duplicate interrupts from tasklet.
++  This function used to exit from DMA tasklet(tasklet don't need to run again
++  and again once the DMA owner bit belongs to CPU i.e. packet is available in
++  descriptor ) This is used to avoid multiple pseudo interrupt (RCV_INT) per
++  packet. All buffers are provided by DMA client. DMA client driver has no
++  method to modify descriptor state. The only method is to change descriptor
++  status.
++
++  \param    dma_dev --> pointer to DMA device structure
++  \param    dir     --> Direction of the channel
++  \return 0
++*/
++int dma_device_clear_desc_status_word(_dma_device_info *dma_dev, int dir)
++{
++	struct rx_desc *rx_desc_p;
++	struct tx_desc *tx_desc_p;
++	_dma_channel_info *pch = NULL;
++
++	if (dir == IFX_DMA_TX_CH) {
++		pch = dma_dev->tx_chan[dma_dev->current_tx_chan];
++		if (!pch->curr_desc) {
++			tx_desc_p = (struct tx_desc *) pch->desc_base;
++			tx_desc_p->status.word = 0x0;
++		} else {
++			tx_desc_p = (struct tx_desc *) pch->desc_base +
++				(pch->curr_desc - 1);
++			tx_desc_p->status.word = 0x0;
++		}
++		smp_wmb();
++	} else if (dir == IFX_DMA_RX_CH) {
++		pch = dma_dev->rx_chan[dma_dev->current_rx_chan];
++		if (!pch->curr_desc) {
++			rx_desc_p = (struct rx_desc *) pch->desc_base;
++			rx_desc_p->status.word = 0x0;
++		} else {
++			rx_desc_p = (struct rx_desc *) pch->desc_base +
++				(pch->curr_desc - 1);
++			rx_desc_p->status.word = 0x0;
++		}
++		smp_wmb();
++	} else {
++		pr_err("%s: Unknow channel direction\n", __func__);
++		return -EINVAL;
++	}
++	return 0;
++}
++EXPORT_SYMBOL(dma_device_clear_desc_status_word);
++
++/*!
++  \fn void dma_device_setup_class_val(_dma_channel_info* pch, int cls)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Set the DMA chanel class value
++
++  This function used to setup the DMA class value..
++  \param    pch --> pointer to DMA channel structure
++  \param    cls --> chanel Class value
++  \return void
++*/
++void dma_device_setup_class_val(_dma_channel_info *pch, int cls)
++{
++	unsigned long flags;
++	int prev_cn, cn = CN_GET(pch);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	prev_cn = ltq_dma_r32(DMA_CS);
++	ltq_dma_w32(cn, DMA_CS);
++	ltq_dma_w32_mask(0, dma_class_value_set(cls), DMA_CCTRL);
++	ltq_dma_w32(prev_cn, DMA_CS);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++}
++EXPORT_SYMBOL(dma_device_setup_class_val);
++
++/*!
++  \fn int dma_device_clear_int(_dma_device_info *dma_dev, int dir)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Clear the interrupt status flag
++
++  Clear the interrupt status flag from the client driver once receive
++  pseudo interrupt
++  from the dma driver to avoid duplicate interrupts from tasklet.
++  This function used to exit from DMA tasklet(tasklet don't need to
++  run again and again until the DMA owner bit is cleared)
++  This is used to avoid multiple pseudo interrupt (RCV_INT) per packet.
++  All buffers are provided by DMA client. DMA client driver has no method
++  to modify descriptor state. The only method is to change interrupt status
++
++  \param    dma_dev --> pointer to DMA device structure
++  \param    dir     --> Direction of the channel
++  \return 0
++*/
++int dma_device_clear_int(_dma_device_info *dma_dev, int dir)
++{
++	int cn;
++	unsigned long flags;
++	_dma_channel_info *pch = 0;
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++
++	if (dir == IFX_DMA_TX_CH)
++		pch = dma_dev->tx_chan[dma_dev->current_tx_chan];
++	else if (dir == IFX_DMA_RX_CH)
++		pch = dma_dev->rx_chan[dma_dev->current_rx_chan];
++	else {
++		pr_err("%s: Unknow channel direction\n", __func__);
++		return -EINVAL;
++	}
++	cn = CN_GET(pch);
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	stats->dma_int_status &= ~(1 << cn);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	return 0;
++}
++EXPORT_SYMBOL(dma_device_clear_int);
++
++/*!
++ \fn void poll_dma_ownership_bit(_dma_device_info *dma_dev)
++ \ingroup  IFX_DMA_DRV_API
++ \brief    poll the rx descriptor OWN and C bits
++ \param    dma_dev --> pointer to DMA device structure
++ \param    dir     --> Direction of the channel
++ \return
++*/
++void poll_dma_ownership_bit(_dma_device_info *dma_dev)
++{
++	struct rx_desc *rx_desc_p;
++
++	_dma_channel_info *pch =
++		dma_dev->rx_chan[dma_dev->current_rx_chan];
++	rx_desc_p = (struct rx_desc *) pch->desc_base + pch->curr_desc;
++
++	while (rx_desc_p->status.field.OWN
++		|| !(rx_desc_p->status.field.C))
++		;
++}
++EXPORT_SYMBOL(poll_dma_ownership_bit);
++
++/*!
++  \fn int dma_device_read(struct dma_device_info* dma_dev,
++   unsigned char** dataptr, void** opt)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Get data packet from DMA Rx channel.
++
++  This functions is called when the client driver gets a pseudo
++  * DMA interrupt(RCV_INT). Handle with care when call this function
++   as well as dma_device_desc_setup function.
++  Allocated memory data pointer should align with DMA burst length.
++  The follwoing action are done:
++*   Get current receive descriptor
++*   Allocate memory via buffer alloc callback function. If buffer
++ allocation failed, then return 0 and use the existing data pointer.
++*   Pass valid descriptor data pointer to the client buffer pointer.
++*   Update current descriptor position.
++*   Clear the descriptor status once it's called when the buffer is
++*    not allocated with client driver and also set desc_handle is 1
++
++  \param    dma_dev     --> pointer to DMA device structure
++  \param    dataptr     --> pointer to received data
++  \param    opt         --> Generic pointer
++  \return   -1          --> Data is not available
++	0           --> Data is available but buffer allocation for new
++		descriptor failed.
++	len	--> Valid packet data length.
++*/
++int dma_device_read(struct dma_device_info *dma_dev,
++		unsigned char **dataptr, void **opt)
++{
++	unsigned long sys_flag;
++	unsigned char *buf;
++	int len, byte_offset = 0;
++	void *p = NULL;
++	struct rx_desc *rx_desc_p;
++	_dma_channel_info *pch =
++		dma_dev->rx_chan[dma_dev->current_rx_chan];
++
++	spin_lock_irqsave(&pch->irq_lock, sys_flag);
++	/* Get the rx data first */
++	rx_desc_p = (struct rx_desc *) pch->desc_base + pch->curr_desc;
++	if (!((rx_desc_p->status.field.OWN == CPU_OWN)
++		&& rx_desc_p->status.field.C)) {
++		spin_unlock_irqrestore(&pch->irq_lock, sys_flag);
++		return -1;
++	}
++	buf = (u8 *) __va(rx_desc_p->data_pointer);
++	*(u32 *) dataptr = (u32) buf;
++	len = rx_desc_p->status.field.data_length;
++	if (opt)
++		*(int *) opt = (int) pch->opt[pch->curr_desc];
++
++	/* Replace with a new allocated buffer */
++	buf = dma_dev->buffer_alloc(pch->packet_size, &byte_offset, &p);
++	if (buf != NULL) {
++		pch->opt[pch->curr_desc] = p;
++		if (ltq_is_ase())
++			dma_cache_wback_inv((unsigned long) buf, 64);
++		else
++			dma_cache_inv((unsigned long) buf, pch->packet_size);
++		rx_desc_p->data_pointer = (u32) CPHYSADDR((u32) buf);
++		rx_desc_p->status.word = (DMA_OWN << 31) | ((byte_offset) << 23)
++			| pch->packet_size;
++		smp_wmb();
++	} else {
++
++	/** It will handle client driver using the dma_device_desc_setup
++	 * function. So, handle with care. */
++		if (!pch->desc_handle) {
++			*(u32 *) dataptr = 0;
++			if (opt)
++				*(int *) opt = 0;
++			len = 0;
++			rx_desc_p->status.word = ((DMA_OWN << 31) |
++				 ((byte_offset) << 23) | pch->packet_size);
++			smp_wmb();
++		} else {
++			/* Better to clear used descriptor status bits
++			 * (C, SOP & EOP) to avoid multiple read access as
++			 * well as to keep current descriptor pointers
++			 * correctly */
++			rx_desc_p->status.word = 0;
++		}
++	}
++	if (!dma_hw_poll) {
++		unsigned long flags;
++
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		if (pch->dur) {
++			unsigned int prev_cn = ltq_dma_r32(DMA_CS);
++
++			ltq_dma_w32(CN_GET(pch), DMA_CS);
++			ltq_dma_w32(DMA_CIS_DUR, DMA_CIS);
++			ltq_dma_w32(prev_cn, DMA_CS);
++			pch->dur = 0;
++		}
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	}
++	/* Increase descriptor index and process wrap around
++	   Handle ith care when use the dma_device_desc_setup fucntion */
++	pch->curr_desc++;
++	if (pch->curr_desc == pch->desc_len)
++		pch->curr_desc = 0;
++	spin_unlock_irqrestore(&pch->irq_lock, sys_flag);
++	return len;
++}
++EXPORT_SYMBOL(dma_device_read);
++
++/*!
++  \fn int dma_device_write(struct dma_device_info* dma_dev,
++  unsigned char* dataptr, int len, void* opt)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Write data Packet to DMA Tx channel.
++
++  This function gets the data packet from the client driver and sends over on
++  DMA channel.
++  The following actions are done
++*   Descriptor is not available then sends a pseudo DMA
++*   interrupt(TX_BUF_FULL_INT) to client driver to stop transmission.
++*   The client driver should stop the transmission and enable the Tx dma
++*   channel interrupt.
++*   Once descriptor is free then the DMA interrupt handler send an pseudo
++*   DMA interrupt(TRANSMIT_CPT_INT)
++*   to client driver to start transmission.
++*   The client driver should start the transmission and disable the Tx dma
++*   channel interrupt.
++*   Previous descriptor already sendout, then Free memory via buffer free
++*   callback function
++*   Update current descriptor position
++*   If the channel is not open, then it will open.
++  \param   dma_dev --> pointer to DMA device structure
++  \param   dataptr --> pointer to Transmit data
++  \param   len     --> length of transmit data
++  \param   opt     --> Generic void pointer
++  \return length   --> Return the length of the packet.
++*/
++int dma_device_write(struct dma_device_info *dma_dev,
++		unsigned char *dataptr, int len, void *opt)
++{
++	unsigned long sys_flag, flags;
++	u32 cctrls;
++	_dma_channel_info *pch;
++	int cn, byte_offset, prev_ch;
++	struct tx_desc *tx_desc_p;
++	pch = dma_dev->tx_chan[dma_dev->current_tx_chan];
++	cn = CN_GET(pch);
++
++	spin_lock_irqsave(&pch->irq_lock, sys_flag);
++
++	/* Set the previous descriptor pointer to verify data is sendout or not
++	 * If its sendout then clear the buffer based on the client driver
++	 * buffer free callaback function */
++	tx_desc_p = (struct tx_desc *) pch->desc_base + pch->prev_desc;
++	while (tx_desc_p->status.field.OWN == CPU_OWN
++		&& tx_desc_p->status.field.C) {
++		dma_dev->buffer_free((u8 *) __va(tx_desc_p->data_pointer),
++			pch->opt[pch->prev_desc]);
++		memset(tx_desc_p, 0, sizeof(struct tx_desc));
++		pch->prev_desc = (pch->prev_desc + 1) % (pch->desc_len);
++		tx_desc_p = (struct tx_desc *) pch->desc_base + pch->prev_desc;
++	}
++	/* Set the current descriptor pointer */
++	tx_desc_p = (struct tx_desc *) pch->desc_base + pch->curr_desc;
++
++	/* Check whether this descriptor is available CPU and DMA excute
++	 * tasks in its own envionment. DMA will change ownership and
++	 * complete bit. Check the descriptors are avaliable or not to
++	 * process the packet. */
++	if (tx_desc_p->status.field.OWN == DMA_OWN
++		|| tx_desc_p->status.field.C) {
++		/* This descriptor has not been released */
++		dma_dev->intr_handler(dma_dev, TX_BUF_FULL_INT);
++		spin_unlock_irqrestore(&pch->irq_lock, sys_flag);
++		return 0;
++	}
++
++	pch->opt[pch->curr_desc] = opt;
++	byte_offset = ((u32) CPHYSADDR((u32) dataptr)) %
++		((dma_dev->tx_burst_len) * 4);
++	if (ltq_is_ase())
++		dma_cache_wback_inv((unsigned long) dataptr, len);
++	else
++		dma_cache_wback((unsigned long) dataptr, len);
++	tx_desc_p->data_pointer = (u32)CPHYSADDR((u32)dataptr) - byte_offset;
++	tx_desc_p->status.word = (DMA_OWN << 31) | DMA_DESC_SOP_SET |
++		DMA_DESC_EOP_SET | ((byte_offset) << 23) | len;
++	smp_wmb();
++	pch->curr_desc++;
++	if (pch->curr_desc == pch->desc_len)
++		pch->curr_desc = 0;
++	/* Check if the next descriptor is available */
++	tx_desc_p = (struct tx_desc *) pch->desc_base + pch->curr_desc;
++	if ((tx_desc_p->status.field.OWN == DMA_OWN)
++		/* || tx_desc_p->status.field.C */)
++		dma_dev->intr_handler(dma_dev, TX_BUF_FULL_INT);
++
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	prev_ch = ltq_dma_r32(DMA_CS);
++	ltq_dma_w32(cn, DMA_CS);
++	cctrls = ltq_dma_r32(DMA_CCTRL);
++	ltq_dma_w32(prev_ch, DMA_CS);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++
++	/* If not open this channel, open it */
++	if (!(cctrls & DMA_CCTRL_ON))
++		pch->open(pch);
++
++	if (!dma_hw_poll) {
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		prev_ch = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(cn, DMA_CS);
++		/* trigger descriptor fetching by clearing DUR interrupt */
++		ltq_dma_w32(DMA_CIS_DUR, DMA_CIS);
++		ltq_dma_w32(prev_ch, DMA_CS);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	}
++	spin_unlock_irqrestore(&pch->irq_lock, sys_flag);
++	return len;
++}
++EXPORT_SYMBOL(dma_device_write);
++
++int dma_device_write_sdio(struct dma_device_info *dma_dev,
++			u32 *chunkdataptr, void *opt)
++{
++	unsigned long flags;
++	u32 cctrls, byte_offset, prev_cn, status_word;
++	_dma_channel_info *pch;
++	int cn, len = 0;
++	int total_length, cur_length, cur_chunk;
++	struct tx_desc *tx_desc_p;
++	struct tx_desc *first_tx_desc;
++	u8 *dataptr;
++
++	pch = dma_dev->tx_chan[dma_dev->current_tx_chan];
++	cn = CN_GET(pch);
++	tx_desc_p = (struct tx_desc *) pch->desc_base + pch->prev_desc;
++	while (tx_desc_p->status.field.OWN == CPU_OWN
++		&& tx_desc_p->status.field.C) {
++		dma_dev->buffer_free((u8 *) __va(tx_desc_p->data_pointer),
++			pch->opt[pch->prev_desc]);
++		memset(tx_desc_p, 0, sizeof(struct tx_desc));
++		pch->prev_desc = (pch->prev_desc + 1) % (pch->desc_len);
++		tx_desc_p = (struct tx_desc *) pch->desc_base + pch->prev_desc;
++	}
++
++	total_length = chunkdataptr[0];
++	cur_length = 0;
++	cur_chunk = 0;
++
++	first_tx_desc = (struct tx_desc *) pch->desc_base + pch->curr_desc;
++	while (cur_length < total_length) {
++		len = chunkdataptr[1 + cur_chunk];
++		dataptr = (u8 *) chunkdataptr[2 + cur_chunk];
++		cur_length += len;
++
++		tx_desc_p = (struct tx_desc *) pch->desc_base + pch->curr_desc;
++
++		/*Check whether this descriptor is available */
++		if (tx_desc_p->status.field.OWN == DMA_OWN
++			|| tx_desc_p->status.field.C) {
++			/*if not , the tell the upper layer device */
++			dma_dev->intr_handler(dma_dev, TX_BUF_FULL_INT);
++			pr_info("%s : failed to write!\n", __func__);
++			return 0;
++		}
++		pch->opt[pch->curr_desc] = opt;
++		byte_offset = ((u32) CPHYSADDR((u32) dataptr)) %
++			((dma_dev->tx_burst_len) * 4);
++		dma_cache_wback_inv((unsigned long) dataptr, len);
++		byte_offset = byte_offset & 0x1f;
++		tx_desc_p->data_pointer =
++			(u32)CPHYSADDR((u32) dataptr) - byte_offset;
++		smp_wmb();
++		/* For the first descriptor for this packet, we do not set
++		 * the OWN bit to DM to prevent starting the DMA before
++		 * everything has been set up */
++		status_word = (byte_offset << 23) | len;
++		if (cur_chunk == 0) /* First data packet for this transaction */
++			status_word |= DMA_DESC_SOP_SET;
++		else
++			status_word |= (DMA_OWN << 31);
++
++		if (cur_length == total_length)/* Last data packet */
++			status_word |= DMA_DESC_EOP_SET;
++		tx_desc_p->status.word = status_word;
++		smp_wmb();
++
++		pch->curr_desc++;
++		if (pch->curr_desc == pch->desc_len)
++			pch->curr_desc = 0;
++		cur_chunk += 2; /* 2 words per entry (length, dataptr) */
++	}
++	/* Now let the DMA start working, by releasing the first descriptor
++	 * for this packet. */
++	/*Check whether this descriptor is available */
++	tx_desc_p = (struct tx_desc *) pch->desc_base + pch->curr_desc;
++	if (tx_desc_p->status.field.OWN == DMA_OWN) {
++		/*if not , the tell the upper layer device */
++		dma_dev->intr_handler(dma_dev, TX_BUF_FULL_INT);
++	}
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	prev_cn = ltq_dma_r32(DMA_CS);
++	ltq_dma_w32(cn, DMA_CS);
++	cctrls = ltq_dma_r32(DMA_CCTRL);
++	first_tx_desc->status.word |= (DMA_OWN << 31);
++	ltq_dma_w32_mask(0, DMA_CIS_ALL, DMA_CIS);
++	ltq_dma_w32(prev_cn, DMA_CS);
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++
++	if (!(cctrls & DMA_CCTRL_ON))
++		pch->open(pch);
++
++	if (!dma_hw_poll) {
++		spin_lock_irqsave(&g_dma_spinlock, flags);
++		prev_cn = ltq_dma_r32(DMA_CS);
++		ltq_dma_w32(cn, DMA_CS);
++		/* trigger descriptor fetching by clearing DUR interrupt */
++		ltq_dma_w32(DMA_CIS_DUR, DMA_CIS);
++		ltq_dma_w32(prev_cn, DMA_CS);
++		spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	}
++	return len;
++}
++EXPORT_SYMBOL(dma_device_write_sdio);
++
++/*!
++  \fn int dma_device_poll(struct dma_device_info* dma_dev,
++    int work_to_do, int *work_done)
++  \ingroup  IFX_DMA_DRV_API
++  \brief Poll the DMA device
++
++  This function keeps polls the DMA owner bit status.
++  if valid packets available then  send a pseudo interrupt to the client driver
++
++  \param  dma_dev     --> pointer to DMA device structure
++  \param  work_to_do  --> poll budget value
++  \param  work_done   --> Remaining budget value
++  \return 0/1.
++*/
++int dma_device_poll(struct dma_device_info *dma_dev, int work_to_do,
++		    int *work_done)
++{
++#ifdef CONFIG_NAPI_ENABLED
++	int ret, i, dma_int_status_mask, cn = 0;
++	u32 prev_cn, cis;
++	_dma_channel_info *pch;
++	struct rx_desc *rx_desc_p;
++	struct dma_sched_status *stats =
++		&per_cpu(dma_sched_stats, smp_processor_id());
++	unsigned long flags;
++
++	dma_int_status_mask = 0;
++	for (i = 0; i < dma_dev->max_rx_chan_num; i++)
++		dma_int_status_mask |= (1 << CN_GET(dma_dev->rx_chan[i]));
++
++	i = 0;
++	while ((stats->dma_poll_int_status & dma_int_status_mask)) {
++		cn = CN_GET(dma_dev->rx_chan[i]);
++		pch = &dma_chan[cn];
++		rx_desc_p = (struct rx_desc *) pch->desc_base + pch->curr_desc;
++
++		if ((rx_desc_p->status.field.OWN == CPU_OWN)
++			&& rx_desc_p->status.field.C) {
++			dma_dev->current_rx_chan = pch->rel_chan_no;
++			if (dma_dev->intr_handler)
++				dma_dev->intr_handler(dma_dev, RCV_INT);
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			prev_cn = ltq_dma_r32(DMA_CS);
++			ltq_dma_w32(cn, DMA_CS);
++			cis = ltq_dma_r32(DMA_CIS);
++			ltq_dma_w32((cis | DMA_CIS_ALL), DMA_CIS);
++			ltq_dma_w32(prev_cn, DMA_CS);
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++			if (!dma_hw_poll) {
++				if (cis & DMA_CIS_DUR)
++					pch->dur = 1;
++			}
++
++			if (++*work_done >= work_to_do)
++				return 1;
++		} else {
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++			/* Clear the DMA Node status register bit to remove
++			 * the dummy interrupts */
++			ltq_dma_w32((1 << cn), DMA_IRNCR);
++			prev_cn = ltq_dma_r32(DMA_CS);
++			ltq_dma_w32(cn, DMA_CS);
++			cis = ltq_dma_r32(DMA_CIS);
++
++			/*
++			 * In most cases, interrupt bit has been cleared in the
++			 * first branch, it will hit the following. However,
++			 * in some corner cases, the above branch doesn't
++			 * happen, so it indicates that interrupt is received
++			 * actually, but OWN bit and Complete bit not update
++			 * yet, so go back to DMA tasklet and let DMA tasklet
++			 * wait <polling>. This is the most efficient way.
++			 * NB, only EOP and Descript Complete interrupt enabled
++			 * in Channel Interrupt Enable Register.
++			 */
++			if ((cis & ltq_dma_r32(DMA_CIE)) == 0) {
++				if (rx_desc_p->status.field.OWN != CPU_OWN) {
++					stats->dma_poll_int_status &=
++						~(1 << cn);
++				}
++				/*
++				 * Enable this channel interrupt again after
++				 * processing all packets available
++				 * Placing this line at the last line will avoid
++				 * interrupt coming again in heavy load to great
++				 * extent
++				 */
++				ltq_dma_w32_mask(0, (1 << cn), DMA_IRNEN);
++			}
++			ltq_dma_w32(prev_cn, DMA_CS);
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++		}
++		if (!dma_hw_poll) {
++			/* remember that we ran out of descriptors,
++			 * don't trigger polling now */
++			if (cis & DMA_CIS_DUR)
++				pch->dur = 1;
++		}
++		if (++i == dma_dev->max_rx_chan_num)
++			i = 0;
++	}
++	spin_lock_irqsave(&g_dma_spinlock, flags);
++	if ((stats->dma_poll_int_status & dma_int_status_mask) == 0) {
++		if (dma_dev->inactivate_poll) {
++			spin_unlock_irqrestore(&g_dma_spinlock, flags);
++			dma_dev->inactivate_poll(dma_dev);
++			spin_lock_irqsave(&g_dma_spinlock, flags);
++		}
++		ret = 0;
++	} else
++		ret = 1;
++
++	spin_unlock_irqrestore(&g_dma_spinlock, flags);
++	return ret;
++#else
++	return 0;
++#endif /* CONFIG_NAPI_ENABLED */
++}
++EXPORT_SYMBOL(dma_device_poll);
++
++static void *dma_desc_seq_start(struct seq_file *s, loff_t *pos)
++{
++	if (*pos >= dma_max_chans)
++		return NULL;
++	return &dma_chan[*pos];
++}
++
++static void *dma_desc_seq_next(struct seq_file *s, void *v, loff_t *pos)
++{
++	if (++*pos >= dma_max_chans)
++		return NULL;
++	return  &dma_chan[*pos];
++}
++
++static void dma_desc_seq_stop(struct seq_file *s, void *v)
++{
++
++}
++
++static int dma_desc_seq_show(struct seq_file *s, void *v)
++{
++	u32 *p;
++	int desc_pos;
++	_dma_channel_info *pch = (_dma_channel_info *)v;
++	int cn = CN_GET(pch);
++
++	p = (u32 *) pch->desc_base;
++
++	for (desc_pos = 0; desc_pos < pch->desc_len; desc_pos++) {
++		if (desc_pos == 0) {
++			if (pch->dir == IFX_DMA_RX_CH) {
++				seq_printf(s,
++					"channel %d %s Rx descriptor list:\n",
++					cn, ((_dma_device_info *)pch->dma_dev)
++					->device_name);
++			} else {
++				seq_printf(s,
++					"channel %d %s Tx descriptor list:\n",
++					cn, ((_dma_device_info *)pch->dma_dev)
++					->device_name);
++			}
++			seq_puts(s, " no  address        data pointer command");
++			seq_puts(s, " bits Own, Complete, SoP, EoP, Offset)\n");
++			seq_puts(s, "----------------------------------------");
++			seq_puts(s, "--------------------------------------\n");
++		}
++		seq_printf(s, "%3d  ", desc_pos);
++		seq_printf(s, "0x%08x     ", (u32) (p + (desc_pos * 2)));
++		seq_printf(s, "%08x     ", *(p + (desc_pos * 2 + 1)));
++		seq_printf(s, "%08x     ", *(p + (desc_pos * 2)));
++
++		if (*(p + (desc_pos * 2)) & 0x80000000)
++			seq_puts(s, "D ");
++		else
++			seq_puts(s, "C ");
++		if (*(p + (desc_pos * 2)) & 0x40000000)
++			seq_puts(s, "C ");
++		else
++			seq_puts(s, "c ");
++		if (*(p + (desc_pos * 2)) & 0x20000000)
++			seq_puts(s, "S ");
++		else
++			seq_puts(s, "s ");
++		if (*(p + (desc_pos * 2)) & 0x10000000)
++			seq_puts(s, "E ");
++		else
++			seq_puts(s, "e ");
++		/* byte offset is different for rx and tx descriptors */
++		if (pch->dir == IFX_DMA_RX_CH) {
++			seq_printf(s, "%01x ",
++				(*(p + (desc_pos * 2)) & 0x01800000) >> 23);
++		} else {
++			seq_printf(s, "%02x ",
++				(*(p + (desc_pos * 2)) & 0x0F800000) >> 23);
++		}
++		if (pch->curr_desc == desc_pos)
++			seq_puts(s, "<- CURR");
++		if (pch->prev_desc == desc_pos)
++			seq_puts(s, "<- PREV");
++		seq_puts(s, "\n");
++	}
++	return 0;
++}
++
++static const struct seq_operations dma_desc_seq_ops = {
++	.start = dma_desc_seq_start,
++	.next = dma_desc_seq_next,
++	.stop = dma_desc_seq_stop,
++	.show = dma_desc_seq_show,
++};
++
++static int dma_desc_seq_open(struct inode *inode, struct file *file)
++{
++	return seq_open(file, &dma_desc_seq_ops);
++}
++
++static const struct file_operations dma_desc_proc_fops = {
++	.open = dma_desc_seq_open,
++	.read = seq_read,
++	.llseek = seq_lseek,
++	.release = seq_release,
++};
++
++static void *dma_weight_seq_start(struct seq_file *s, loff_t *pos)
++{
++	seq_puts(s, "Qos dma channel weight list\n");
++	seq_puts(s, "chan_no default_weight current_weight   device   Tx/Rx\n");
++	if (*pos >= dma_max_chans)
++		return NULL;
++	return &dma_chan[*pos];
++}
++
++static void *dma_weight_seq_next(struct seq_file *s, void *v, loff_t *pos)
++{
++	if (++*pos >= dma_max_chans)
++		return NULL;
++	return  &dma_chan[*pos];
++}
++
++static void dma_weight_seq_stop(struct seq_file *s, void *v)
++{
++
++}
++
++static int dma_weight_seq_show(struct seq_file *s, void *v)
++{
++	_dma_channel_info *pch = (_dma_channel_info *)v;
++	int cn = CN_GET(pch);
++
++	if (pch->dir == IFX_DMA_RX_CH) {
++		seq_printf(s, "     %2d      %08x        %08x      %10s   Rx\n",
++			cn, pch->default_weight, pch->weight,
++			((_dma_device_info *) (pch->dma_dev))->device_name);
++	} else {
++		seq_printf(s, "     %2d      %08x        %08x      %10s   Tx\n",
++			cn, pch->default_weight, pch->weight,
++			((_dma_device_info *) (pch->dma_dev))->device_name);
++	}
++	return 0;
++}
++
++static const struct seq_operations dma_weight_seq_ops = {
++	.start = dma_weight_seq_start,
++	.next = dma_weight_seq_next,
++	.stop = dma_weight_seq_stop,
++	.show = dma_weight_seq_show,
++};
++
++static int dma_weight_seq_open(struct inode *inode, struct file *file)
++{
++	return seq_open(file, &dma_weight_seq_ops);
++}
++
++static const struct file_operations dma_weight_proc_fops = {
++	.open = dma_weight_seq_open,
++	.read = seq_read,
++	.llseek = seq_lseek,
++	.release = seq_release,
++};
++
++static void *dma_reg_seq_start(struct seq_file *s, loff_t *pos)
++{
++	if (*pos >= dma_max_chans)
++		return NULL;
++	return &dma_chan[*pos];
++}
++
++static void *dma_reg_seq_next(struct seq_file *s, void *v, loff_t *pos)
++{
++	if (++*pos >= dma_max_chans)
++		return NULL;
++	return  &dma_chan[*pos];
++}
++
++static void dma_reg_seq_stop(struct seq_file *s, void *v)
++{
++}
++
++static int dma_reg_seq_show(struct seq_file *s, void *v)
++{
++	_dma_channel_info *pch = (_dma_channel_info *)v;
++	int cn = CN_GET(pch);
++
++	seq_puts(s, "-----------------------------------------\n");
++	if (pch->dir == IFX_DMA_RX_CH) {
++		seq_printf(s, "Channel %d - Device %s Rx\n", cn,
++			((_dma_device_info *) (pch->dma_dev))->device_name);
++	} else {
++		seq_printf(s, "Channel %d - Device %s Tx\n", cn,
++			((_dma_device_info *) (pch->dma_dev))->device_name);
++	}
++	ltq_dma_w32(cn, DMA_CS);
++	seq_printf(s, "DMA_CCTRL=  %08x\n", ltq_dma_r32(DMA_CCTRL));
++	seq_printf(s, "DMA_CDBA=   %08x\n", ltq_dma_r32(DMA_CDBA));
++	seq_printf(s, "DMA_CIE=    %08x\n", ltq_dma_r32(DMA_CIE));
++	seq_printf(s, "DMA_CIS=    %08x\n", ltq_dma_r32(DMA_CIS));
++	seq_printf(s, "DMA_CDLEN=  %08x\n", ltq_dma_r32(DMA_CDLEN));
++	return 0;
++}
++
++static const struct seq_operations dma_reg_seq_ops = {
++	.start = dma_reg_seq_start,
++	.next = dma_reg_seq_next,
++	.stop = dma_reg_seq_stop,
++	.show = dma_reg_seq_show,
++};
++
++static int dma_reg_seq_open(struct inode *inode, struct file *file)
++{
++	return seq_open(file, &dma_reg_seq_ops);
++}
++
++static const struct file_operations dma_reg_proc_fops = {
++	.open = dma_reg_seq_open,
++	.read = seq_read,
++	.llseek = seq_lseek,
++	.release = seq_release,
++};
++
++static int dma_gen_port_read_proc(struct seq_file *s, void *v)
++{
++	int i;
++
++	seq_puts(s, "\nGeneral DMA Registers\n");
++	seq_puts(s, "-----------------------------------------\n");
++	seq_printf(s, "CLC=        %08x\n", ltq_dma_r32(DMA_CLC));
++	seq_printf(s, "ID=         %08x\n", ltq_dma_r32(DMA_ID));
++	seq_printf(s, "DMA_CPOLL=  %08x\n", ltq_dma_r32(DMA_CPOLL));
++	seq_printf(s, "DMA_CS=     %08x\n", ltq_dma_r32(DMA_CS));
++	seq_printf(s, "DMA_PS=     %08x\n", ltq_dma_r32(DMA_PS));
++	seq_printf(s, "DMA_IRNEN=  %08x\n", ltq_dma_r32(DMA_IRNEN));
++	seq_printf(s, "DMA_IRNCR=  %08x\n", ltq_dma_r32(DMA_IRNCR));
++	seq_printf(s, "DMA_IRNICR= %08x\n", ltq_dma_r32(DMA_IRNICR));
++
++	seq_puts(s, "\nDMA Port Registers\n");
++	seq_puts(s, "-----------------------------------------\n");
++	for (i = 0; i < dma_max_ports; i++) {
++		ltq_dma_w32(i, DMA_PS);
++		seq_printf(s, "Port %d DMA_PCTRL= %08x\n",
++			i, ltq_dma_r32(DMA_PCTRL));
++	}
++
++	return 0;
++}
++static int dma_gen_port_read_proc_open(struct inode *inode, struct file *file)
++{
++	return single_open(file, dma_gen_port_read_proc, NULL);
++}
++
++static const struct file_operations dma_gen_port_proc_fops = {
++	.open           = dma_gen_port_read_proc_open,
++	.read           = seq_read,
++	.llseek         = seq_lseek,
++	.release        = single_release,
++};
++
++/**  Set the  default values in dma device/channel structure */
++static int map_dma_chan(struct ifx_dma_chan_map *map)
++{
++	int i, j, result;
++	unsigned int reg_val;
++
++	for (i = 0; i < dma_max_ports; i++)
++		strcpy(dma_devs[i].device_name, dma_device_name[i]);
++
++	for (i = 0; i < dma_max_chans; i++) {
++		dma_chan[i].irq = map[i].irq;
++
++		result = request_irq(dma_chan[i].irq, dma_interrupt,
++			IRQF_DISABLED, "dma-core", (void *) &dma_chan[i]);
++		if (result) {
++			pr_err("%s: Request IRQ failed!!!\n", __func__);
++			free_irq(dma_chan[i].irq, (void *) &dma_chan[i]);
++			return -EFAULT;
++		}
++		disable_irq(dma_chan[i].irq);
++	}
++	for (i = 0; i < dma_max_ports; i++) {
++		dma_devs[i].num_tx_chan = 0;
++		dma_devs[i].num_rx_chan = 0;
++		dma_devs[i].max_rx_chan_num = 0;
++		dma_devs[i].max_tx_chan_num = 0;
++		dma_devs[i].mem_port_control = 0;
++		dma_devs[i].port_tx_weight = DMA_TX_PORT_DEFAULT_WEIGHT;
++		dma_devs[i].tx_endianness_mode = DMA_DEFAULT_TX_ENDIANNESS;
++		dma_devs[i].rx_endianness_mode = DMA_DEFAULT_RX_ENDIANNESS;
++		dma_devs[i].port_packet_drop_enable =
++			DMA_DEF_PORT_BASED_PKT_DROP_EN;
++		dma_devs[i].port_packet_drop_counter = 0;
++		dma_devs[i].buffer_alloc = &common_buffer_alloc;
++		dma_devs[i].buffer_free = &common_buffer_free;
++		dma_devs[i].intr_handler = NULL;
++		dma_devs[i].tx_burst_len = DMA_PORT_DEFAULT_TX_BURST_LEN;
++		dma_devs[i].rx_burst_len = DMA_PORT_DEFAULT_RX_BURST_LEN;
++		dma_devs[i].port_num = i;
++
++		for (j = 0; j < dma_max_chans; j++) {
++			dma_chan[j].byte_offset = 0;
++			dma_chan[j].open = &open_chan;
++			dma_chan[j].close = &close_chan;
++			dma_chan[j].reset = &reset_chan;
++			dma_chan[j].enable_irq = &enable_ch_irq;
++			dma_chan[j].disable_irq = &disable_ch_irq;
++			dma_chan[j].rel_chan_no = map[j].rel_chan_no;
++			dma_chan[j].control = IFX_DMA_CH_OFF;
++			dma_chan[j].default_weight = DMA_CH_DEFAULT_WEIGHT;
++			dma_chan[j].weight = dma_chan[j].default_weight;
++			dma_chan[j].tx_channel_weight =
++				DMA_TX_CHAN_DEFAULT_WEIGHT;
++			dma_chan[j].channel_packet_drop_enable =
++				DMA_DEF_CHAN_BASED_PKT_DROP_EN;
++			dma_chan[j].channel_packet_drop_counter = 0;
++			dma_chan[j].class_value = 0;
++			dma_chan[j].peri_to_peri = 0;
++			dma_chan[j].global_buffer_len = 0;
++			dma_chan[j].loopback_enable = 0;
++			dma_chan[j].loopback_channel_number = 0;
++			dma_chan[j].curr_desc = 0;
++			dma_chan[j].desc_handle = 0;
++			dma_chan[j].prev_desc = 0;
++			dma_chan[j].req_irq_to_free = -1;
++			dma_chan[j].dur = 0;
++		}
++
++		for (j = 0; j < dma_max_chans; j++) {
++			if (strcmp(dma_devs[i].device_name,
++				map[j].dev_name) == 0) {
++				if (map[j].dir == IFX_DMA_RX_CH) {
++					dma_chan[j].dir = IFX_DMA_RX_CH;
++					dma_devs[i].max_rx_chan_num++;
++					dma_devs[i].rx_chan[dma_devs[i].
++						max_rx_chan_num - 1] =
++						&dma_chan[j];
++					dma_devs[i].rx_chan[dma_devs[i].
++						max_rx_chan_num - 1]->
++						class_value = map[j].pri;
++					dma_chan[j].dma_dev =
++						(void *) &dma_devs[i];
++					/*Program default class value */
++					ltq_dma_w32(j, DMA_CS);
++					reg_val = ltq_dma_r32(DMA_CCTRL);
++					reg_val &= ~dma_class_mask_get();
++					reg_val |=
++						dma_class_value_set(map[j].pri);
++					ltq_dma_w32(reg_val, DMA_CCTRL);
++				} else if (map[j].dir == IFX_DMA_TX_CH) {
++					dma_chan[j].dir = IFX_DMA_TX_CH;
++					dma_devs[i].max_tx_chan_num++;
++					dma_devs[i].tx_chan[dma_devs[i].
++						max_tx_chan_num - 1] =
++						&dma_chan[j];
++					dma_devs[i].tx_chan[dma_devs[i].
++						max_tx_chan_num - 1]->
++						class_value = map[j].pri;
++					dma_chan[j].dma_dev =
++						(void *) &dma_devs[i];
++					/*Program default class value */
++					ltq_dma_w32(j, DMA_CS);
++					reg_val = ltq_dma_r32(DMA_CCTRL);
++					reg_val &= ~dma_class_mask_get();
++					reg_val |=
++						dma_class_value_set(map[j].pri);
++					ltq_dma_w32(reg_val, DMA_CCTRL);
++				} else
++					pr_info("%s wrong dma channel map!\n",
++						__func__);
++
++			}
++		}
++	}
++
++	return 0;
++}
++
++static int dma_chip_init(void)
++{
++	int i;
++
++	/* Reset DMA */
++	ltq_dma_w32_mask(0, DMA_CTRL_RST, DMA_CTRL);
++	/* Disable all the interrupts first */
++	ltq_dma_w32(0, DMA_IRNEN);
++	ltq_dma_w32((1 << dma_max_chans) - 1, DMA_IRNCR);
++
++	for (i = 0; i < dma_max_chans; i++) {
++		ltq_dma_w32(i, DMA_CS);
++		ltq_dma_w32_mask(0, DMA_CCTRL_RST, DMA_CCTRL);
++		while ((ltq_dma_r32(DMA_CCTRL) & DMA_CCTRL_RST))
++			;
++		/* dislabe all interrupts here */
++		ltq_dma_w32(0, DMA_CIE);
++	}
++	for (i = 0; i < dma_max_chans; i++)
++		disable_chan(i);
++
++	if (dma_hw_poll) {
++		/* enable polling for all channels */
++		ltq_dma_w32(DMA_CPOLL_EN |
++			DMA_CPOLL_CNT_VAL(DMA_DEFAULT_POLL_VALUE), DMA_CPOLL);
++	}
++	if (dma_pkt_arb_enable) {
++		if ((ltq_get_soc_type() == SOC_TYPE_VR9)
++			&& !(ltq_r32((u32 *)0xBF401010) & 0x01)) { /* ToDo*/
++			ltq_dma_w32_mask(DMA_CTRL_PKTARB, 0, DMA_CTRL);
++		} else
++			ltq_dma_w32_mask(0, DMA_CTRL_PKTARB, DMA_CTRL);
++	}
++	return 0;
++}
++
++/** create proc for debug  info, used ifx_dma_init_module */
++static int dma_core_proc_init(void)
++{
++	struct proc_dir_entry *entry;
++
++	g_dma_dir = proc_mkdir("driver/ltq_dma", NULL);
++	if (!g_dma_dir)
++		return -ENOMEM;
++
++	entry = proc_create("register", 0,
++			g_dma_dir, &dma_reg_proc_fops);
++	if (!entry)
++		goto err1;
++
++	entry = proc_create("port", 0,
++			g_dma_dir, &dma_gen_port_proc_fops);
++	if (!entry)
++		goto err2;
++
++	entry = proc_create("chan_weight", 0,
++			g_dma_dir, &dma_weight_proc_fops);
++	if (!entry)
++		goto err3;
++
++	entry = proc_create("desc_list", 0,
++			g_dma_dir, &dma_desc_proc_fops);
++	if (!entry)
++		goto err4;
++	return 0;
++err4:
++	remove_proc_entry("chan_weight", g_dma_dir);
++err3:
++	remove_proc_entry("port", g_dma_dir);
++err2:
++	remove_proc_entry("register", g_dma_dir);
++err1:
++	remove_proc_entry("driver/ltq_dma", NULL);
++	return -ENOMEM;
++}
++
++static void dma_build_device_chan_tbl(void)
++{
++	if (ltq_is_ase()) {
++		const char device_name[3][DMA_DEV_NAME_LEN] = {
++			{"PPE"}, {"SDIO"}, {"MCTRL0"}
++		};
++		const struct ifx_dma_chan_map dma_map[10] = {
++		{0, "PPE",      IFX_DMA_RX_CH,  0,  INT_NUM_IM3_IRL0,    0},
++		{0, "PPE",      IFX_DMA_TX_CH,  0,  INT_NUM_IM3_IRL0 + 1,    0},
++		{0, "PPE",      IFX_DMA_RX_CH,  1,  INT_NUM_IM3_IRL0 + 2,    1},
++		{0, "PPE",      IFX_DMA_TX_CH,  1,  INT_NUM_IM3_IRL0 + 3,    1},
++		{0, "PPE",      IFX_DMA_RX_CH,  2,  INT_NUM_IM3_IRL0 + 4,    2},
++		{0, "PPE",      IFX_DMA_RX_CH,  3,  INT_NUM_IM3_IRL0 + 5,    3},
++		{1, "SDIO",     IFX_DMA_RX_CH,  0,  INT_NUM_IM3_IRL0 + 6,    0},
++		{1, "SDIO",     IFX_DMA_TX_CH,  0,  INT_NUM_IM3_IRL0 + 7,    0},
++		{2, "MCTRL0",   IFX_DMA_RX_CH,  0,  INT_NUM_IM3_IRL0 + 8,    0},
++		{2, "MCTRL0",   IFX_DMA_TX_CH,  0,  INT_NUM_IM3_IRL0 + 9,    0},
++		};
++		memcpy(dma_device_name, device_name, sizeof(device_name));
++		memcpy(ifx_default_dma_map, dma_map, sizeof(dma_map));
++	} else if (ltq_is_danube() || ltq_is_ar9()) {
++		const char device_name[6][DMA_DEV_NAME_LEN] = {
++			{"PPE"}, {"DEU"}, {"SPI"}, {"SDIO"},
++			{"MCTRL0"}, {"MCTRL1"}
++		};
++		const struct ifx_dma_chan_map dma_map[20] = {
++		/* portnum, device name, channel direction, class value,
++		 * IRQ number, relative channel number */
++		{0, "PPE",      IFX_DMA_RX_CH,  0,  DMA_CH0_INT,    0},
++		{0, "PPE",      IFX_DMA_TX_CH,  0,  DMA_CH1_INT,    0},
++		{0, "PPE",      IFX_DMA_RX_CH,  1,  DMA_CH2_INT,    1},
++		{0, "PPE",      IFX_DMA_TX_CH,  1,  DMA_CH3_INT,    1},
++		{0, "PPE",      IFX_DMA_RX_CH,  2,  DMA_CH4_INT,    2},
++		{0, "PPE",      IFX_DMA_TX_CH,  2,  DMA_CH5_INT,    2},
++		{0, "PPE",      IFX_DMA_RX_CH,  3,  DMA_CH6_INT,    3},
++		{0, "PPE",      IFX_DMA_TX_CH,  3,  DMA_CH7_INT,    3},
++		{1, "DEU",      IFX_DMA_RX_CH,  0,  DMA_CH8_INT,    0},
++		{1, "DEU",      IFX_DMA_TX_CH,  0,  DMA_CH9_INT,    0},
++		{1, "DEU",      IFX_DMA_RX_CH,  1,  DMA_CH10_INT,   1},
++		{1, "DEU",      IFX_DMA_TX_CH,  1,  DMA_CH11_INT,   1},
++		{2, "SPI",      IFX_DMA_RX_CH,  0,  DMA_CH12_INT,   0},
++		{2, "SPI",      IFX_DMA_TX_CH,  0,  DMA_CH13_INT,   0},
++		{3, "SDIO",     IFX_DMA_RX_CH,  0,  DMA_CH14_INT,   0},
++		{3, "SDIO",     IFX_DMA_TX_CH,  0,  DMA_CH15_INT,   0},
++		{4, "MCTRL0",   IFX_DMA_RX_CH,  0,  DMA_CH16_INT,   0},
++		{4, "MCTRL0",   IFX_DMA_TX_CH,  0,  DMA_CH17_INT,   0},
++		{4, "MCTRL1",   IFX_DMA_RX_CH,  1,  DMA_CH18_INT,   1},
++		{4, "MCTRL1",   IFX_DMA_TX_CH,  1,  DMA_CH19_INT,   1},
++		};
++		memcpy(dma_device_name, device_name, sizeof(device_name));
++		memcpy(ifx_default_dma_map, dma_map, sizeof(dma_map));
++	} else if (of_machine_is_compatible("lantiq,vr9")) {
++		const char device_name[7][DMA_DEV_NAME_LEN] = {
++			{"PPE"}, {"DEU"}, {"SPI"}, {"SDIO" },
++			{"MCTRL"}, {"USIF"}, {"HSNAND"}
++		};
++		const struct ifx_dma_chan_map dma_map[28] = {
++		/* portnum, device name, channel direction, class value,
++		 * IRQ number, relative channel number */
++		{0, "PPE",      IFX_DMA_RX_CH,  0,  DMA_CH0_INT,    0},
++		{0, "PPE",      IFX_DMA_TX_CH,  0,  DMA_CH1_INT,    0},
++		{0, "PPE",      IFX_DMA_RX_CH,  1,  DMA_CH2_INT,    1},
++		{0, "PPE",      IFX_DMA_TX_CH,  1,  DMA_CH3_INT,    1},
++		{0, "PPE",      IFX_DMA_RX_CH,  2,  DMA_CH4_INT,    2},
++		{0, "PPE",      IFX_DMA_TX_CH,  2,  DMA_CH5_INT,    2},
++		{0, "PPE",      IFX_DMA_RX_CH,  3,  DMA_CH6_INT,    3},
++		{0, "PPE",      IFX_DMA_TX_CH,  3,  DMA_CH7_INT,    3},
++		{1, "DEU",      IFX_DMA_RX_CH,  0,  DMA_CH8_INT,    0},
++		{1, "DEU",      IFX_DMA_TX_CH,  0,  DMA_CH9_INT,    0},
++		{1, "DEU",      IFX_DMA_RX_CH,  1,  DMA_CH10_INT,   1},
++		{1, "DEU",      IFX_DMA_TX_CH,  1,  DMA_CH11_INT,   1},
++		{2, "SPI",      IFX_DMA_RX_CH,  0,  DMA_CH12_INT,   0},
++		{2, "SPI",      IFX_DMA_TX_CH,  0,  DMA_CH13_INT,   0},
++		{3, "SDIO",     IFX_DMA_RX_CH,  0,  DMA_CH14_INT,   0},
++		{3, "SDIO",     IFX_DMA_TX_CH,  0,  DMA_CH15_INT,   0},
++		{4, "MCTRL",    IFX_DMA_RX_CH,  0,  DMA_CH16_INT,   0},
++		{4, "MCTRL",    IFX_DMA_TX_CH,  0,  DMA_CH17_INT,   0},
++		{4, "MCTRL",    IFX_DMA_RX_CH,  1,  DMA_CH18_INT,   1},
++		{4, "MCTRL",    IFX_DMA_TX_CH,  1,  DMA_CH19_INT,   1},
++		{0, "PPE",      IFX_DMA_RX_CH,  4,  DMA_CH20_INT,   4},
++		{0, "PPE",      IFX_DMA_RX_CH,  5,  DMA_CH21_INT,   5},
++		{0, "PPE",      IFX_DMA_RX_CH,  6,  DMA_CH22_INT,   6},
++		{0, "PPE",      IFX_DMA_RX_CH,  7,  DMA_CH23_INT,   7},
++		{5, "USIF",     IFX_DMA_RX_CH,  0,  DMA_CH24_INT,   0},
++		{5, "USIF",     IFX_DMA_TX_CH,  0,  DMA_CH25_INT,   0},
++		{6, "HSNAND",   IFX_DMA_RX_CH,  0,  DMA_CH26_INT,   0},
++		{6, "HSNAND",   IFX_DMA_TX_CH,  0,  DMA_CH27_INT,   0},
++		};
++		memcpy(dma_device_name, device_name, sizeof(device_name));
++		memcpy(ifx_default_dma_map, dma_map, sizeof(dma_map));
++	} else if (of_machine_is_compatible("lantiq,ar10")
++		|| of_machine_is_compatible("lantiq,grx390")) {
++		const char device_name[5][DMA_DEV_NAME_LEN] = {
++			{"PPE"}, {"DEU"}, {"USIF"},
++			{"HSNAND"}, {"MCTRL"}
++		};
++		const struct ifx_dma_chan_map dma_map[24] = {
++		/* portnum, device name, channel direction, class value,
++		 * IRQ number, relative channel number */
++		{0, "PPE",      IFX_DMA_RX_CH,  0,  DMA_CH0_INT,    0},
++		{0, "PPE",      IFX_DMA_TX_CH,  0,  DMA_CH1_INT,    0},
++		{0, "PPE",      IFX_DMA_RX_CH,  1,  DMA_CH2_INT,    1},
++		{0, "PPE",      IFX_DMA_TX_CH,  1,  DMA_CH3_INT,    1},
++		{0, "PPE",      IFX_DMA_RX_CH,  2,  DMA_CH4_INT,    2},
++		{0, "PPE",      IFX_DMA_TX_CH,  2,  DMA_CH5_INT,    2},
++		{0, "PPE",      IFX_DMA_RX_CH,  3,  DMA_CH6_INT,    3},
++		{0, "PPE",      IFX_DMA_TX_CH,  3,  DMA_CH7_INT,    3},
++		{1, "DEU",      IFX_DMA_RX_CH,  0,  DMA_CH8_INT,    0},
++		{1, "DEU",      IFX_DMA_TX_CH,  0,  DMA_CH9_INT,    0},
++		{1, "DEU",      IFX_DMA_RX_CH,  1,  DMA_CH10_INT,   1},
++		{1, "DEU",      IFX_DMA_TX_CH,  1,  DMA_CH11_INT,   1},
++		{2, "USIF",     IFX_DMA_RX_CH,  0,  DMA_CH12_INT,   0},
++		{2, "USIF",     IFX_DMA_TX_CH,  0,  DMA_CH13_INT,   0},
++		{3, "HSNAND",   IFX_DMA_RX_CH,  0,  DMA_CH14_INT,   0},
++		{3, "HSNAND",   IFX_DMA_TX_CH,  0,  DMA_CH15_INT,   0},
++		{4, "MCTRL",    IFX_DMA_RX_CH,  0,  DMA_CH16_INT,   0},
++		{4, "MCTRL",    IFX_DMA_TX_CH,  0,  DMA_CH17_INT,   0},
++		{4, "MCTRL",    IFX_DMA_RX_CH,  1,  DMA_CH18_INT,   1},
++		{4, "MCTRL",    IFX_DMA_TX_CH,  1,  DMA_CH19_INT,   1},
++		{0, "PPE",      IFX_DMA_RX_CH,  4,  DMA_CH20_INT,   4},
++		{0, "PPE",      IFX_DMA_RX_CH,  5,  DMA_CH21_INT,   5},
++		{0, "PPE",      IFX_DMA_RX_CH,  6,  DMA_CH22_INT,   6},
++		{0, "PPE",      IFX_DMA_RX_CH,  7,  DMA_CH23_INT,   7},
++		};
++		memcpy(dma_device_name, device_name, sizeof(device_name));
++		memcpy(ifx_default_dma_map, dma_map, sizeof(dma_map));
++	} else if (of_machine_is_compatible("lantiq,svip")) {
++		const char device_name[9][DMA_DEV_NAME_LEN] = {
++			{"SW"}, {"DEU"}, {"SSC0"}, {"SSC1"},
++			{"MCTRL"}, {"PCM0"}, {"PCM1"}, {"PCM2"}, {"PCM3"}
++		};
++		const struct ifx_dma_chan_map dma_map[24] = {
++		/* portnum, device name, channel direction, class value,
++		 * IRQ number, relative channel number */
++		{0, "SW",       IFX_DMA_RX_CH,  0,  INT_NUM_IM4_IRL0 + 0,    0},
++		{0, "SW",       IFX_DMA_TX_CH,  0,  INT_NUM_IM4_IRL0 + 1,    0},
++		{0, "SW",       IFX_DMA_RX_CH,  1,  INT_NUM_IM4_IRL0 + 2,    1},
++		{0, "SW",       IFX_DMA_TX_CH,  1,  INT_NUM_IM4_IRL0 + 3,    1},
++		{0, "SW",       IFX_DMA_RX_CH,  2,  INT_NUM_IM4_IRL0 + 4,    2},
++		{0, "SW",       IFX_DMA_TX_CH,  2,  INT_NUM_IM4_IRL0 + 5,    2},
++		{0, "SW",       IFX_DMA_RX_CH,  3,  INT_NUM_IM4_IRL0 + 6,    3},
++		{0, "SW",       IFX_DMA_TX_CH,  3,  INT_NUM_IM4_IRL0 + 7,    3},
++		{1, "DEU",      IFX_DMA_RX_CH,  0,  INT_NUM_IM4_IRL0 + 8,    0},
++		{1, "DEU",      IFX_DMA_TX_CH,  0,  INT_NUM_IM4_IRL0 + 9,    0},
++		{2, "SSC0",     IFX_DMA_RX_CH,  1,  INT_NUM_IM4_IRL0 + 10,   0},
++		{2, "SSC0",     IFX_DMA_TX_CH,  1,  INT_NUM_IM4_IRL0 + 11,   0},
++		{3, "SSC1",     IFX_DMA_RX_CH,  0,  INT_NUM_IM4_IRL0 + 12,   0},
++		{3, "SSC1",     IFX_DMA_TX_CH,  0,  INT_NUM_IM4_IRL0 + 13,   0},
++		{4, "MCTRL",    IFX_DMA_RX_CH,  0,  INT_NUM_IM4_IRL0 + 14,   0},
++		{4, "MCTRL",    IFX_DMA_TX_CH,  0,  INT_NUM_IM4_IRL0 + 15,   0},
++		{5, "PCM0",     IFX_DMA_RX_CH,  0,  INT_NUM_IM4_IRL0 + 16,   0},
++		{5, "PCM0",     IFX_DMA_TX_CH,  0,  INT_NUM_IM4_IRL0 + 17,   0},
++		{6, "PCM1",     IFX_DMA_RX_CH,  1,  INT_NUM_IM4_IRL0 + 18,   0},
++		{6, "PCM1",     IFX_DMA_TX_CH,  1,  INT_NUM_IM4_IRL0 + 19,   0},
++		{7, "PCM2",     IFX_DMA_RX_CH,  4,  INT_NUM_IM4_IRL0 + 20,   0},
++		{7, "PCM2",     IFX_DMA_RX_CH,  5,  INT_NUM_IM4_IRL0 + 21,   0},
++		{8, "PCM3",     IFX_DMA_RX_CH,  6,  INT_NUM_IM4_IRL0 + 22,   0},
++		{8, "PCM3",     IFX_DMA_RX_CH,  7,  INT_NUM_IM4_IRL0 + 23,   0},
++		};
++		memcpy(dma_device_name, device_name, sizeof(device_name));
++		memcpy(ifx_default_dma_map, dma_map, sizeof(dma_map));
++	} else
++		pr_debug("%s: SoC not found\n", __func__);
++}
++
++static int ltq_dma_init(struct platform_device *pdev)
++{
++	int i;
++	struct clk *clk;
++	const __be32 *desc_num, *hw_poll, *pkt_arb;
++	struct resource *res;
++	struct device_node *node = pdev->dev.of_node;
++	u32 id;
++
++	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
++	if (!res)
++		panic("Failed to get dma resource");
++
++	for_each_possible_cpu(i)
++		tasklet_init(&dma_tasklet[i],
++			do_dma_tasklet, (unsigned long) i);
++	/* remap dma register range */
++	ltq_dma_membase = devm_request_and_ioremap(&pdev->dev, res);
++	if (!ltq_dma_membase)
++		panic("Failed to remap dma resource");
++
++	desc_num = of_get_property(node, "lantiq,desc-num", NULL);
++	if (!desc_num)
++		panic("Failed to get maximum DMA descriptor number");
++	dma_desc_num = (*desc_num);
++	if (dma_desc_num > MAX_DMA_DESC_NUM)
++		dma_desc_num = MAX_DMA_DESC_NUM;
++
++	hw_poll = of_get_property(node, "lantiq,dma-hw-poll", NULL);
++	if (!hw_poll)
++		panic("Failed to DMA hardware poll setting");
++	dma_hw_poll = *hw_poll;
++
++	pkt_arb = of_get_property(node, "lantiq,dma-pkt-arb", NULL);
++	if (!pkt_arb)
++		panic("Failed to DMA Packet Arbitration setting");
++	dma_pkt_arb_enable = *pkt_arb;
++	/* power up and reset the dma engine */
++	clk = clk_get(&pdev->dev, NULL);
++	if (IS_ERR(clk))
++		panic("Failed to get dma clock");
++
++	clk_enable(clk);
++
++	id = ltq_dma_r32(DMA_ID);
++	dma_max_ports = (id >> 16) & 0xf;
++	dma_max_chans = id >> 20;
++	/** Reset the dma channels and disable channel interrupts */
++	dma_chip_init();
++
++	dma_build_device_chan_tbl();
++	/** Map the default values in dma device/channel structure */
++	map_dma_chan(ifx_default_dma_map);
++
++	g_desc_list = kmalloc(dma_desc_num * dma_max_chans * sizeof(u64),
++				GFP_DMA);
++	if (g_desc_list == NULL) {
++		dev_err(&pdev->dev,
++			"%s: No memory for DMA descriptors list\n", __func__);
++		return -ENOMEM;
++	}
++	/** Invalidate the DMA cache memory for all the descriptors*/
++	dma_cache_inv((unsigned long) g_desc_list,
++		dma_desc_num * dma_max_chans * sizeof(u64));
++	/** just backup when want to cleanup the module */
++	g_desc_list_backup = g_desc_list;
++	/** Set uncached memory for DMA descriptors */
++	g_desc_list = (u64 *) ((u32) g_desc_list | KSEG1);
++	/** Clear whole descriptor memory */
++	memset(g_desc_list, 0, dma_desc_num * dma_max_chans * sizeof(u64));
++	for (i = 0; i < dma_max_chans; i++) {
++		/** init spin lock */
++		spin_lock_init(&dma_chan[i].irq_lock);
++
++		/** set the Desc base address based per channel */
++		dma_chan[i].desc_base = (u32) g_desc_list +
++			i * dma_desc_num * sizeof(u64);
++		dma_chan[i].curr_desc = 0;
++		/** Number of descriptor per channel */
++		dma_chan[i].desc_len = dma_desc_num;
++		/** select the channel */
++		select_chan(i);
++		/** set the desc base address and number of
++		 * descriptord per channel */
++		ltq_dma_w32(((u32)CPHYSADDR(dma_chan[i].desc_base)), DMA_CDBA);
++		ltq_dma_w32(dma_chan[i].desc_len, DMA_CDLEN);
++	}
++
++	/** create proc for debug  info*/
++	dma_core_proc_init();
++
++	/* Print the driver version info */
++	dev_info(&pdev->dev,
++		"Init done - hw rev: %X, ports: %d, channels: %d\n",
++		id & 0x1f, dma_max_ports, dma_max_chans);
++	return 0;
++}
++
++
++static const struct of_device_id dma_match[] = {
++	{ .compatible = "lantiq,dma-xway" },
++	{},
++};
++MODULE_DEVICE_TABLE(of, dma_match);
++
++static struct platform_driver dma_driver = {
++	.probe = ltq_dma_init,
++	.driver = {
++		.name = "dma-xway",
++		.owner = THIS_MODULE,
++		.of_match_table = dma_match,
++	},
++};
++
++static int __init dma_init(void)
++{
++	return platform_driver_register(&dma_driver);
++}
++
++postcore_initcall(dma_init);
++
++MODULE_LICENSE("GPL");
++MODULE_AUTHOR("Reddy.Mallikarjun@infineon.com");
++MODULE_DESCRIPTION("LTQ CPE Central DMA device driver");
++MODULE_SUPPORTED_DEVICE("LTQ CPE Devices(ASE, Danube, AR9, VR9, AR10/11)");
+diff --git a/arch/mips/lantiq/svip/sysctrl.c b/arch/mips/lantiq/svip/sysctrl.c
+--- a/arch/mips/lantiq/svip/sysctrl.c
++++ b/arch/mips/lantiq/svip/sysctrl.c
+@@ -222,8 +222,9 @@ void __init ltq_soc_init(void)
+ 
+ 	clkdev_add_static(svip_cpu_hz(), svip_fpi_hz(),
+ 		svip_io_region_clock(), 0);
+-	clkdev_add_sys("14100100.serial", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
+-	clkdev_add_sys("14100200.serial", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
++	clkdev_add_sys("14100100.serial0", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
++	clkdev_add_sys("14100200.serial1", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
++	clkdev_add_sys("14104000.dma", SYSCTL_SYS1, SYS1_CLKENR_DMA);
+ }
+ 
+ /*
diff --git a/target/linux/lantiq/patches-3.10/2215-svip-add-pinctrl.patch b/target/linux/lantiq/patches-3.10/2215-svip-add-pinctrl.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2215-svip-add-pinctrl.patch
@@ -0,0 +1,1237 @@
+# HG changeset patch
+# Parent f81f72402a572c8556e239e52ad3a5f9c183a31d
+
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/pinctrl-svip.h b/arch/mips/include/asm/mach-lantiq/svip/pinctrl-svip.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/pinctrl-svip.h
+@@ -0,0 +1,14 @@
++/*
++ *  This program is free software; you can redistribute it and/or modify it
++ *  under the terms of the GNU General Public License version 2 as published
++ *  by the Free Software Foundation.
++ *
++ */
++
++#ifndef _LANTIQ_PINCTRL_SVIP_H__
++#define _LANTIQ_PINCTRL_SVIP_H__
++
++extern int svip_enable_external_int(u32 exint, u32 mode);
++extern int svip_disable_external_int(u32 exint);
++
++#endif /* _LANTIQ_PINCTRL_SVIP_H__ */
+diff --git a/drivers/pinctrl/Kconfig b/drivers/pinctrl/Kconfig
+--- a/drivers/pinctrl/Kconfig
++++ b/drivers/pinctrl/Kconfig
+@@ -169,6 +169,11 @@ config PINCTRL_SUNXI
+ 	select PINMUX
+ 	select GENERIC_PINCONF
+ 
++config PINCTRL_SVIP
++	bool
++	depends on SOC_SVIP
++	depends on PINCTRL_LANTIQ
++
+ config PINCTRL_TEGRA
+ 	bool
+ 	select PINMUX
+diff --git a/drivers/pinctrl/Makefile b/drivers/pinctrl/Makefile
+--- a/drivers/pinctrl/Makefile
++++ b/drivers/pinctrl/Makefile
+@@ -33,6 +33,7 @@ obj-$(CONFIG_PINCTRL_DB8540)	+= pinctrl-
+ obj-$(CONFIG_PINCTRL_SINGLE)	+= pinctrl-single.o
+ obj-$(CONFIG_PINCTRL_SIRF)	+= pinctrl-sirf.o
+ obj-$(CONFIG_PINCTRL_SUNXI)	+= pinctrl-sunxi.o
++obj-$(CONFIG_PINCTRL_SVIP)	+= pinctrl-svip.o
+ obj-$(CONFIG_PINCTRL_TEGRA)	+= pinctrl-tegra.o
+ obj-$(CONFIG_PINCTRL_TEGRA20)	+= pinctrl-tegra20.o
+ obj-$(CONFIG_PINCTRL_TEGRA30)	+= pinctrl-tegra30.o
+diff --git a/drivers/pinctrl/pinctrl-svip.c b/drivers/pinctrl/pinctrl-svip.c
+new file mode 100644
+--- /dev/null
++++ b/drivers/pinctrl/pinctrl-svip.c
+@@ -0,0 +1,1184 @@
++/*
++ *  linux/drivers/pinctrl/pinctrl-svip.c
++ *  based on linux/drivers/pinctrl/pinmux-pxa910.c
++ *
++ *  This program is free software; you can redistribute it and/or modify
++ *  it under the terms of the GNU General Public License version 2 as
++ *  publishhed by the Free Software Foundation.
++ *
++ *  Copyright (C) 2012 John Crispin <blogic@openwrt.org>
++ */
++#include <linux/err.h>
++#include <linux/slab.h>
++#include <linux/module.h>
++#include <linux/of_platform.h>
++#include <linux/of_address.h>
++#include <linux/of_gpio.h>
++#include <linux/ioport.h>
++#include <linux/io.h>
++#include <linux/device.h>
++#include <linux/platform_device.h>
++
++#include "pinctrl-lantiq.h"
++
++#include <lantiq_soc.h>
++
++#define PINS			32
++#define PORTS			5
++#define PORT(x)			(x / PINS)
++#define PORT_PIN(x)		(x % PINS)
++
++/* we have 2 mux bits that can be set for each pin */
++#define MUX_ALT0		0x1
++#define MUX_ALT1		0x2
++
++/* number of external interrupts */
++#define SVIP_EIU		17
++/* start number of external interrupts */
++#define SVIP_EIU_START		180
++
++/* these are the offsets to our registers */
++#define GPIO_OUT		0x00
++#define GPIO_IN			0x04
++#define GPIO_DIR		0x08
++#define GPIO_ALT0		0x0C
++#define GPIO_ALT1		0x10
++#define GPIO_PUEN		0x14
++#define GPIO_EXINTCR0		0x18
++#define GPIO_EXINTCR1		0x1C
++#define GPIO_IRNCR		0x20
++#define GPIO_IRNICR		0x24
++#define GPIO_IRNEN		0x28
++#define GPIO_IRNCFG		0x2C
++#define GPIO_IRNENSET		0x30
++#define GPIO_IRNENCLR		0x34
++
++/* macros to help us access the registers */
++#define gpio_getbit(m, r, p)	(!!(ltq_r32(m + r) & BIT(p)))
++#define gpio_setbit(m, r, p)	ltq_w32_mask(0, BIT(p), m + r)
++#define gpio_clearbit(m, r, p)	ltq_w32_mask(BIT(p), 0, m + r)
++
++#define MFP_SVIP(a, f0, f1, f2, f3)	\
++	{				\
++		.name = #a,		\
++		.pin = a,		\
++		.func = {		\
++			SVIP_MUX_##f0,	\
++			SVIP_MUX_##f1,	\
++			SVIP_MUX_##f2,	\
++			SVIP_MUX_##f3,	\
++		},			\
++	}
++
++#define GRP_MUX(a, m, p)		\
++	{ .name = a, .mux = SVIP_MUX_##m, .pins = p, .npins = ARRAY_SIZE(p), }
++
++#define GPIO_CHIP_SVIP(a)				\
++	{						\
++		.label = a,				\
++		.direction_input = svip_gpio_dir_in,	\
++		.direction_output = svip_gpio_dir_out,	\
++		.get = svip_gpio_get,			\
++		.set = svip_gpio_set,			\
++		.request = svip_gpio_req,		\
++		.free = svip_gpio_free,			\
++		.to_irq = svip_gpio_to_irq,		\
++		.base = -1				\
++	}
++
++#define SVIP_MAX_PIN_PORT0		20
++#define SVIP_MAX_PIN_PORT1		20
++#define SVIP_MAX_PIN_PORT2		19
++#define SVIP_MAX_PIN_PORT3		20
++#define SVIP_MAX_PIN_PORT4		24
++
++enum svip_mux {
++	SVIP_MUX_GPIO = 0,
++	SVIP_MUX_SPI,
++	SVIP_MUX_ASC,
++	SVIP_MUX_PCM,
++	SVIP_MUX_SLIC,
++	SVIP_MUX_EBU,
++	SVIP_MUX_JTAG,
++	SVIP_MUX_EXIN,
++	SVIP_MUX_TDM,
++	SVIP_MUX_STP,
++	SVIP_MUX_SIN,
++	SVIP_MUX_GPT,
++	SVIP_MUX_NMI,
++	SVIP_MUX_MDIO,
++	SVIP_MUX_MII,
++	SVIP_MUX_EPHY,
++	SVIP_MUX_DFE,
++	SVIP_MUX_SDIO,
++	SVIP_MUX_GPHY,
++	SVIP_MUX_NONE = 0xffff,
++};
++
++static const struct ltq_mfp_pin svip_mfp_port0[SVIP_MAX_PIN_PORT0] = {
++/*       pin    f0	f1	f2	f3   */
++MFP_SVIP(GPIO0, SPI, GPIO, NONE, NONE),
++MFP_SVIP(GPIO1, SPI, GPIO, NONE, NONE),
++MFP_SVIP(GPIO2, SPI, GPIO, NONE, NONE),
++MFP_SVIP(GPIO3, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO4, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO5, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO6, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO7, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO8, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO9, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO10, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO11, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO12, SPI, GPIO, SPI, NONE),
++MFP_SVIP(GPIO13, SPI, GPIO, SPI, SPI),
++MFP_SVIP(GPIO14, GPIO, ASC, NONE, NONE),
++MFP_SVIP(GPIO15, GPIO, ASC, NONE, NONE),
++MFP_SVIP(GPIO16, GPIO, ASC, NONE, NONE),
++MFP_SVIP(GPIO17, GPIO, ASC, NONE, NONE),
++MFP_SVIP(GPIO18, JTAG, GPIO, NONE, NONE),
++MFP_SVIP(GPIO19, NONE, GPIO, NONE, NONE) };
++static const struct ltq_mfp_pin svip_mfp_port1[SVIP_MAX_PIN_PORT1] = {
++/*       pin    f0	f1	f2	f3   */
++MFP_SVIP(GPIO32, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO33, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO34, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO35, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO36, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO37, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO38, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO39, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO40, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO41, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO42, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO43, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO44, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO45, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO46, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO47, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO48, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO49, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO50, PCM, GPIO, NONE, NONE),
++MFP_SVIP(GPIO51, PCM, GPIO, NONE, NONE) };
++static const struct ltq_mfp_pin svip_mfp_port2[SVIP_MAX_PIN_PORT2] = {
++/*       pin    f0	f1	f2	f3   */
++MFP_SVIP(GPIO64, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO65, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO66, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO67, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO68, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO69, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO70, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO71, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO72, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO73, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO74, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO75, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO76, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO77, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO78, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO79, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO80, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO81, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO82, EBU, GPIO, NONE, NONE) };
++static const struct ltq_mfp_pin svip_mfp_port3[SVIP_MAX_PIN_PORT3] = {
++/*       pin    f0	f1	f2	f3   */
++MFP_SVIP(GPIO96, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO97, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO98, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO99, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO100, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO101, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO102, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO103, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO104, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO105, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO106, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO107, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO108, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO109, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO110, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO111, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO112, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO113, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO114, EBU, GPIO, NONE, NONE),
++MFP_SVIP(GPIO115, EBU, GPIO, NONE, NONE) };
++static const struct ltq_mfp_pin svip_mfp_port4[SVIP_MAX_PIN_PORT4] = {
++/*       pin    f0	f1	f2	f3   */
++MFP_SVIP(GPIO128, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO129, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO130, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO131, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO132, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO133, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO134, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO135, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO136, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO137, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO138, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO139, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO140, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO141, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO142, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO143, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO144, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO145, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO146, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO147, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO148, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO149, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO150, SLIC, GPIO, NONE, NONE),
++MFP_SVIP(GPIO151, SLIC, GPIO, NONE, NONE) };
++
++/* Port 0 */
++static const unsigned pins_spi0[] = { GPIO0, GPIO1, GPIO2 };
++static const unsigned pins_spi1[] = { GPIO3, GPIO4, GPIO5 };
++static const unsigned pins_spi2[] = { GPIO4, GPIO5 };
++
++/* For both master and slave modes */
++static const unsigned pins_spi0_cs0[] = { GPIO6 };
++/* Next 7 used only in master mode */
++static const unsigned pins_spi0_cs1[] = { GPIO7 };
++static const unsigned pins_spi0_cs2[] = { GPIO8 };
++static const unsigned pins_spi0_cs3[] = { GPIO9 };
++static const unsigned pins_spi0_cs4[] = { GPIO10 };
++static const unsigned pins_spi0_cs5[] = { GPIO11 };
++static const unsigned pins_spi0_cs6[] = { GPIO12 };
++static const unsigned pins_spi0_cs7[] = { GPIO13 };
++
++/* For master mode only */
++static const unsigned pins_spi1_m_cs0[] = { GPIO6 };
++/* For slave mode only */
++static const unsigned pins_spi1_s_cs0[] = { GPIO7 };
++/* Next 7 used only in master mode */
++static const unsigned pins_spi1_cs1[] = { GPIO7 };
++static const unsigned pins_spi1_cs2[] = { GPIO8 };
++static const unsigned pins_spi1_cs3[] = { GPIO9 };
++static const unsigned pins_spi1_cs4[] = { GPIO10 };
++static const unsigned pins_spi1_cs5[] = { GPIO11 };
++static const unsigned pins_spi1_cs6[] = { GPIO12 };
++static const unsigned pins_spi1_cs7[] = { GPIO13 };
++
++/* SPI2 works only in slave mode */
++static const unsigned pins_spi2_cs0[] = { GPIO7 };
++static const unsigned pins_spi2_int[] = { GPIO13 };
++
++static const unsigned pins_asc0[] = { GPIO14, GPIO15 };
++static const unsigned pins_asc1[] = { GPIO16, GPIO17 };
++static const unsigned pins_jtag[] = { GPIO18 };
++
++static const unsigned pins_exin0[] = { GPIO7 };
++static const unsigned pins_exin1[] = { GPIO8 };
++static const unsigned pins_exin2[] = { GPIO9 };
++static const unsigned pins_exin3[] = { GPIO10 };
++static const unsigned pins_exin4[] = { GPIO11 };
++static const unsigned pins_exin5[] = { GPIO12 };
++static const unsigned pins_exin6[] = { GPIO13 };
++static const unsigned pins_exin7[] = { GPIO14 };
++static const unsigned pins_exin8[] = { GPIO15 };
++static const unsigned pins_exin9[] = { GPIO16 };
++static const unsigned pins_exin10[] = { GPIO17 };
++static const unsigned pins_exin16[] = { GPIO19 };
++
++/* Port 1 */
++static const unsigned pins_pcm0[] = { GPIO32, GPIO33, GPIO34, GPIO35, GPIO36 };
++static const unsigned pins_pcm1[] = { GPIO37, GPIO38, GPIO39, GPIO40, GPIO41 };
++static const unsigned pins_pcm2[] = { GPIO42, GPIO43, GPIO44, GPIO45, GPIO46 };
++static const unsigned pins_pcm3[] = { GPIO47, GPIO48, GPIO49, GPIO50, GPIO51 };
++static const unsigned pins_exin11[] = { GPIO50 };
++static const unsigned pins_exin12[] = { GPIO49 };
++static const unsigned pins_exin13[] = { GPIO48 };
++static const unsigned pins_exin14[] = { GPIO47 };
++static const unsigned pins_exin15[] = { GPIO51 };
++
++/* Port 2 */
++static const unsigned pins_ebu_a0[] = { GPIO64 };
++static const unsigned pins_ebu_a1[] = { GPIO65 };
++static const unsigned pins_ebu_a2[] = { GPIO66 };
++static const unsigned pins_ebu_a3[] = { GPIO67 };
++static const unsigned pins_ebu_a4[] = { GPIO68 };
++static const unsigned pins_ebu_a5[] = { GPIO69 };
++static const unsigned pins_ebu_a6[] = { GPIO70 };
++static const unsigned pins_ebu_a7[] = { GPIO71 };
++static const unsigned pins_ebu_a8[] = { GPIO72 };
++static const unsigned pins_ebu_a9[] = { GPIO73 };
++static const unsigned pins_ebu_a10[] = { GPIO74 };
++static const unsigned pins_ebu_a11[] = { GPIO75 };
++static const unsigned pins_ebu_rd[] = { GPIO76 };
++static const unsigned pins_ebu_wr[] = { GPIO77 };
++static const unsigned pins_ebu_ale[] = { GPIO78 };
++static const unsigned pins_ebu_wait[] = { GPIO79 };
++static const unsigned pins_ebu_rdy[] = { GPIO80 };
++static const unsigned pins_ebu_bc0[] = { GPIO81 };
++static const unsigned pins_ebu_bc1[] = { GPIO82 };
++
++/* Port 3 */
++static const unsigned pins_ebu_ad0[] = { GPIO96 };
++static const unsigned pins_ebu_ad1[] = { GPIO97 };
++static const unsigned pins_ebu_ad2[] = { GPIO98 };
++static const unsigned pins_ebu_ad3[] = { GPIO99 };
++static const unsigned pins_ebu_ad4[] = { GPIO100 };
++static const unsigned pins_ebu_ad5[] = { GPIO101 };
++static const unsigned pins_ebu_ad6[] = { GPIO102 };
++static const unsigned pins_ebu_ad7[] = { GPIO103 };
++static const unsigned pins_ebu_ad8[] = { GPIO104 };
++static const unsigned pins_ebu_ad9[] = { GPIO105 };
++static const unsigned pins_ebu_ad10[] = { GPIO106 };
++static const unsigned pins_ebu_ad11[] = { GPIO107 };
++static const unsigned pins_ebu_ad12[] = { GPIO108 };
++static const unsigned pins_ebu_ad13[] = { GPIO109 };
++static const unsigned pins_ebu_ad14[] = { GPIO110 };
++static const unsigned pins_ebu_ad15[] = { GPIO111 };
++static const unsigned pins_ebu_cs0[] = { GPIO112 };
++static const unsigned pins_ebu_cs1[] = { GPIO113 };
++static const unsigned pins_ebu_cs2[] = { GPIO114 };
++static const unsigned pins_ebu_cs3[] = { GPIO115 };
++
++/* Port 4 */
++static const unsigned pins_slic0[] = { GPIO128, GPIO129, GPIO130 };
++static const unsigned pins_slic1[] = { GPIO131, GPIO132, GPIO133 };
++static const unsigned pins_slic2[] = { GPIO134, GPIO135, GPIO136 };
++static const unsigned pins_slic3[] = { GPIO137, GPIO138, GPIO139 };
++static const unsigned pins_slic4[] = { GPIO140, GPIO141, GPIO142 };
++static const unsigned pins_slic5[] = { GPIO143, GPIO144, GPIO145 };
++static const unsigned pins_slic6[] = { GPIO146, GPIO147, GPIO148 };
++static const unsigned pins_slic7[] = { GPIO149, GPIO150, GPIO151 };
++
++static const struct ltq_pin_group svip_grps_port0[] = {
++GRP_MUX("spi0", SPI, pins_spi0),
++GRP_MUX("spi1", SPI, pins_spi1),
++GRP_MUX("spi2", SPI, pins_spi2),
++GRP_MUX("spi0 cs0", SPI, pins_spi0_cs0),
++GRP_MUX("spi0 cs1", SPI, pins_spi0_cs1),
++GRP_MUX("spi0 cs2", SPI, pins_spi0_cs2),
++GRP_MUX("spi0 cs3", SPI, pins_spi0_cs3),
++GRP_MUX("spi0 cs4", SPI, pins_spi0_cs4),
++GRP_MUX("spi0 cs5", SPI, pins_spi0_cs5),
++GRP_MUX("spi0 cs6", SPI, pins_spi0_cs6),
++GRP_MUX("spi0 cs7", SPI, pins_spi0_cs7),
++GRP_MUX("spi1 m cs0", SPI, pins_spi1_m_cs0),
++GRP_MUX("spi1 s cs0", SPI, pins_spi1_s_cs0),
++GRP_MUX("spi1 cs1", SPI, pins_spi1_cs1),
++GRP_MUX("spi1 cs2", SPI, pins_spi1_cs2),
++GRP_MUX("spi1 cs3", SPI, pins_spi1_cs3),
++GRP_MUX("spi1 cs4", SPI, pins_spi1_cs4),
++GRP_MUX("spi1 cs5", SPI, pins_spi1_cs5),
++GRP_MUX("spi1 cs6", SPI, pins_spi1_cs6),
++GRP_MUX("spi1 cs7", SPI, pins_spi1_cs7),
++GRP_MUX("spi2 cs0", SPI, pins_spi2_cs0),
++GRP_MUX("spi2 int", SPI, pins_spi2_int),
++GRP_MUX("asc0", ASC, pins_asc0),
++GRP_MUX("asc1", ASC, pins_asc1),
++GRP_MUX("jtag", JTAG, pins_jtag),
++GRP_MUX("exin0", EXIN, pins_exin0),
++GRP_MUX("exin1", EXIN, pins_exin1),
++GRP_MUX("exin2", EXIN, pins_exin2),
++GRP_MUX("exin3", EXIN, pins_exin3),
++GRP_MUX("exin4", EXIN, pins_exin4),
++GRP_MUX("exin5", EXIN, pins_exin5),
++GRP_MUX("exin6", EXIN, pins_exin6),
++GRP_MUX("exin7", EXIN, pins_exin7),
++GRP_MUX("exin8", EXIN, pins_exin8),
++GRP_MUX("exin9", EXIN, pins_exin9),
++GRP_MUX("exin10", EXIN, pins_exin10),
++GRP_MUX("exin16", EXIN, pins_exin16) };
++
++static const struct ltq_pin_group svip_grps_port1[] = {
++GRP_MUX("pcm0", PCM, pins_pcm0),
++GRP_MUX("pcm1", PCM, pins_pcm1),
++GRP_MUX("pcm2", PCM, pins_pcm2),
++GRP_MUX("pcm3", PCM, pins_pcm3),
++GRP_MUX("exin11", EXIN, pins_exin11),
++GRP_MUX("exin12", EXIN, pins_exin12),
++GRP_MUX("exin13", EXIN, pins_exin13),
++GRP_MUX("exin14", EXIN, pins_exin14),
++GRP_MUX("exin15", EXIN, pins_exin15) };
++
++static const struct ltq_pin_group svip_grps_port2[] = {
++GRP_MUX("ebu a0", EBU, pins_ebu_a0),
++GRP_MUX("ebu a1", EBU, pins_ebu_a1),
++GRP_MUX("ebu a2", EBU, pins_ebu_a2),
++GRP_MUX("ebu a3", EBU, pins_ebu_a3),
++GRP_MUX("ebu a4", EBU, pins_ebu_a4),
++GRP_MUX("ebu a5", EBU, pins_ebu_a5),
++GRP_MUX("ebu a6", EBU, pins_ebu_a6),
++GRP_MUX("ebu a7", EBU, pins_ebu_a7),
++GRP_MUX("ebu a8", EBU, pins_ebu_a8),
++GRP_MUX("ebu a9", EBU, pins_ebu_a9),
++GRP_MUX("ebu a10", EBU, pins_ebu_a10),
++GRP_MUX("ebu a11", EBU, pins_ebu_a11),
++GRP_MUX("ebu rd", EBU, pins_ebu_rd),
++GRP_MUX("ebu wr", EBU, pins_ebu_wr),
++GRP_MUX("ebu ale", EBU, pins_ebu_ale),
++GRP_MUX("ebu wait", EBU, pins_ebu_wait),
++GRP_MUX("ebu rdy", EBU, pins_ebu_rdy),
++GRP_MUX("ebu bc0", EBU, pins_ebu_bc0),
++GRP_MUX("ebu bc1", EBU, pins_ebu_bc1) };
++
++static const struct ltq_pin_group svip_grps_port3[] = {
++GRP_MUX("ebu ad0", EBU, pins_ebu_ad0),
++GRP_MUX("ebu ad1", EBU, pins_ebu_ad1),
++GRP_MUX("ebu ad2", EBU, pins_ebu_ad2),
++GRP_MUX("ebu ad3", EBU, pins_ebu_ad3),
++GRP_MUX("ebu ad4", EBU, pins_ebu_ad4),
++GRP_MUX("ebu ad5", EBU, pins_ebu_ad5),
++GRP_MUX("ebu ad6", EBU, pins_ebu_ad6),
++GRP_MUX("ebu ad7", EBU, pins_ebu_ad7),
++GRP_MUX("ebu ad8", EBU, pins_ebu_ad8),
++GRP_MUX("ebu ad9", EBU, pins_ebu_ad9),
++GRP_MUX("ebu ad10", EBU, pins_ebu_ad10),
++GRP_MUX("ebu ad11", EBU, pins_ebu_ad11),
++GRP_MUX("ebu ad12", EBU, pins_ebu_ad12),
++GRP_MUX("ebu ad13", EBU, pins_ebu_ad13),
++GRP_MUX("ebu ad14", EBU, pins_ebu_ad14),
++GRP_MUX("ebu ad15", EBU, pins_ebu_ad15),
++GRP_MUX("ebu cs0", EBU, pins_ebu_cs0),
++GRP_MUX("ebu cs1", EBU, pins_ebu_cs1),
++GRP_MUX("ebu cs2", EBU, pins_ebu_cs2),
++GRP_MUX("ebu cs3", EBU, pins_ebu_cs3) };
++
++static const struct ltq_pin_group svip_grps_port4[] = {
++GRP_MUX("slic0", SLIC, pins_slic0),
++GRP_MUX("slic1", SLIC, pins_slic1),
++GRP_MUX("slic2", SLIC, pins_slic2),
++GRP_MUX("slic3", SLIC, pins_slic3),
++GRP_MUX("slic4", SLIC, pins_slic4),
++GRP_MUX("slic5", SLIC, pins_slic5),
++GRP_MUX("slic6", SLIC, pins_slic6),
++GRP_MUX("slic7", SLIC, pins_slic7) };
++
++/* Port 0 groups */
++static const char * const svip_spi_grps[] = {
++	"spi0", "spi1", "spi2", "spi0 cs0", "spi0 cs1", "spi0 cs2", "spi0 cs3",
++	"spi0 cs4", "spi0 cs5", "spi0 cs6", "spi0 cs7",
++	"spi1 m cs0", "spi1 s cs0", "spi1 cs1", "spi1 cs2",
++	"spi1 cs3", "spi1 cs4", "spi1 cs5", "spi1 cs6",
++	"spi1 cs7", "spi2 cs0", "spi2 int" };
++static const char * const svip_asc_grps[] = { "asc0", "asc1" };
++static const char * const svip_jtag_grps[] = { "jtag" };
++static const char * const svip_exin_grps0[] = {
++	"exin0", "exin1", "exin2", "exin3", "exin4", "exin5", "exin6", "exin7",
++	"exin8", "exin9", "exin10", "exin16" };
++
++/* Port 1 groups */
++static const char * const svip_pcm_grps[] = { "pcm0", "pcm1", "pcm2", "pcm3" };
++static const char * const svip_exin_grps1[] = {
++	"exin11", "exin12", "exin13", "exin14", "exin15" };
++
++/* Port 2 groups */
++static const char * const svip_ebu_grps2[] = {
++	"ebu a0", "ebu a1", "ebu a2", "ebu a3", "ebu a4", "ebu a5", "ebu a6",
++	"ebu a7", "ebu a8", "ebu a9", "ebu a10", "ebu rd",
++	"ebu wr", "ebu ale", "ebu wait", "ebu rdy", "ebu bc0",
++	"ebu bc1" };
++
++/* Port 3 groups */
++static const char * const svip_ebu_grps3[] = {
++	"ebu ad0", "ebu ad1", "ebu ad2", "ebu ad3", "ebu ad4", "ebu ad5",
++	"ebu ad6", "ebu ad7", "ebu ad8", "ebu ad9", "ebu ad10", "ebu ad11",
++	"ebu ad12", "ebu ad13", "ebu ad14", "ebu ad15",
++	"ebu cs0", "ebu cs1", "ebu cs2", "ebu cs3" };
++
++/* Port 4 groups */
++static const char * const svip_slic_grps[] = {
++	"slic0", "slic1", "slic2", "slic3", "slic4", "slic5", "slic6",
++	"slic7" };
++
++static const struct ltq_pmx_func svip_funcs_port0[] = {
++{ "spi", ARRAY_AND_SIZE(svip_spi_grps) },
++{ "asc", ARRAY_AND_SIZE(svip_asc_grps) },
++{ "jtag", ARRAY_AND_SIZE(svip_jtag_grps) },
++{ "exin0", ARRAY_AND_SIZE(svip_exin_grps0) } };
++
++static const struct ltq_pmx_func svip_funcs_port1[] = {
++{ "pcm", ARRAY_AND_SIZE(svip_pcm_grps) },
++{ "exin1", ARRAY_AND_SIZE(svip_exin_grps1) } };
++static const struct ltq_pmx_func svip_funcs_port2[] = {
++{ "ebu2", ARRAY_AND_SIZE(svip_ebu_grps2) } };
++static const struct ltq_pmx_func svip_funcs_port3[] = {
++{ "ebu3", ARRAY_AND_SIZE(svip_ebu_grps3) } };
++static const struct ltq_pmx_func svip_funcs_port4[] = {
++{ "slic", ARRAY_AND_SIZE(svip_slic_grps) } };
++
++/* ---------  pinconf related code --------- */
++static int svip_pinconf_get(struct pinctrl_dev *pctldev, unsigned pin,
++	unsigned long *config)
++{
++	struct ltq_pinmux_info *info = pinctrl_dev_get_drvdata(pctldev);
++	enum ltq_pinconf_param param = LTQ_PINCONF_UNPACK_PARAM(*config);
++
++	switch (param) {
++	case LTQ_PINCONF_PARAM_PULL:
++		if (gpio_getbit(info->membase[0], GPIO_PUEN, PORT_PIN(pin)))
++			*config = LTQ_PINCONF_PACK(param, 2);
++		else
++			*config = LTQ_PINCONF_PACK(param, 0);
++		break;
++
++	case LTQ_PINCONF_PARAM_OUTPUT:
++		*config = LTQ_PINCONF_PACK(param,
++			gpio_getbit(info->membase[0], GPIO_DIR, PORT_PIN(pin)));
++		break;
++	default:
++		dev_err(pctldev->dev, "Invalid config param %04x\n", param);
++		return -ENOTSUPP;
++	}
++	return 0;
++}
++
++static int svip_pinconf_set(struct pinctrl_dev *pctldev, unsigned pin,
++	unsigned long config)
++{
++	struct ltq_pinmux_info *info = pinctrl_dev_get_drvdata(pctldev);
++	enum ltq_pinconf_param param = LTQ_PINCONF_UNPACK_PARAM(config);
++	int arg = LTQ_PINCONF_UNPACK_ARG(config);
++
++	switch (param) {
++	case LTQ_PINCONF_PARAM_PULL:
++		if (arg == 0) {
++			gpio_clearbit(info->membase[0], GPIO_PUEN,
++				PORT_PIN(pin));
++			break;
++		} else if (arg == 2) {
++			gpio_setbit(info->membase[0], GPIO_PUEN, PORT_PIN(pin));
++		} else
++			dev_err(pctldev->dev, "Invalid pull value %d\n", arg);
++		break;
++
++	case LTQ_PINCONF_PARAM_OUTPUT:
++		if (arg == 0)
++			gpio_clearbit(info->membase[0], GPIO_DIR,
++				PORT_PIN(pin));
++		else
++			gpio_setbit(info->membase[0], GPIO_DIR, PORT_PIN(pin));
++		break;
++
++	default:
++		dev_err(pctldev->dev, "Invalid config param %04x\n", param);
++		return -ENOTSUPP;
++	}
++	return 0;
++}
++
++static int svip_pinconf_group_get(struct pinctrl_dev *pctldev,
++	unsigned selector, unsigned long *config)
++{
++	struct ltq_pinmux_info *info = pinctrl_dev_get_drvdata(pctldev);
++	int i, ret = 0;
++
++	for (i = 0; i < info->grps[selector].npins && !ret; i++)
++		ret = svip_pinconf_get(pctldev, info->grps[selector].pins[i],
++			config);
++
++	return ret;
++}
++
++int svip_pinconf_group_set(struct pinctrl_dev *pctldev, unsigned selector,
++	unsigned long config)
++{
++	struct ltq_pinmux_info *info = pinctrl_dev_get_drvdata(pctldev);
++	int i, ret = 0;
++
++	for (i = 0; i < info->grps[selector].npins && !ret; i++)
++		ret = svip_pinconf_set(pctldev, info->grps[selector].pins[i],
++			config);
++
++	return ret;
++}
++
++static const struct pinconf_ops svip_pinconf_ops = {
++	.pin_config_get = svip_pinconf_get, .pin_config_set = svip_pinconf_set,
++	.pin_config_group_get = svip_pinconf_group_get,
++	.pin_config_group_set = svip_pinconf_group_set, };
++
++static struct pinctrl_desc svip_pctrl_desc[PORTS] = {
++{ .owner = THIS_MODULE, .confops = &svip_pinconf_ops },
++{ .owner = THIS_MODULE, .confops = &svip_pinconf_ops },
++{ .owner = THIS_MODULE, .confops = &svip_pinconf_ops },
++{ .owner = THIS_MODULE, .confops = &svip_pinconf_ops },
++{ .owner = THIS_MODULE, .confops = &svip_pinconf_ops } };
++
++static int mux_apply(struct ltq_pinmux_info *info, int pin, int mux)
++{
++	if (mux & MUX_ALT0)
++		gpio_setbit(info->membase[0], GPIO_ALT0, PORT_PIN(pin));
++	else
++		gpio_clearbit(info->membase[0], GPIO_ALT0, PORT_PIN(pin));
++
++	if (mux & MUX_ALT1)
++		gpio_setbit(info->membase[0], GPIO_ALT1, PORT_PIN(pin));
++	else
++		gpio_clearbit(info->membase[0], GPIO_ALT1, PORT_PIN(pin));
++
++	return 0;
++}
++
++static inline int svip_mux_apply(struct pinctrl_dev *pctrldev, int pin, int mux)
++{
++	struct ltq_pinmux_info *info = pinctrl_dev_get_drvdata(pctrldev);
++
++	return mux_apply(info, pin, mux);
++}
++
++static const struct ltq_cfg_param svip_cfg_params[] = {
++{ "lantiq,pull", LTQ_PINCONF_PARAM_PULL },
++{ "lantiq,open-drain", LTQ_PINCONF_PARAM_OPEN_DRAIN },
++{ "lantiq,output", LTQ_PINCONF_PARAM_OUTPUT }, };
++
++static struct ltq_pinmux_info svip_info[PORTS] = {
++{ .desc = &svip_pctrl_desc[0], .apply_mux = svip_mux_apply, .params =
++	svip_cfg_params, .num_params = ARRAY_SIZE(svip_cfg_params) },
++{ .desc = &svip_pctrl_desc[1], .apply_mux = svip_mux_apply, .params =
++	svip_cfg_params, .num_params = ARRAY_SIZE(svip_cfg_params) },
++{ .desc = &svip_pctrl_desc[2], .apply_mux = svip_mux_apply, .params =
++	svip_cfg_params, .num_params = ARRAY_SIZE(svip_cfg_params) },
++{ .desc = &svip_pctrl_desc[3], .apply_mux = svip_mux_apply, .params =
++	svip_cfg_params, .num_params = ARRAY_SIZE(svip_cfg_params) },
++{ .desc = &svip_pctrl_desc[4], .apply_mux = svip_mux_apply, .params =
++	svip_cfg_params, .num_params = ARRAY_SIZE(svip_cfg_params) } };
++
++/* ---------  gpio_chip related code --------- */
++
++static struct irq_chip svip_gpio_irq_chip;
++
++static void svip_gpio_set(struct gpio_chip *chip, unsigned int pin, int val)
++{
++	struct ltq_pinmux_info *info = dev_get_drvdata(chip->dev);
++
++	if (val)
++		gpio_setbit(info->membase[0], GPIO_OUT, PORT_PIN(pin));
++	else
++		gpio_clearbit(info->membase[0], GPIO_OUT, PORT_PIN(pin));
++}
++
++static int svip_gpio_get(struct gpio_chip *chip, unsigned int pin)
++{
++	struct ltq_pinmux_info *info = dev_get_drvdata(chip->dev);
++
++	return gpio_getbit(info->membase[0], GPIO_IN, PORT_PIN(pin));
++}
++
++static int svip_gpio_dir_in(struct gpio_chip *chip, unsigned int pin)
++{
++	struct ltq_pinmux_info *info = dev_get_drvdata(chip->dev);
++
++	gpio_clearbit(info->membase[0], GPIO_DIR, PORT_PIN(pin));
++
++	return 0;
++}
++
++static int svip_gpio_dir_out(struct gpio_chip *chip, unsigned int pin, int val)
++{
++	struct ltq_pinmux_info *info = dev_get_drvdata(chip->dev);
++
++	gpio_setbit(info->membase[0], GPIO_DIR, PORT_PIN(pin));
++	svip_gpio_set(chip, pin, val);
++
++	return 0;
++}
++
++static int svip_gpio_req(struct gpio_chip *chip, unsigned offset)
++{
++	int gpio = chip->base + offset;
++
++	return pinctrl_request_gpio(gpio);
++}
++
++static void svip_gpio_free(struct gpio_chip *chip, unsigned offset)
++{
++	int gpio = chip->base + offset;
++
++	pinctrl_free_gpio(gpio);
++}
++
++static int svip_gpio_to_exin(unsigned offset)
++{
++	int svip_exin[SVIP_EIU] = {
++		GPIO7,
++		GPIO8,
++		GPIO9,
++		GPIO10,
++		GPIO11,
++		GPIO12,
++		GPIO13,
++		GPIO14,
++		GPIO15,
++		GPIO16,
++		GPIO17,
++		GPIO50,
++		GPIO49,
++		GPIO48,
++		GPIO47,
++		GPIO51,
++		GPIO19
++	};
++	int i;
++
++	for (i = 0; i < sizeof(svip_exin) / sizeof(svip_exin[0]); i++)
++		if (offset == svip_exin[i])
++			return i;
++	return -1;
++}
++
++
++static int svip_gpio_to_irq(struct gpio_chip *chip, unsigned offset)
++{
++	struct ltq_pinmux_info *info = dev_get_drvdata(chip->dev);
++	int i, exin;
++
++	for (i = 0; i < info->num_exin; i++)
++		if (info->exin[i] == offset) {
++			exin = svip_gpio_to_exin(offset);
++			if (exin < 0)
++				return -1;
++			return SVIP_EIU_START + exin;
++		}
++	return -1;
++}
++
++
++/**
++ * Convert interrupt number to corresponding port/pin pair
++ * Returns the port/pin pair serving the selected external interrupt;
++ * needed since mapping not linear.
++ *
++ * \param uirq       Interrupt number
++ * \param port      Pointer for resulting port
++ * \param pin       Pointer for resutling pin
++ * \return -EINVAL  Invalid exint
++ * \return 0        port/pin updated
++ * \ingroup API
++ */
++static int svip_irq_to_gpio(u32 uirq, int *port, int *pin)
++{
++	int irq = uirq - SVIP_EIU_START;
++
++	if ((irq >= 0) && (irq <= 10)) {
++		*port = 0;
++		*pin  = irq + 7;
++	} else if ((irq >= 11) && (irq <= 14)) {
++		*port = 1;
++		*pin  = 18 - (irq - 11);
++	} else if (irq == 15) {
++		*port = 1;
++		*pin  = 19;
++	} else if (irq == 16) {
++		*port = 0;
++		*pin  = 19;
++	} else {
++		return -EINVAL;
++	}
++	return 0;
++}
++
++
++/**
++ * Enable external interrupt.
++ * This function enables an external interrupt and sets the given mode.
++ * valid values for mode are:
++ *   - 0   = Interrupt on rising edge
++ *   - 1   = Interrupt on falling edge
++ *   - 2-3 = Interrupt on rising and falling edge
++ *   - 4   = Interrupt on high level detection
++ *   - 5   = Interrupt on low level detection
++  *
++ * \param   exint - Number of external interrupt (starts from SVIP_EIU_START)
++ * \param   mode  - Trigger mode
++ * \return  0 on success
++ * \ingroup API
++ */
++int svip_enable_external_int(u32 exint, u32 mode)
++{
++	int port;
++	int pin;
++
++	if ((mode < 0) || (mode > 5))
++		return -EINVAL;
++
++	if (svip_irq_to_gpio(exint, &port, &pin))
++		return -EINVAL;
++
++	gpio_clearbit(svip_info[port].membase[0], GPIO_EXINTCR0, pin);
++	gpio_clearbit(svip_info[port].membase[0], GPIO_EXINTCR1, pin);
++	gpio_clearbit(svip_info[port].membase[0], GPIO_IRNCFG, pin);
++
++	if (mode & 0x1)
++		gpio_setbit(svip_info[port].membase[0], GPIO_EXINTCR0, pin);
++	if (mode & 0x2)
++		gpio_setbit(svip_info[port].membase[0], GPIO_EXINTCR1, pin);
++	if (mode & 0x4)
++		gpio_setbit(svip_info[port].membase[0], GPIO_IRNCFG, pin);
++
++	gpio_setbit(svip_info[port].membase[0], GPIO_IRNENSET, pin);
++	return 0;
++}
++EXPORT_SYMBOL(svip_enable_external_int);
++
++
++/**
++ * Disable external interrupt.
++ *
++ * \param   exint - Number of external interrupt (starts from SVIP_EIU_START)
++ * \return  0 on success
++ * \ingroup API
++ */
++int svip_disable_external_int(u32 exint)
++{
++	int port;
++	int pin;
++
++	if (svip_irq_to_gpio(exint, &port, &pin))
++		return -EINVAL;
++
++	gpio_setbit(svip_info[port].membase[0], GPIO_IRNENCLR, pin);
++	return 0;
++}
++EXPORT_SYMBOL(svip_disable_external_int);
++
++
++static void svip_gpio_disable_irq(struct irq_data *d)
++{
++	struct ltq_pinmux_info *info = irq_get_chip_data(d->irq);
++	int port;
++	int pin;
++
++	if (svip_irq_to_gpio(d->irq, &port, &pin))
++		return;
++
++	gpio_setbit(info->membase[0], GPIO_IRNENCLR, pin);
++}
++
++static void svip_gpio_enable_irq(struct irq_data *d)
++{
++	struct ltq_pinmux_info *info = irq_get_chip_data(d->irq);
++	int port;
++	int pin;
++
++	if (svip_irq_to_gpio(d->irq, &port, &pin))
++		return;
++
++	gpio_setbit(info->membase[0], GPIO_IRNENSET, pin);
++}
++
++static void svip_gpio_ack_irq(struct irq_data *d)
++{
++	struct ltq_pinmux_info *info = irq_get_chip_data(d->irq);
++	int port;
++	int pin;
++
++	if (svip_irq_to_gpio(d->irq, &port, &pin))
++		return;
++
++	gpio_setbit(info->membase[0], GPIO_IRNCR, pin);
++}
++
++
++static void svip_gpio_mask_and_ack_irq(struct irq_data *d)
++{
++	struct ltq_pinmux_info *info = irq_get_chip_data(d->irq);
++	int port;
++	int pin;
++
++	if (svip_irq_to_gpio(d->irq, &port, &pin))
++		return;
++
++	gpio_setbit(info->membase[0], GPIO_IRNENCLR, pin);
++	gpio_setbit(info->membase[0], GPIO_IRNCR, pin);
++}
++
++
++static int svip_gpio_irq_type(struct irq_data *d, unsigned int type)
++{
++	struct ltq_pinmux_info *info = irq_get_chip_data(d->irq);
++	int port;
++	int pin;
++	int ret;
++
++	ret = svip_irq_to_gpio(d->irq, &port, &pin);
++	if (ret)
++		return ret;
++
++	if ((type & IRQ_TYPE_SENSE_MASK) == IRQ_TYPE_NONE)
++		return 0;
++
++	if ((type & (IRQ_TYPE_LEVEL_HIGH | IRQ_TYPE_LEVEL_LOW)) != 0) {
++		/* level triggered */
++		gpio_setbit(info->membase[0], GPIO_IRNCFG, pin);
++		irq_set_chip_and_handler_name(d->irq,
++			&svip_gpio_irq_chip, handle_level_irq, "mux");
++	} else {
++		/* edge triggered */
++		gpio_clearbit(info->membase[0], GPIO_IRNCFG, pin);
++		irq_set_chip_and_handler_name(d->irq,
++			&svip_gpio_irq_chip, handle_simple_irq, "mux");
++	}
++
++	if ((type & IRQ_TYPE_EDGE_BOTH) == IRQ_TYPE_EDGE_BOTH)
++		gpio_setbit(info->membase[0], GPIO_EXINTCR1, pin);
++	else {
++		if ((type & (IRQ_TYPE_EDGE_RISING | IRQ_TYPE_LEVEL_HIGH)) != 0)
++			/* positive logic: rising edge, high level */
++			gpio_clearbit(info->membase[0], GPIO_EXINTCR0, pin);
++		else
++			/* negative logic: falling edge, low level */
++			gpio_setbit(info->membase[0], GPIO_EXINTCR0, pin);
++		gpio_clearbit(info->membase[0], GPIO_EXINTCR1, pin);
++	}
++
++	gpio_clearbit(info->membase[0], GPIO_DIR, pin);
++	return 0;
++}
++
++
++#if 0
++static void svip_gpio_irq_handler(unsigned int irq, struct irq_desc *desc)
++{
++	struct ltq_pinmux_info *info = irq_desc_get_handler_data(desc);
++	unsigned long irncr;
++	int i;
++
++	kstat_incr_irqs_this_cpu(irq, desc);
++
++	/* acknowledge interrupt */
++	irncr = ltq_r32(info->membase[0] + GPIO_IRNCR);
++	ltq_w32(irncr, info->membase[0] + GPIO_IRNCR);
++
++	desc->irq_data.chip->irq_ack(&desc->irq_data);
++
++	for (i = 0; i < info->num_exin; i++)
++		if (irncr & BIT(PORT_PIN(info->exin[i])))
++			generic_handle_irq(svip_gpio_to_exin(info->exin[i]) +
++				SVIP_EIU_START);
++}
++#endif
++
++
++static int svip_gpio_irq_map(struct irq_domain *d, unsigned int irq,
++	irq_hw_number_t hw)
++{
++	struct ltq_pinmux_info *info = d->host_data;
++
++	irq_set_chip_and_handler_name(irq, &svip_gpio_irq_chip,
++		handle_simple_irq, "mux");
++	irq_set_chip_data(irq, info);
++
++	/* set to negative logic (falling edge, low level) */
++	gpio_setbit(info->membase[0], GPIO_EXINTCR0, hw);
++	return 0;
++}
++
++static struct irq_chip svip_gpio_irq_chip = {
++	.name = "gpio_irq_mux",
++	.irq_mask = svip_gpio_disable_irq,
++	.irq_unmask = svip_gpio_enable_irq,
++	.irq_ack = svip_gpio_ack_irq,
++	.irq_mask_ack = svip_gpio_mask_and_ack_irq,
++	.irq_set_type = svip_gpio_irq_type,
++};
++
++static const struct irq_domain_ops irq_domain_ops = {
++	.xlate = irq_domain_xlate_onetwocell,
++	.map = svip_gpio_irq_map,
++};
++
++
++#if 0
++static struct irqaction gpio_cascade = {
++	.handler = no_action,
++	.flags = IRQF_DISABLED,
++	.name = "gpio_cascade",
++};
++#endif
++
++
++static struct gpio_chip svip_gpio_chip[PORTS] = {
++	GPIO_CHIP_SVIP("gpio-svip-bank0"),
++	GPIO_CHIP_SVIP("gpio-svip-bank1"),
++	GPIO_CHIP_SVIP("gpio-svip-bank2"),
++	GPIO_CHIP_SVIP("gpio-svip-bank3"),
++	GPIO_CHIP_SVIP("gpio-svip-bank4")
++};
++
++/* --------- register the pinctrl layer --------- */
++/* External Interrupt Control Registers: */
++/* todo map External Interrupt number to the global GPIO number */
++/* P0_EXINTCR0 P0_EXINTCR1 */
++static const unsigned svip_exin_pin_map_port0[] = {
++	GPIO7, GPIO8, GPIO9, GPIO10, GPIO11, GPIO12, GPIO13, GPIO14, GPIO15,
++	GPIO16, GPIO17, GPIO19 };
++/* P1_EXINTCR0 P1_EXINTCR1 */
++static const unsigned svip_exin_pin_map_port1[] = {
++	GPIO50, GPIO49, GPIO48, GPIO47, GPIO51 };
++#define SVIP_EXIN_NONE 0xffff
++/* external interrupt possible only on port 0 and port 1 */
++static const unsigned svip_exin_pin_map_port2[] = { SVIP_EXIN_NONE };
++static const unsigned svip_exin_pin_map_port3[] = { SVIP_EXIN_NONE };
++static const unsigned svip_exin_pin_map_port4[] = { SVIP_EXIN_NONE };
++
++#define soc_cfg_port_x(_port_num, _num_exin)\
++	{SVIP_MAX_PIN_PORT##_port_num, svip_mfp_port##_port_num,\
++	svip_grps_port##_port_num, ARRAY_SIZE(svip_grps_port##_port_num),\
++	svip_funcs_port##_port_num, ARRAY_SIZE(svip_funcs_port##_port_num),\
++	svip_exin_pin_map_port##_port_num, _num_exin}
++
++static struct pinctrl_svip_soc {
++	int pin_count;
++	const struct ltq_mfp_pin *mfp;
++	const struct ltq_pin_group *grps;
++	unsigned int num_grps;
++	const struct ltq_pmx_func *funcs;
++	unsigned int num_funcs;
++	const unsigned *exin;
++	unsigned int num_exin;
++} soc_cfg[PORTS] = {
++soc_cfg_port_x(0, 12),
++soc_cfg_port_x(1, 5),
++soc_cfg_port_x(2, 0),
++soc_cfg_port_x(3, 0),
++soc_cfg_port_x(4, 0), };
++
++static const struct of_device_id svip_match[] = {
++{ .compatible = "lantiq,pinctrl-svip" },
++{ }, };
++MODULE_DEVICE_TABLE(of, svip_match);
++
++static int pinmux_svip_probe(struct platform_device *pdev)
++{
++	struct device_node *node = pdev->dev.of_node;
++	struct pinctrl_gpio_range *svip_gpio_range;
++	int ret = 0;
++	/* load and remap the gpio resources of the different banks */
++	struct platform_device *ppdev;
++	const __be32 *prop;
++	int bank;
++	const struct pinctrl_svip_soc *svip_soc;
++	struct resource *gpiores;
++	int i = 0;
++
++	svip_gpio_range = devm_kzalloc(&pdev->dev,
++		sizeof(struct pinctrl_gpio_range), GFP_KERNEL);
++	if (!svip_gpio_range)
++		return -ENOMEM;
++
++	if (!of_device_is_available(node)) {
++		dev_err(&pdev->dev, "!of_device_is_available\n");
++		return -1;
++	}
++	prop = of_get_property(node, "lantiq,bank", NULL);
++	if (!prop) {
++		dev_err(&pdev->dev, "failed to read banks\n");
++		return -1;
++	}
++	bank = be32_to_cpup(prop);
++
++	ppdev = of_find_device_by_node(node);
++	if (!ppdev) {
++		dev_err(&pdev->dev, "failed to find port pdev\n");
++		return -1;
++	}
++
++	svip_soc = &soc_cfg[bank];
++	/* find out how many pads we have */
++	svip_gpio_chip[bank].ngpio = svip_soc->pin_count;
++
++	svip_info[bank].clk[0] = clk_get(&ppdev->dev, NULL);
++	if (IS_ERR(svip_info[bank].clk[0])) {
++		dev_err(&ppdev->dev, "failed to get clock\n");
++		return PTR_ERR(svip_info[bank].clk[0]);
++	}
++
++	/* get and remap our register range */
++	gpiores = platform_get_resource(pdev, IORESOURCE_MEM, 0);
++	/*if (of_address_to_resource(node, 0, gpiores))
++	 return -1;*/
++
++	svip_info[bank].membase[0] = devm_ioremap_resource(&pdev->dev,
++		gpiores);
++	if (IS_ERR(svip_info[bank].membase[0]))
++		return PTR_ERR(svip_info[bank].membase[0]);
++
++	/* load our pad descriptors */
++	svip_info[bank].pads = devm_kzalloc(&pdev->dev,
++		sizeof(struct pinctrl_pin_desc) * svip_gpio_chip[bank].ngpio,
++		GFP_KERNEL);
++
++	if (!svip_info[bank].pads) {
++		dev_err(&pdev->dev, "Failed to allocate pads\n");
++		return -ENOMEM;
++	}
++	for (i = 0; i < svip_gpio_chip[bank].ngpio; i++) {
++		/* strlen("ioXY") + 1 = 5 */
++		char *name = devm_kzalloc(&pdev->dev, 5, GFP_KERNEL);
++
++		if (!name) {
++			dev_err(&pdev->dev, "Failed to allocate pad name\n");
++			return -ENOMEM;
++		}
++		snprintf(name, 5, "io%d", bank * PINS + i);
++		svip_info[bank].pads[i].number = bank * PINS + i;
++		svip_info[bank].pads[i].name = name;
++	}
++	clk_enable(svip_info[bank].clk[0]);
++	svip_pctrl_desc[bank].pins = svip_info[bank].pads;
++
++	/* load the gpio chip */
++	svip_gpio_chip[bank].dev = &pdev->dev;
++	of_gpiochip_add(&svip_gpio_chip[bank]);
++	ret = gpiochip_add(&svip_gpio_chip[bank]);
++	if (ret) {
++		dev_err(&pdev->dev, "Failed to register gpio chip\n");
++		return ret;
++	}
++
++	/* setup the data needed by pinctrl */
++	svip_pctrl_desc[bank].name = dev_name(&pdev->dev);
++	svip_pctrl_desc[bank].npins = svip_gpio_chip[bank].ngpio;
++	svip_info[bank].num_pads = svip_gpio_chip[bank].ngpio;
++	svip_info[bank].num_mfp = svip_gpio_chip[bank].ngpio;
++	svip_info[bank].mfp = svip_soc->mfp;
++	svip_info[bank].grps = svip_soc->grps;
++	svip_info[bank].num_grps = svip_soc->num_grps;
++	svip_info[bank].funcs = svip_soc->funcs;
++	svip_info[bank].num_funcs = svip_soc->num_funcs;
++	svip_info[bank].exin = svip_soc->exin;
++	svip_info[bank].num_exin = svip_soc->num_exin;
++
++	/* register with the generic lantiq layer */
++	ret = ltq_pinctrl_register(pdev, &svip_info[bank]);
++	if (ret) {
++		dev_err(&pdev->dev, "Failed to register pinctrl driver\n");
++		return ret;
++	}
++
++#if 0
++	for (i = 0; i < svip_info[bank].num_exin; i++)
++		irq_domain_add_legacy(node, 1,
++			svip_gpio_to_irq(&svip_gpio_chip[bank],
++				svip_info[bank].exin[i]),
++				PORT_PIN(svip_info[bank].exin[i]),
++				&irq_domain_ops, &svip_info[bank]);
++#endif
++
++	/* finish with registering the gpio range in pinctrl */
++	svip_gpio_range->npins = svip_gpio_chip[bank].ngpio;
++	svip_gpio_range->base = svip_gpio_chip[bank].base;
++	svip_gpio_range->pin_base = svip_info[bank].pads[0].number;
++	pinctrl_add_gpio_range(svip_info[bank].pctrl, svip_gpio_range);
++	dev_info(&pdev->dev, "Init done\n");
++	return 0;
++}
++
++static struct platform_driver pinmux_svip_driver = {
++	.probe = pinmux_svip_probe,
++	.driver = {
++		.name = "pinctrl-svip",
++		.owner = THIS_MODULE,
++		.of_match_table = svip_match,
++	},
++};
++
++static int __init pinmux_svip_init(void)
++{
++	return platform_driver_register(&pinmux_svip_driver);
++}
++
++core_initcall_sync(pinmux_svip_init);
diff --git a/target/linux/lantiq/patches-3.10/2220-lantiq-add-eth-drv.patch b/target/linux/lantiq/patches-3.10/2220-lantiq-add-eth-drv.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2220-lantiq-add-eth-drv.patch
@@ -0,0 +1,3939 @@
+# HG changeset patch
+# Parent 904769a83c013ba523943e158d0ef70df2708562
+
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/es.h b/arch/mips/include/asm/mach-lantiq/svip/es.h
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/include/asm/mach-lantiq/svip/es.h
+@@ -0,0 +1,2096 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++
++#ifndef __ES_REG_H
++#define __ES_REG_H
++
++#define es_r32(reg) ltq_r32(&es->reg)
++#define es_w32(val, reg) ltq_w32(val, &es->reg)
++#define es_w32_mask(clear, set, reg) ltq_w32_mask(clear, set, &es->reg)
++
++/** ES register structure */
++struct svip_reg_es {
++	volatile unsigned long  ps;  /*  0x0000 */
++	volatile unsigned long  p0_ctl;  /*  0x0004 */
++	volatile unsigned long  p1_ctl;  /*  0x0008 */
++	volatile unsigned long  p2_ctl;  /*  0x000C */
++	volatile unsigned long  p0_vlan;  /*  0x0010 */
++	volatile unsigned long  p1_vlan;  /*  0x0014 */
++	volatile unsigned long  p2_vlan;  /*  0x0018 */
++	volatile unsigned long  reserved1[1];  /*  0x001C */
++	volatile unsigned long  p0_inctl;  /*  0x0020 */
++	volatile unsigned long  p1_inctl;  /*  0x0024 */
++	volatile unsigned long  p2_inctl;  /*  0x0028 */
++	volatile unsigned long  reserved2[1];  /*  0x002C */
++	volatile unsigned long  p0_ecs_q32;  /*  0x0030 */
++	volatile unsigned long  p0_ecs_q10;  /*  0x0034 */
++	volatile unsigned long  p0_ecw_q32;  /*  0x0038 */
++	volatile unsigned long  p0_ecw_q10;  /*  0x003C */
++	volatile unsigned long  p1_ecs_q32;  /*  0x0040 */
++	volatile unsigned long  p1_ecs_q10;  /*  0x0044 */
++	volatile unsigned long  p1_ecw_q32;  /*  0x0048 */
++	volatile unsigned long  p1_ecw_q10;  /*  0x004C */
++	volatile unsigned long  p2_ecs_q32;  /*  0x0050 */
++	volatile unsigned long  p2_ecs_q10;  /*  0x0054 */
++	volatile unsigned long  p2_ecw_q32;  /*  0x0058 */
++	volatile unsigned long  p2_ecw_q10;  /*  0x005C */
++	volatile unsigned long  int_ena;  /*  0x0060 */
++	volatile unsigned long  int_st;  /*  0x0064 */
++	volatile unsigned long  sw_gctl0;  /*  0x0068 */
++	volatile unsigned long  sw_gctl1;  /*  0x006C */
++	volatile unsigned long  arp;  /*  0x0070 */
++	volatile unsigned long  strm_ctl;  /*  0x0074 */
++	volatile unsigned long  rgmii_ctl;  /*  0x0078 */
++	volatile unsigned long  prt_1p;  /*  0x007C */
++	volatile unsigned long  gbkt_szbs;  /*  0x0080 */
++	volatile unsigned long  gbkt_szebs;  /*  0x0084 */
++	volatile unsigned long  bf_th;  /*  0x0088 */
++	volatile unsigned long  pmac_hd_ctl;  /*  0x008C */
++	volatile unsigned long  pmac_sa1;  /*  0x0090 */
++	volatile unsigned long  pmac_sa2;  /*  0x0094 */
++	volatile unsigned long  pmac_da1;  /*  0x0098 */
++	volatile unsigned long  pmac_da2;  /*  0x009C */
++	volatile unsigned long  pmac_vlan;  /*  0x00A0 */
++	volatile unsigned long  pmac_tx_ipg;  /*  0x00A4 */
++	volatile unsigned long  pmac_rx_ipg;  /*  0x00A8 */
++	volatile unsigned long  adr_tb_ctl0;  /*  0x00AC */
++	volatile unsigned long  adr_tb_ctl1;  /*  0x00B0 */
++	volatile unsigned long  adr_tb_ctl2;  /*  0x00B4 */
++	volatile unsigned long  adr_tb_st0;  /*  0x00B8 */
++	volatile unsigned long  adr_tb_st1;  /*  0x00BC */
++	volatile unsigned long  adr_tb_st2;  /*  0x00C0 */
++	volatile unsigned long  rmon_ctl;  /*  0x00C4 */
++	volatile unsigned long  rmon_st;  /*  0x00C8 */
++	volatile unsigned long  mdio_ctl;  /*  0x00CC */
++	volatile unsigned long  mdio_data;  /*  0x00D0 */
++	volatile unsigned long  tp_flt_act;  /*  0x00D4 */
++	volatile unsigned long  prtcl_flt_act;  /*  0x00D8 */
++	volatile unsigned long  reserved4[9];  /*  0xdc */
++	volatile unsigned long  vlan_flt0;  /*  0x0100 */
++	volatile unsigned long  vlan_flt1;  /*  0x0104 */
++	volatile unsigned long  vlan_flt2;  /*  0x0108 */
++	volatile unsigned long  vlan_flt3;  /*  0x010C */
++	volatile unsigned long  vlan_flt4;  /*  0x0110 */
++	volatile unsigned long  vlan_flt5;  /*  0x0114 */
++	volatile unsigned long  vlan_flt6;  /*  0x0118 */
++	volatile unsigned long  vlan_flt7;  /*  0x011C */
++	volatile unsigned long  vlan_flt8;  /*  0x0120 */
++	volatile unsigned long  vlan_flt9;  /*  0x0124 */
++	volatile unsigned long  vlan_flt10;  /*  0x0128 */
++	volatile unsigned long  vlan_flt11;  /*  0x012C */
++	volatile unsigned long  vlan_flt12;  /*  0x0130 */
++	volatile unsigned long  vlan_flt13;  /*  0x0134 */
++	volatile unsigned long  vlan_flt14;  /*  0x0138 */
++	volatile unsigned long  vlan_flt15;  /*  0x013C */
++	volatile unsigned long  tp_flt10;  /*  0x0140 */
++	volatile unsigned long  tp_flt32;  /*  0x0144 */
++	volatile unsigned long  tp_flt54;  /*  0x0148 */
++	volatile unsigned long  tp_flt76;  /*  0x014C */
++	volatile unsigned long  dfsrv_map0;  /*  0x0150 */
++	volatile unsigned long  dfsrv_map1;  /*  0x0154 */
++	volatile unsigned long  dfsrv_map2;  /*  0x0158 */
++	volatile unsigned long  dfsrv_map3;  /*  0x015C */
++	volatile unsigned long  tcp_pf0;  /*  0x0160 */
++	volatile unsigned long  tcp_pf1;  /*  0x0164 */
++	volatile unsigned long  tcp_pf2;  /*  0x0168 */
++	volatile unsigned long  tcp_pf3;  /*  0x016C */
++	volatile unsigned long  tcp_pf4;  /*  0x0170 */
++	volatile unsigned long  tcp_pf5;  /*  0x0174 */
++	volatile unsigned long  tcp_pf6;  /*  0x0178 */
++	volatile unsigned long  tcp_pf7;  /*  0x017C */
++	volatile unsigned long  ra_03_00;  /*  0x0180 */
++	volatile unsigned long  ra_07_04;  /*  0x0184 */
++	volatile unsigned long  ra_0b_08;  /*  0x0188 */
++	volatile unsigned long  ra_0f_0c;  /*  0x018C */
++	volatile unsigned long  ra_13_10;  /*  0x0190 */
++	volatile unsigned long  ra_17_14;  /*  0x0194 */
++	volatile unsigned long  ra_1b_18;  /*  0x0198 */
++	volatile unsigned long  ra_1f_1c;  /*  0x019C */
++	volatile unsigned long  ra_23_20;  /*  0x01A0 */
++	volatile unsigned long  ra_27_24;  /*  0x01A4 */
++	volatile unsigned long  ra_2b_28;  /*  0x01A8 */
++	volatile unsigned long  ra_2f_2c;  /*  0x01AC */
++	volatile unsigned long  prtcl_f0;  /*  0x01B0 */
++	volatile unsigned long  prtcl_f1;  /*  0x01B4 */
++};
++
++/*******************************************************************************
++ * ES
++ ******************************************************************************/
++#define LTQ_ES_PS_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0000))
++#define LTQ_ES_P0_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0004))
++#define LTQ_ES_P1_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0008))
++#define LTQ_ES_P2_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x000C))
++#define LTQ_ES_P0_VLAN_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0010))
++#define LTQ_ES_P1_VLAN_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0014))
++#define LTQ_ES_P2_VLAN_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0018))
++#define LTQ_ES_P0_INCTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0020))
++#define LTQ_ES_P1_INCTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0024))
++#define LTQ_ES_P2_INCTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0028))
++#define LTQ_ES_P0_ECS_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0030))
++#define LTQ_ES_P0_ECS_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0034))
++#define LTQ_ES_P0_ECW_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0038))
++#define LTQ_ES_P0_ECW_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x003C))
++#define LTQ_ES_P1_ECS_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0030))
++#define LTQ_ES_P1_ECS_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0034))
++#define LTQ_ES_P1_ECW_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0038))
++#define LTQ_ES_P1_ECW_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x003C))
++#define LTQ_ES_P2_ECS_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0030))
++#define LTQ_ES_P2_ECS_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0034))
++#define LTQ_ES_P2_ECW_Q32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0038))
++#define LTQ_ES_P2_ECW_Q10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x003C))
++#define LTQ_ES_INT_ENA_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0060))
++#define LTQ_ES_INT_ST_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0064))
++#define LTQ_ES_SW_GCTL0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0068))
++#define LTQ_ES_SW_GCTL1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x006C))
++#define LTQ_ES_ARP_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0070))
++#define LTQ_ES_STRM_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0074))
++#define LTQ_ES_RGMII_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0078))
++#define LTQ_ES_PRT_1P_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x007C))
++#define LTQ_ES_GBKT_SZBS_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0080))
++#define LTQ_ES_GBKT_SZEBS_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0084))
++#define LTQ_ES_BF_TH_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0088))
++#define LTQ_ES_PMAC_HD_CTL   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x008C))
++#define LTQ_ES_PMAC_SA1   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0090))
++#define LTQ_ES_PMAC_SA2   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0094))
++#define LTQ_ES_PMAC_DA1   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0098))
++#define LTQ_ES_PMAC_DA2   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x009C))
++#define LTQ_ES_PMAC_VLAN   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00A0))
++#define LTQ_ES_PMAC_TX_IPG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00A4))
++#define LTQ_ES_PMAC_RX_IPG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00A8))
++#define LTQ_ES_ADR_TB_CTL0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00AC))
++#define LTQ_ES_ADR_TB_CTL1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00B0))
++#define LTQ_ES_ADR_TB_CTL2_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00B4))
++#define LTQ_ES_ADR_TB_ST0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00B8))
++#define LTQ_ES_ADR_TB_ST1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00BC))
++#define LTQ_ES_ADR_TB_ST2_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00C0))
++#define LTQ_ES_RMON_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00C4))
++#define LTQ_ES_RMON_ST_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00C8))
++#define LTQ_ES_MDIO_CTL_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00CC))
++#define LTQ_ES_MDIO_DATA_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00D0))
++#define LTQ_ES_TP_FLT_ACT_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00D4))
++#define LTQ_ES_PRTCL_FLT_ACT_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x00D8))
++#define LTQ_ES_VLAN_FLT0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0100))
++#define LTQ_ES_VLAN_FLT1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0104))
++#define LTQ_ES_VLAN_FLT2_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0108))
++#define LTQ_ES_VLAN_FLT3_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x010C))
++#define LTQ_ES_VLAN_FLT4_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0110))
++#define LTQ_ES_VLAN_FLT5_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0114))
++#define LTQ_ES_VLAN_FLT6_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0118))
++#define LTQ_ES_VLAN_FLT7_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x011C))
++#define LTQ_ES_VLAN_FLT8_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0120))
++#define LTQ_ES_VLAN_FLT9_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0124))
++#define LTQ_ES_VLAN_FLT10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0128))
++#define LTQ_ES_VLAN_FLT11_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x012C))
++#define LTQ_ES_VLAN_FLT12_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0130))
++#define LTQ_ES_VLAN_FLT13_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0134))
++#define LTQ_ES_VLAN_FLT14_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0138))
++#define LTQ_ES_VLAN_FLT15_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x013C))
++#define LTQ_ES_TP_FLT10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0140))
++#define LTQ_ES_TP_FLT32_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0144))
++#define LTQ_ES_TP_FLT54_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0148))
++#define LTQ_ES_TP_FLT76_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x014C))
++#define LTQ_ES_DFSRV_MAP0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0150))
++#define LTQ_ES_DFSRV_MAP1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0154))
++#define LTQ_ES_DFSRV_MAP2_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0158))
++#define LTQ_ES_DFSRV_MAP3_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x015C))
++#define LTQ_ES_TCP_PF0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0160))
++#define LTQ_ES_TCP_PF1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0164))
++#define LTQ_ES_TCP_PF2_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0168))
++#define LTQ_ES_TCP_PF3_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x016C))
++#define LTQ_ES_TCP_PF4_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0170))
++#define LTQ_ES_TCP_PF5_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0174))
++#define LTQ_ES_TCP_PF6_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0178))
++#define LTQ_ES_TCP_PF7_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x017C))
++#define LTQ_ES_RA_03_00_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0180))
++#define LTQ_ES_RA_07_04_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0184))
++#define LTQ_ES_RA_0B_08_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0188))
++#define LTQ_ES_RA_0F_0C_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x018C))
++#define LTQ_ES_RA_13_10_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0190))
++#define LTQ_ES_RA_17_14_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0194))
++#define LTQ_ES_RA_1B_18_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x0198))
++#define LTQ_ES_RA_1F_1C_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x019C))
++#define LTQ_ES_RA_23_20_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01A0))
++#define LTQ_ES_RA_27_24_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01A4))
++#define LTQ_ES_RA_2B_28_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01A8))
++#define LTQ_ES_RA_2F_2C_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01AC))
++#define LTQ_ES_PRTCL_F0_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01B0))
++#define LTQ_ES_PRTCL_F1_REG   ((volatile u32 *)(KSEG1 + LTQ_ES_BASE_ADDR + 0x01B4))
++
++/*******************************************************************************
++ * Port Status Register
++ ******************************************************************************/
++
++/* Port 1 Flow Control Status (12) */
++#define LTQ_ES_PS_REG_P1FCS   (0x1 << 12)
++#define LTQ_ES_PS_REG_P1FCS_GET(val)   ((((val) & LTQ_ES_PS_REG_P1FCS) >> 12) & 0x1)
++/* Port 1 Duplex Status (11) */
++#define LTQ_ES_PS_REG_P1DS   (0x1 << 11)
++#define LTQ_ES_PS_REG_P1DS_GET(val)   ((((val) & LTQ_ES_PS_REG_P1DS) >> 11) & 0x1)
++/* Port 1 Speed High Status (10) */
++#define LTQ_ES_PS_REG_P1SHS   (0x1 << 10)
++#define LTQ_ES_PS_REG_P1SHS_GET(val)   ((((val) & LTQ_ES_PS_REG_P1SHS) >> 10) & 0x1)
++/* Port 1 Speed Status (9) */
++#define LTQ_ES_PS_REG_P1SS   (0x1 << 9)
++#define LTQ_ES_PS_REG_P1SS_GET(val)   ((((val) & LTQ_ES_PS_REG_P1SS) >> 9) & 0x1)
++/* Port 1 Link Status (8) */
++#define LTQ_ES_PS_REG_P1LS   (0x1 << 8)
++#define LTQ_ES_PS_REG_P1LS_GET(val)   ((((val) & LTQ_ES_PS_REG_P1LS) >> 8) & 0x1)
++/* Port 0 Flow Control Status (4) */
++#define LTQ_ES_PS_REG_P0FCS   (0x1 << 4)
++#define LTQ_ES_PS_REG_P0FCS_GET(val)   ((((val) & LTQ_ES_PS_REG_P0FCS) >> 4) & 0x1)
++/* Port 0 Duplex Status (3) */
++#define LTQ_ES_PS_REG_P0DS   (0x1 << 3)
++#define LTQ_ES_PS_REG_P0DS_GET(val)   ((((val) & LTQ_ES_PS_REG_P0DS) >> 3) & 0x1)
++/* Port 0 Speed High Status (2) */
++#define LTQ_ES_PS_REG_P0SHS   (0x1 << 2)
++#define LTQ_ES_PS_REG_P0SHS_GET(val)   ((((val) & LTQ_ES_PS_REG_P0SHS) >> 2) & 0x1)
++/* Port 0 Speed Status (1) */
++#define LTQ_ES_PS_REG_P0SS   (0x1 << 1)
++#define LTQ_ES_PS_REG_P0SS_GET(val)   ((((val) & LTQ_ES_PS_REG_P0SS) >> 1) & 0x1)
++/* Port 0 Link Status (0) */
++#define LTQ_ES_PS_REG_P0LS   (0x1)
++#define LTQ_ES_PS_REG_P0LS_GET(val)   ((((val) & LTQ_ES_PS_REG_P0LS) >> 0) & 0x1)
++
++/*******************************************************************************
++ * P0 Control Register
++ ******************************************************************************/
++
++/* STP/RSTP port state (31:30) */
++#define LTQ_ES_P0_CTL_REG_SPS   (0x3 << 30)
++#define LTQ_ES_P0_CTL_REG_SPS_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_P0_CTL_REG_SPS_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_SPS) >> 30) & 0x3)
++#define LTQ_ES_P0_CTL_REG_SPS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_SPS) | (((val) & 0x3) << 30))
++/* TCP/UDP PRIEN (29) */
++#define LTQ_ES_P0_CTL_REG_TCPE   (0x1 << 29)
++#define LTQ_ES_P0_CTL_REG_TCPE_VAL(val)   (((val) & 0x1) << 29)
++#define LTQ_ES_P0_CTL_REG_TCPE_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_TCPE) >> 29) & 0x1)
++#define LTQ_ES_P0_CTL_REG_TCPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_TCPE) | (((val) & 0x1) << 29))
++/*  IP over TCP/UDP (28) */
++#define LTQ_ES_P0_CTL_REG_IPOVTU   (0x1 << 28)
++#define LTQ_ES_P0_CTL_REG_IPOVTU_VAL(val)   (((val) & 0x1) << 28)
++#define LTQ_ES_P0_CTL_REG_IPOVTU_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_IPOVTU) >> 28) & 0x1)
++#define LTQ_ES_P0_CTL_REG_IPOVTU_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_IPOVTU) | (((val) & 0x1) << 28))
++/* VLAN Priority Enable (27) */
++#define LTQ_ES_P0_CTL_REG_VPE   (0x1 << 27)
++#define LTQ_ES_P0_CTL_REG_VPE_VAL(val)   (((val) & 0x1) << 27)
++#define LTQ_ES_P0_CTL_REG_VPE_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_VPE) >> 27) & 0x1)
++#define LTQ_ES_P0_CTL_REG_VPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_VPE) | (((val) & 0x1) << 27))
++/* Service Priority Enable (26) */
++#define LTQ_ES_P0_CTL_REG_SPE   (0x1 << 26)
++#define LTQ_ES_P0_CTL_REG_SPE_VAL(val)   (((val) & 0x1) << 26)
++#define LTQ_ES_P0_CTL_REG_SPE_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_SPE) >> 26) & 0x1)
++#define LTQ_ES_P0_CTL_REG_SPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_SPE) | (((val) & 0x1) << 26))
++/* IP over VLAN PRI (25) */
++#define LTQ_ES_P0_CTL_REG_IPVLAN   (0x1 << 25)
++#define LTQ_ES_P0_CTL_REG_IPVLAN_VAL(val)   (((val) & 0x1) << 25)
++#define LTQ_ES_P0_CTL_REG_IPVLAN_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_IPVLAN) >> 25) & 0x1)
++#define LTQ_ES_P0_CTL_REG_IPVLAN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_IPVLAN) | (((val) & 0x1) << 25))
++/* Ether Type Priority Enable (24) */
++#define LTQ_ES_P0_CTL_REG_TPE   (0x1 << 24)
++#define LTQ_ES_P0_CTL_REG_TPE_VAL(val)   (((val) & 0x1) << 24)
++#define LTQ_ES_P0_CTL_REG_TPE_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_TPE) >> 24) & 0x1)
++#define LTQ_ES_P0_CTL_REG_TPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_TPE) | (((val) & 0x1) << 24))
++/* Force Link Up (18) */
++#define LTQ_ES_P0_CTL_REG_FLP   (0x1 << 18)
++#define LTQ_ES_P0_CTL_REG_FLP_VAL(val)   (((val) & 0x1) << 18)
++#define LTQ_ES_P0_CTL_REG_FLP_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_FLP) >> 18) & 0x1)
++#define LTQ_ES_P0_CTL_REG_FLP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_FLP) | (((val) & 0x1) << 18))
++/* Force Link Down (17) */
++#define LTQ_ES_P0_CTL_REG_FLD   (0x1 << 17)
++#define LTQ_ES_P0_CTL_REG_FLD_VAL(val)   (((val) & 0x1) << 17)
++#define LTQ_ES_P0_CTL_REG_FLD_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_FLD) >> 17) & 0x1)
++#define LTQ_ES_P0_CTL_REG_FLD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_FLD) | (((val) & 0x1) << 17))
++/* Ratio Mode for WFQ (16) */
++#define LTQ_ES_P0_CTL_REG_RMWFQ   (0x1 << 16)
++#define LTQ_ES_P0_CTL_REG_RMWFQ_VAL(val)   (((val) & 0x1) << 16)
++#define LTQ_ES_P0_CTL_REG_RMWFQ_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_RMWFQ) >> 16) & 0x1)
++#define LTQ_ES_P0_CTL_REG_RMWFQ_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_RMWFQ) | (((val) & 0x1) << 16))
++/* Aging Disable (15) */
++#define LTQ_ES_P0_CTL_REG_AD   (0x1 << 15)
++#define LTQ_ES_P0_CTL_REG_AD_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_P0_CTL_REG_AD_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_AD) >> 15) & 0x1)
++#define LTQ_ES_P0_CTL_REG_AD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_AD) | (((val) & 0x1) << 15))
++/* Learning Disable (14) */
++#define LTQ_ES_P0_CTL_REG_LD   (0x1 << 14)
++#define LTQ_ES_P0_CTL_REG_LD_VAL(val)   (((val) & 0x1) << 14)
++#define LTQ_ES_P0_CTL_REG_LD_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_LD) >> 14) & 0x1)
++#define LTQ_ES_P0_CTL_REG_LD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_LD) | (((val) & 0x1) << 14))
++/* Maximum Number of Addresses (12:8) */
++#define LTQ_ES_P0_CTL_REG_MNA024   (0x1f << 8)
++#define LTQ_ES_P0_CTL_REG_MNA024_VAL(val)   (((val) & 0x1f) << 8)
++#define LTQ_ES_P0_CTL_REG_MNA024_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_MNA024) >> 8) & 0x1f)
++#define LTQ_ES_P0_CTL_REG_MNA024_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_MNA024) | (((val) & 0x1f) << 8))
++/* PPPOE Port Only (7) */
++#define LTQ_ES_P0_CTL_REG_PPPOEP   (0x1 << 7)
++#define LTQ_ES_P0_CTL_REG_PPPOEP_VAL(val)   (((val) & 0x1) << 7)
++#define LTQ_ES_P0_CTL_REG_PPPOEP_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_PPPOEP) >> 7) & 0x1)
++#define LTQ_ES_P0_CTL_REG_PPPOEP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_PPPOEP) | (((val) & 0x1) << 7))
++/* PPPOE Manage (6) */
++#define LTQ_ES_P0_CTL_REG_PM   (0x1 << 6)
++#define LTQ_ES_P0_CTL_REG_PM_VAL(val)   (((val) & 0x1) << 6)
++#define LTQ_ES_P0_CTL_REG_PM_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_PM) >> 6) & 0x1)
++#define LTQ_ES_P0_CTL_REG_PM_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_PM) | (((val) & 0x1) << 6))
++/* Port Mirror Option (5:4) */
++#define LTQ_ES_P0_CTL_REG_IPMO   (0x3 << 4)
++#define LTQ_ES_P0_CTL_REG_IPMO_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_P0_CTL_REG_IPMO_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_IPMO) >> 4) & 0x3)
++#define LTQ_ES_P0_CTL_REG_IPMO_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_IPMO) | (((val) & 0x3) << 4))
++/* 802.1x Port Authorized state (3:2) */
++#define LTQ_ES_P0_CTL_REG_PAS   (0x3 << 2)
++#define LTQ_ES_P0_CTL_REG_PAS_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_P0_CTL_REG_PAS_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_PAS) >> 2) & 0x3)
++#define LTQ_ES_P0_CTL_REG_PAS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_PAS) | (((val) & 0x3) << 2))
++/* Drop Scheme for voilation 802.1x (1) */
++#define LTQ_ES_P0_CTL_REG_DSV8021X   (0x1 << 1)
++#define LTQ_ES_P0_CTL_REG_DSV8021X_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_ES_P0_CTL_REG_DSV8021X_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_DSV8021X) >> 1) & 0x1)
++#define LTQ_ES_P0_CTL_REG_DSV8021X_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_DSV8021X) | (((val) & 0x1) << 1))
++/* ByPass Mode for Output (0) */
++#define LTQ_ES_P0_CTL_REG_BYPASS   (0x1)
++#define LTQ_ES_P0_CTL_REG_BYPASS_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_ES_P0_CTL_REG_BYPASS_GET(val)   ((((val) & LTQ_ES_P0_CTL_REG_BYPASS) >> 0) & 0x1)
++#define LTQ_ES_P0_CTL_REG_BYPASS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_CTL_REG_BYPASS) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * Port 0 VLAN Control Register
++ ******************************************************************************/
++
++/* Default FID (31:30) */
++#define LTQ_ES_P0_VLAN_REG_DFID   (0x3 << 30)
++#define LTQ_ES_P0_VLAN_REG_DFID_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_P0_VLAN_REG_DFID_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_DFID) >> 30) & 0x3)
++#define LTQ_ES_P0_VLAN_REG_DFID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_DFID) | (((val) & 0x3) << 30))
++/* Tagged Base VLAN Enable (29) */
++#define LTQ_ES_P0_VLAN_REG_TBVE   (0x1 << 29)
++#define LTQ_ES_P0_VLAN_REG_TBVE_VAL(val)   (((val) & 0x1) << 29)
++#define LTQ_ES_P0_VLAN_REG_TBVE_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_TBVE) >> 29) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_TBVE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_TBVE) | (((val) & 0x1) << 29))
++/* Input Force No TAG Enable (28) */
++#define LTQ_ES_P0_VLAN_REG_IFNTE   (0x1 << 28)
++#define LTQ_ES_P0_VLAN_REG_IFNTE_VAL(val)   (((val) & 0x1) << 28)
++#define LTQ_ES_P0_VLAN_REG_IFNTE_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_IFNTE) >> 28) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_IFNTE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_IFNTE) | (((val) & 0x1) << 28))
++/* VID Check with the VID table (27) */
++#define LTQ_ES_P0_VLAN_REG_VC   (0x1 << 27)
++#define LTQ_ES_P0_VLAN_REG_VC_VAL(val)   (((val) & 0x1) << 27)
++#define LTQ_ES_P0_VLAN_REG_VC_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_VC) >> 27) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_VC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_VC) | (((val) & 0x1) << 27))
++/* VLAN Security Disable (26) */
++#define LTQ_ES_P0_VLAN_REG_VSD   (0x1 << 26)
++#define LTQ_ES_P0_VLAN_REG_VSD_VAL(val)   (((val) & 0x1) << 26)
++#define LTQ_ES_P0_VLAN_REG_VSD_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_VSD) >> 26) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_VSD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_VSD) | (((val) & 0x1) << 26))
++/* Admit Only VLAN_Tagged Packet (25) */
++#define LTQ_ES_P0_VLAN_REG_AOVTP   (0x1 << 25)
++#define LTQ_ES_P0_VLAN_REG_AOVTP_VAL(val)   (((val) & 0x1) << 25)
++#define LTQ_ES_P0_VLAN_REG_AOVTP_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_AOVTP) >> 25) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_AOVTP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_AOVTP) | (((val) & 0x1) << 25))
++/* VLAN Member Check Enable (24) */
++#define LTQ_ES_P0_VLAN_REG_VMCE   (0x1 << 24)
++#define LTQ_ES_P0_VLAN_REG_VMCE_VAL(val)   (((val) & 0x1) << 24)
++#define LTQ_ES_P0_VLAN_REG_VMCE_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_VMCE) >> 24) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_VMCE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_VMCE) | (((val) & 0x1) << 24))
++/* Reserved (23:19) */
++#define LTQ_ES_P0_VLAN_REG_RES   (0x1f << 19)
++#define LTQ_ES_P0_VLAN_REG_RES_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_RES) >> 19) & 0x1f)
++/* Default VLAN Port Map (18:16) */
++#define LTQ_ES_P0_VLAN_REG_DVPM   (0x7 << 16)
++#define LTQ_ES_P0_VLAN_REG_DVPM_VAL(val)   (((val) & 0x7) << 16)
++#define LTQ_ES_P0_VLAN_REG_DVPM_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_DVPM) >> 16) & 0x7)
++#define LTQ_ES_P0_VLAN_REG_DVPM_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_DVPM) | (((val) & 0x7) << 16))
++/* Port Priority (15:14) */
++#define LTQ_ES_P0_VLAN_REG_PP   (0x3 << 14)
++#define LTQ_ES_P0_VLAN_REG_PP_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_P0_VLAN_REG_PP_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_PP) >> 14) & 0x3)
++#define LTQ_ES_P0_VLAN_REG_PP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_PP) | (((val) & 0x3) << 14))
++/* Port Priority Enable (13) */
++#define LTQ_ES_P0_VLAN_REG_PPE   (0x1 << 13)
++#define LTQ_ES_P0_VLAN_REG_PPE_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_P0_VLAN_REG_PPE_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_PPE) >> 13) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_PPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_PPE) | (((val) & 0x1) << 13))
++/* Portbase VLAN tag member for Port 0 (12) */
++#define LTQ_ES_P0_VLAN_REG_PVTAGMP   (0x1 << 12)
++#define LTQ_ES_P0_VLAN_REG_PVTAGMP_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_P0_VLAN_REG_PVTAGMP_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_PVTAGMP) >> 12) & 0x1)
++#define LTQ_ES_P0_VLAN_REG_PVTAGMP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_PVTAGMP) | (((val) & 0x1) << 12))
++/* PVID (11:0) */
++#define LTQ_ES_P0_VLAN_REG_PVID   (0xfff)
++#define LTQ_ES_P0_VLAN_REG_PVID_VAL(val)   (((val) & 0xfff) << 0)
++#define LTQ_ES_P0_VLAN_REG_PVID_GET(val)   ((((val) & LTQ_ES_P0_VLAN_REG_PVID) >> 0) & 0xfff)
++#define LTQ_ES_P0_VLAN_REG_PVID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_VLAN_REG_PVID) | (((val) & 0xfff) << 0))
++
++/*******************************************************************************
++ * Port 0 Ingress Control Register
++ ******************************************************************************/
++
++/* Reserved  (31:13) */
++#define LTQ_ES_P0_INCTL_REG_RES   (0x7ffff << 13)
++#define LTQ_ES_P0_INCTL_REG_RES_GET(val)   ((((val) & LTQ_ES_P0_INCTL_REG_RES) >> 13) & 0x7ffff)
++/* Port 0 Ingress/Egress Timer Tick T selection (12:11) */
++#define LTQ_ES_P0_INCTL_REG_P0ITT   (0x3 << 11)
++#define LTQ_ES_P0_INCTL_REG_P0ITT_VAL(val)   (((val) & 0x3) << 11)
++#define LTQ_ES_P0_INCTL_REG_P0ITT_GET(val)   ((((val) & LTQ_ES_P0_INCTL_REG_P0ITT) >> 11) & 0x3)
++#define LTQ_ES_P0_INCTL_REG_P0ITT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_INCTL_REG_P0ITT) | (((val) & 0x3) << 11))
++/* Port 0 Igress Token R (10:0) */
++#define LTQ_ES_P0_INCTL_REG_P0ITR   (0x7ff)
++#define LTQ_ES_P0_INCTL_REG_P0ITR_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_P0_INCTL_REG_P0ITR_GET(val)   ((((val) & LTQ_ES_P0_INCTL_REG_P0ITR) >> 0) & 0x7ff)
++#define LTQ_ES_P0_INCTL_REG_P0ITR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_INCTL_REG_P0ITR) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Port 0 Egress Control for Strict Q32 Register
++ ******************************************************************************/
++
++/* Port 0 Egress Token R for Strict Priority Q3 (26:16) */
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR   (0x7ff << 16)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR_VAL(val)   (((val) & 0x7ff) << 16)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR_GET(val)   ((((val) & LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR) >> 16) & 0x7ff)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECS_Q32_REG_P0SPQ3TR) | (((val) & 0x7ff) << 16))
++/* Port 0 Egress Token R for Strict Priority Q2 (10:0) */
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR   (0x7ff)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR_GET(val)   ((((val) & LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR) >> 0) & 0x7ff)
++#define LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECS_Q32_REG_P0SPQ2TR) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Port 0 Egress Control for Strict Q10 Register
++ ******************************************************************************/
++
++/* Reserved  (31:27) */
++#define LTQ_ES_P0_ECS_Q10_REG_RES   (0x1f << 27)
++#define LTQ_ES_P0_ECS_Q10_REG_RES_GET(val)   ((((val) & LTQ_ES_P0_ECS_Q10_REG_RES) >> 27) & 0x1f)
++/* Port 0 Egress Token R for Strict Priority Q1 (26:16) */
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR   (0x7ff << 16)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR_VAL(val)   (((val) & 0x7ff) << 16)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR_GET(val)   ((((val) & LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR) >> 16) & 0x7ff)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECS_Q10_REG_P0SPQ1TR) | (((val) & 0x7ff) << 16))
++/* Port 0 Egress Token R for Strict Priority Q0 (10:0) */
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR   (0x7ff)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR_GET(val)   ((((val) & LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR) >> 0) & 0x7ff)
++#define LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECS_Q10_REG_P0SPQ0TR) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Port 0 Egress Control for WFQ Q32 Register
++ ******************************************************************************/
++
++/* Reserved  (31:27) */
++#define LTQ_ES_P0_ECW_Q32_REG_RES   (0x1f << 27)
++#define LTQ_ES_P0_ECW_Q32_REG_RES_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q32_REG_RES) >> 27) & 0x1f)
++/* Port 0 Egress Token R for WFQ Q3 (26:16) */
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR   (0x7ff << 16)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR_VAL(val)   (((val) & 0x7ff) << 16)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR) >> 16) & 0x7ff)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECW_Q32_REG_P0WQ3TR) | (((val) & 0x7ff) << 16))
++/* Port 0 Egress Token R for WFQ Q2 (10:0) */
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR   (0x7ff)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR) >> 0) & 0x7ff)
++#define LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECW_Q32_REG_P0WQ2TR) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Port 0 Egress Control for WFQ Q10 Register
++ ******************************************************************************/
++
++/* Reserved  (31:27) */
++#define LTQ_ES_P0_ECW_Q10_REG_RES   (0x1f << 27)
++#define LTQ_ES_P0_ECW_Q10_REG_RES_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q10_REG_RES) >> 27) & 0x1f)
++/* Port 0 Egress Token R for WFQ Q1 (26:16) */
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR   (0x7ff << 16)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR_VAL(val)   (((val) & 0x7ff) << 16)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR) >> 16) & 0x7ff)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECW_Q10_REG_P0WQ1TR) | (((val) & 0x7ff) << 16))
++/* Port 0 Egress Token R for WFQ Q0 (10:0) */
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR   (0x7ff)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR_GET(val)   ((((val) & LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR) >> 0) & 0x7ff)
++#define LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_P0_ECW_Q10_REG_P0WQ0TR) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Interrupt Enable Register
++ ******************************************************************************/
++
++/* Reserved (31:8) */
++#define LTQ_ES_INT_ENA_REG_RES   (0xffffff << 8)
++#define LTQ_ES_INT_ENA_REG_RES_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_RES) >> 8) & 0xffffff)
++/* Data Buffer is Full Interrupt Enable (7) */
++#define LTQ_ES_INT_ENA_REG_DBFIE   (0x1 << 7)
++#define LTQ_ES_INT_ENA_REG_DBFIE_VAL(val)   (((val) & 0x1) << 7)
++#define LTQ_ES_INT_ENA_REG_DBFIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_DBFIE) >> 7) & 0x1)
++#define LTQ_ES_INT_ENA_REG_DBFIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_DBFIE) | (((val) & 0x1) << 7))
++/* Data Buffer is nearly Full Interrupt Enable (6) */
++#define LTQ_ES_INT_ENA_REG_DBNFIE   (0x1 << 6)
++#define LTQ_ES_INT_ENA_REG_DBNFIE_VAL(val)   (((val) & 0x1) << 6)
++#define LTQ_ES_INT_ENA_REG_DBNFIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_DBNFIE) >> 6) & 0x1)
++#define LTQ_ES_INT_ENA_REG_DBNFIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_DBNFIE) | (((val) & 0x1) << 6))
++/* Learning Table Full Interrupt Enable (5) */
++#define LTQ_ES_INT_ENA_REG_LTFIE   (0x1 << 5)
++#define LTQ_ES_INT_ENA_REG_LTFIE_VAL(val)   (((val) & 0x1) << 5)
++#define LTQ_ES_INT_ENA_REG_LTFIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_LTFIE) >> 5) & 0x1)
++#define LTQ_ES_INT_ENA_REG_LTFIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_LTFIE) | (((val) & 0x1) << 5))
++/* Leaning Table Access Done Interrupt Enable (4) */
++#define LTQ_ES_INT_ENA_REG_LTADIE   (0x1 << 4)
++#define LTQ_ES_INT_ENA_REG_LTADIE_VAL(val)   (((val) & 0x1) << 4)
++#define LTQ_ES_INT_ENA_REG_LTADIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_LTADIE) >> 4) & 0x1)
++#define LTQ_ES_INT_ENA_REG_LTADIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_LTADIE) | (((val) & 0x1) << 4))
++/* Port Security Violation Interrupt Enable (3:1) */
++#define LTQ_ES_INT_ENA_REG_PSVIE   (0x7 << 1)
++#define LTQ_ES_INT_ENA_REG_PSVIE_VAL(val)   (((val) & 0x7) << 1)
++#define LTQ_ES_INT_ENA_REG_PSVIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_PSVIE) >> 1) & 0x7)
++#define LTQ_ES_INT_ENA_REG_PSVIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_PSVIE) | (((val) & 0x7) << 1))
++/* Port Status Change Interrupt Enable (0) */
++#define LTQ_ES_INT_ENA_REG_PSCIE   (0x1)
++#define LTQ_ES_INT_ENA_REG_PSCIE_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_ES_INT_ENA_REG_PSCIE_GET(val)   ((((val) & LTQ_ES_INT_ENA_REG_PSCIE) >> 0) & 0x1)
++#define LTQ_ES_INT_ENA_REG_PSCIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_INT_ENA_REG_PSCIE) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * Interrupt Status Register
++ ******************************************************************************/
++
++/* Reserved (31:8) */
++#define LTQ_ES_INT_ST_REG_RES   (0xffffff << 8)
++#define LTQ_ES_INT_ST_REG_RES_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_RES) >> 8) & 0xffffff)
++/* Data Buffer is Full (7) */
++#define LTQ_ES_INT_ST_REG_DBF   (0x1 << 7)
++#define LTQ_ES_INT_ST_REG_DBF_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_DBF) >> 7) & 0x1)
++/* Data Buffer is nearly Full (6) */
++#define LTQ_ES_INT_ST_REG_DBNF   (0x1 << 6)
++#define LTQ_ES_INT_ST_REG_DBNF_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_DBNF) >> 6) & 0x1)
++/* Learning Table Full (5) */
++#define LTQ_ES_INT_ST_REG_LTF   (0x1 << 5)
++#define LTQ_ES_INT_ST_REG_LTF_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_LTF) >> 5) & 0x1)
++/* Leaning Table Access Done (4) */
++#define LTQ_ES_INT_ST_REG_LTAD   (0x1 << 4)
++#define LTQ_ES_INT_ST_REG_LTAD_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_LTAD) >> 4) & 0x1)
++/* Port Security Violation (3:1) */
++#define LTQ_ES_INT_ST_REG_PSV   (0x7 << 1)
++#define LTQ_ES_INT_ST_REG_PSV_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_PSV) >> 1) & 0x7)
++/* Port Status Change (0) */
++#define LTQ_ES_INT_ST_REG_PSC   (0x1)
++#define LTQ_ES_INT_ST_REG_PSC_GET(val)   ((((val) & LTQ_ES_INT_ST_REG_PSC) >> 0) & 0x1)
++
++/*******************************************************************************
++ * Switch Global Control Register 0
++ ******************************************************************************/
++
++/* Switch Enable (31) */
++#define LTQ_ES_SW_GCTL0_REG_SE   (0x1 << 31)
++#define LTQ_ES_SW_GCTL0_REG_SE_VAL(val)   (((val) & 0x1) << 31)
++#define LTQ_ES_SW_GCTL0_REG_SE_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_SE) >> 31) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_SE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_SE) | (((val) & 0x1) << 31))
++/* CRC Check Disable (30) */
++#define LTQ_ES_SW_GCTL0_REG_ICRCCD   (0x1 << 30)
++#define LTQ_ES_SW_GCTL0_REG_ICRCCD_VAL(val)   (((val) & 0x1) << 30)
++#define LTQ_ES_SW_GCTL0_REG_ICRCCD_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_ICRCCD) >> 30) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_ICRCCD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_ICRCCD) | (((val) & 0x1) << 30))
++/* Replace VID0 (28) */
++#define LTQ_ES_SW_GCTL0_REG_RVID0   (0x1 << 28)
++#define LTQ_ES_SW_GCTL0_REG_RVID0_VAL(val)   (((val) & 0x1) << 28)
++#define LTQ_ES_SW_GCTL0_REG_RVID0_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_RVID0) >> 28) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_RVID0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_RVID0) | (((val) & 0x1) << 28))
++/* Replace VID1 (27) */
++#define LTQ_ES_SW_GCTL0_REG_RVID1   (0x1 << 27)
++#define LTQ_ES_SW_GCTL0_REG_RVID1_VAL(val)   (((val) & 0x1) << 27)
++#define LTQ_ES_SW_GCTL0_REG_RVID1_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_RVID1) >> 27) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_RVID1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_RVID1) | (((val) & 0x1) << 27))
++/* Replace VIDFFF (26) */
++#define LTQ_ES_SW_GCTL0_REG_RVIDFFF   (0x1 << 26)
++#define LTQ_ES_SW_GCTL0_REG_RVIDFFF_VAL(val)   (((val) & 0x1) << 26)
++#define LTQ_ES_SW_GCTL0_REG_RVIDFFF_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_RVIDFFF) >> 26) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_RVIDFFF_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_RVIDFFF) | (((val) & 0x1) << 26))
++/* Priority Change Rule (25) */
++#define LTQ_ES_SW_GCTL0_REG_PCR   (0x1 << 25)
++#define LTQ_ES_SW_GCTL0_REG_PCR_VAL(val)   (((val) & 0x1) << 25)
++#define LTQ_ES_SW_GCTL0_REG_PCR_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_PCR) >> 25) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_PCR_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_PCR) | (((val) & 0x1) << 25))
++/* Priority Change Enable (24) */
++#define LTQ_ES_SW_GCTL0_REG_PCE   (0x1 << 24)
++#define LTQ_ES_SW_GCTL0_REG_PCE_VAL(val)   (((val) & 0x1) << 24)
++#define LTQ_ES_SW_GCTL0_REG_PCE_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_PCE) >> 24) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_PCE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_PCE) | (((val) & 0x1) << 24))
++/* Transmit Short IPG Enable (23) */
++#define LTQ_ES_SW_GCTL0_REG_TSIPGE   (0x1 << 23)
++#define LTQ_ES_SW_GCTL0_REG_TSIPGE_VAL(val)   (((val) & 0x1) << 23)
++#define LTQ_ES_SW_GCTL0_REG_TSIPGE_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_TSIPGE) >> 23) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_TSIPGE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_TSIPGE) | (((val) & 0x1) << 23))
++/* PHY Base Address (22) */
++#define LTQ_ES_SW_GCTL0_REG_PHYBA   (0x1 << 22)
++#define LTQ_ES_SW_GCTL0_REG_PHYBA_VAL(val)   (((val) & 0x1) << 22)
++#define LTQ_ES_SW_GCTL0_REG_PHYBA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_PHYBA) >> 22) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_PHYBA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_PHYBA) | (((val) & 0x1) << 22))
++/* Drop Packet When Excessive Collision Happen (21) */
++#define LTQ_ES_SW_GCTL0_REG_DPWECH   (0x1 << 21)
++#define LTQ_ES_SW_GCTL0_REG_DPWECH_VAL(val)   (((val) & 0x1) << 21)
++#define LTQ_ES_SW_GCTL0_REG_DPWECH_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_DPWECH) >> 21) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_DPWECH_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_DPWECH) | (((val) & 0x1) << 21))
++/* Aging Timer Select (20:18) */
++#define LTQ_ES_SW_GCTL0_REG_ATS   (0x7 << 18)
++#define LTQ_ES_SW_GCTL0_REG_ATS_VAL(val)   (((val) & 0x7) << 18)
++#define LTQ_ES_SW_GCTL0_REG_ATS_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_ATS) >> 18) & 0x7)
++#define LTQ_ES_SW_GCTL0_REG_ATS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_ATS) | (((val) & 0x7) << 18))
++/* Mirror CRC Also (17) */
++#define LTQ_ES_SW_GCTL0_REG_MCA   (0x1 << 17)
++#define LTQ_ES_SW_GCTL0_REG_MCA_VAL(val)   (((val) & 0x1) << 17)
++#define LTQ_ES_SW_GCTL0_REG_MCA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MCA) >> 17) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_MCA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MCA) | (((val) & 0x1) << 17))
++/* Mirror RXER Also (16) */
++#define LTQ_ES_SW_GCTL0_REG_MRA   (0x1 << 16)
++#define LTQ_ES_SW_GCTL0_REG_MRA_VAL(val)   (((val) & 0x1) << 16)
++#define LTQ_ES_SW_GCTL0_REG_MRA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MRA) >> 16) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_MRA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MRA) | (((val) & 0x1) << 16))
++/* Mirror PAUSE Also (15) */
++#define LTQ_ES_SW_GCTL0_REG_MPA   (0x1 << 15)
++#define LTQ_ES_SW_GCTL0_REG_MPA_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_SW_GCTL0_REG_MPA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MPA) >> 15) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_MPA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MPA) | (((val) & 0x1) << 15))
++/* Mirror Long Also (14) */
++#define LTQ_ES_SW_GCTL0_REG_MLA   (0x1 << 14)
++#define LTQ_ES_SW_GCTL0_REG_MLA_VAL(val)   (((val) & 0x1) << 14)
++#define LTQ_ES_SW_GCTL0_REG_MLA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MLA) >> 14) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_MLA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MLA) | (((val) & 0x1) << 14))
++/* Mirror Short Also (13) */
++#define LTQ_ES_SW_GCTL0_REG_MSA   (0x1 << 13)
++#define LTQ_ES_SW_GCTL0_REG_MSA_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_SW_GCTL0_REG_MSA_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MSA) >> 13) & 0x1)
++#define LTQ_ES_SW_GCTL0_REG_MSA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MSA) | (((val) & 0x1) << 13))
++/* Sniffer port number (12:11) */
++#define LTQ_ES_SW_GCTL0_REG_SNIFFPN   (0x3 << 11)
++#define LTQ_ES_SW_GCTL0_REG_SNIFFPN_VAL(val)   (((val) & 0x3) << 11)
++#define LTQ_ES_SW_GCTL0_REG_SNIFFPN_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_SNIFFPN) >> 11) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_SNIFFPN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_SNIFFPN) | (((val) & 0x3) << 11))
++/* Max Packet Length (MAXPKTLEN) (9:8) */
++#define LTQ_ES_SW_GCTL0_REG_MPL   (0x3 << 8)
++#define LTQ_ES_SW_GCTL0_REG_MPL_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_SW_GCTL0_REG_MPL_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_MPL) >> 8) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_MPL_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_MPL) | (((val) & 0x3) << 8))
++/* Discard Mode (Drop scheme for Packets Classified as Q3) (7:6) */
++#define LTQ_ES_SW_GCTL0_REG_DMQ3   (0x3 << 6)
++#define LTQ_ES_SW_GCTL0_REG_DMQ3_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_SW_GCTL0_REG_DMQ3_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_DMQ3) >> 6) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_DMQ3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_DMQ3) | (((val) & 0x3) << 6))
++/* Discard Mode (Drop scheme for Packets Classified as Q2) (5:4) */
++#define LTQ_ES_SW_GCTL0_REG_DMQ2   (0x3 << 4)
++#define LTQ_ES_SW_GCTL0_REG_DMQ2_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_SW_GCTL0_REG_DMQ2_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_DMQ2) >> 4) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_DMQ2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_DMQ2) | (((val) & 0x3) << 4))
++/* Discard Mode (Drop scheme for Packets Classified as Q1) (3:2) */
++#define LTQ_ES_SW_GCTL0_REG_DMQ1   (0x3 << 2)
++#define LTQ_ES_SW_GCTL0_REG_DMQ1_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_SW_GCTL0_REG_DMQ1_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_DMQ1) >> 2) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_DMQ1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_DMQ1) | (((val) & 0x3) << 2))
++/* Discard Mode (Drop scheme for Packets Classified as Q0) (1:0) */
++#define LTQ_ES_SW_GCTL0_REG_DMQ0   (0x3)
++#define LTQ_ES_SW_GCTL0_REG_DMQ0_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_SW_GCTL0_REG_DMQ0_GET(val)   ((((val) & LTQ_ES_SW_GCTL0_REG_DMQ0) >> 0) & 0x3)
++#define LTQ_ES_SW_GCTL0_REG_DMQ0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL0_REG_DMQ0) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * Switch Global Control Register 1
++ ******************************************************************************/
++
++/* BIST Done (27) */
++#define LTQ_ES_SW_GCTL1_REG_BISTDN   (0x1 << 27)
++#define LTQ_ES_SW_GCTL1_REG_BISTDN_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_BISTDN) >> 27) & 0x1)
++/* Enable drop scheme of TX and RX (26) */
++#define LTQ_ES_SW_GCTL1_REG_EDSTX   (0x1 << 26)
++#define LTQ_ES_SW_GCTL1_REG_EDSTX_VAL(val)   (((val) & 0x1) << 26)
++#define LTQ_ES_SW_GCTL1_REG_EDSTX_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_EDSTX) >> 26) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_EDSTX_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_EDSTX) | (((val) & 0x1) << 26))
++/* Congestion threshold for TX queue (25:24) */
++#define LTQ_ES_SW_GCTL1_REG_CTTX   (0x3 << 24)
++#define LTQ_ES_SW_GCTL1_REG_CTTX_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_SW_GCTL1_REG_CTTX_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_CTTX) >> 24) & 0x3)
++#define LTQ_ES_SW_GCTL1_REG_CTTX_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_CTTX) | (((val) & 0x3) << 24))
++/* Input Jam Threshold (23:21) */
++#define LTQ_ES_SW_GCTL1_REG_IJT   (0x7 << 21)
++#define LTQ_ES_SW_GCTL1_REG_IJT_VAL(val)   (((val) & 0x7) << 21)
++#define LTQ_ES_SW_GCTL1_REG_IJT_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_IJT) >> 21) & 0x7)
++#define LTQ_ES_SW_GCTL1_REG_IJT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_IJT) | (((val) & 0x7) << 21))
++/* Do not Identify VLAN after SNAP (20) */
++#define LTQ_ES_SW_GCTL1_REG_DIVS   (0x1 << 20)
++#define LTQ_ES_SW_GCTL1_REG_DIVS_VAL(val)   (((val) & 0x1) << 20)
++#define LTQ_ES_SW_GCTL1_REG_DIVS_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DIVS) >> 20) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DIVS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DIVS) | (((val) & 0x1) << 20))
++/* Do not Identify IPV6 in PPPOE (19) */
++#define LTQ_ES_SW_GCTL1_REG_DII6P   (0x1 << 19)
++#define LTQ_ES_SW_GCTL1_REG_DII6P_VAL(val)   (((val) & 0x1) << 19)
++#define LTQ_ES_SW_GCTL1_REG_DII6P_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DII6P) >> 19) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DII6P_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DII6P) | (((val) & 0x1) << 19))
++/* Do not Identify IP in PPPOE after SNAP (18) */
++#define LTQ_ES_SW_GCTL1_REG_DIIPS   (0x1 << 18)
++#define LTQ_ES_SW_GCTL1_REG_DIIPS_VAL(val)   (((val) & 0x1) << 18)
++#define LTQ_ES_SW_GCTL1_REG_DIIPS_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DIIPS) >> 18) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DIIPS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DIIPS) | (((val) & 0x1) << 18))
++/* Do not Identify Ether-Type = 0x0800, IP VER = 6 as IPV6 packets (17) */
++#define LTQ_ES_SW_GCTL1_REG_DIE   (0x1 << 17)
++#define LTQ_ES_SW_GCTL1_REG_DIE_VAL(val)   (((val) & 0x1) << 17)
++#define LTQ_ES_SW_GCTL1_REG_DIE_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DIE) >> 17) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DIE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DIE) | (((val) & 0x1) << 17))
++/* Do not Identify IP in PPPOE (16) */
++#define LTQ_ES_SW_GCTL1_REG_DIIP   (0x1 << 16)
++#define LTQ_ES_SW_GCTL1_REG_DIIP_VAL(val)   (((val) & 0x1) << 16)
++#define LTQ_ES_SW_GCTL1_REG_DIIP_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DIIP) >> 16) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DIIP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DIIP) | (((val) & 0x1) << 16))
++/* Do not Identify SNAP (15) */
++#define LTQ_ES_SW_GCTL1_REG_DIS   (0x1 << 15)
++#define LTQ_ES_SW_GCTL1_REG_DIS_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_SW_GCTL1_REG_DIS_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_DIS) >> 15) & 0x1)
++#define LTQ_ES_SW_GCTL1_REG_DIS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_DIS) | (((val) & 0x1) << 15))
++/* Unicast Portmap (14:12) */
++#define LTQ_ES_SW_GCTL1_REG_UP   (0x7 << 12)
++#define LTQ_ES_SW_GCTL1_REG_UP_VAL(val)   (((val) & 0x7) << 12)
++#define LTQ_ES_SW_GCTL1_REG_UP_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_UP) >> 12) & 0x7)
++#define LTQ_ES_SW_GCTL1_REG_UP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_UP) | (((val) & 0x7) << 12))
++/* Broadcast Portmap (10:8) */
++#define LTQ_ES_SW_GCTL1_REG_BP   (0x7 << 8)
++#define LTQ_ES_SW_GCTL1_REG_BP_VAL(val)   (((val) & 0x7) << 8)
++#define LTQ_ES_SW_GCTL1_REG_BP_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_BP) >> 8) & 0x7)
++#define LTQ_ES_SW_GCTL1_REG_BP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_BP) | (((val) & 0x7) << 8))
++/* Multicast Portmap (6:4) */
++#define LTQ_ES_SW_GCTL1_REG_MP   (0x7 << 4)
++#define LTQ_ES_SW_GCTL1_REG_MP_VAL(val)   (((val) & 0x7) << 4)
++#define LTQ_ES_SW_GCTL1_REG_MP_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_MP) >> 4) & 0x7)
++#define LTQ_ES_SW_GCTL1_REG_MP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_MP) | (((val) & 0x7) << 4))
++/* Reserve Portmap (2:0) */
++#define LTQ_ES_SW_GCTL1_REG_RP   (0x7)
++#define LTQ_ES_SW_GCTL1_REG_RP_VAL(val)   (((val) & 0x7) << 0)
++#define LTQ_ES_SW_GCTL1_REG_RP_GET(val)   ((((val) & LTQ_ES_SW_GCTL1_REG_RP) >> 0) & 0x7)
++#define LTQ_ES_SW_GCTL1_REG_RP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_SW_GCTL1_REG_RP) | (((val) & 0x7) << 0))
++
++/*******************************************************************************
++ * ARP/RARP Register
++ ******************************************************************************/
++
++/* MAC Control Action (15:14) */
++#define LTQ_ES_ARP_REG_MACA   (0x3 << 14)
++#define LTQ_ES_ARP_REG_MACA_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_ARP_REG_MACA_GET(val)   ((((val) & LTQ_ES_ARP_REG_MACA) >> 14) & 0x3)
++#define LTQ_ES_ARP_REG_MACA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_MACA) | (((val) & 0x3) << 14))
++/* Unicast packet Treated as Cross_VLAN packet (13) */
++#define LTQ_ES_ARP_REG_UPT   (0x1 << 13)
++#define LTQ_ES_ARP_REG_UPT_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_ARP_REG_UPT_GET(val)   ((((val) & LTQ_ES_ARP_REG_UPT) >> 13) & 0x1)
++#define LTQ_ES_ARP_REG_UPT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_UPT) | (((val) & 0x1) << 13))
++/* RARP Packet Treated as Cross_VLAN Packet (12) */
++#define LTQ_ES_ARP_REG_RPT   (0x1 << 12)
++#define LTQ_ES_ARP_REG_RPT_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_ARP_REG_RPT_GET(val)   ((((val) & LTQ_ES_ARP_REG_RPT) >> 12) & 0x1)
++#define LTQ_ES_ARP_REG_RPT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RPT) | (((val) & 0x1) << 12))
++/* RARP/ARP Packet Action (11:10) */
++#define LTQ_ES_ARP_REG_RAPA   (0x3 << 10)
++#define LTQ_ES_ARP_REG_RAPA_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_ARP_REG_RAPA_GET(val)   ((((val) & LTQ_ES_ARP_REG_RAPA) >> 10) & 0x3)
++#define LTQ_ES_ARP_REG_RAPA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RAPA) | (((val) & 0x3) << 10))
++/* RARP/ARP Packet Priority Enable (9) */
++#define LTQ_ES_ARP_REG_RAPPE   (0x1 << 9)
++#define LTQ_ES_ARP_REG_RAPPE_VAL(val)   (((val) & 0x1) << 9)
++#define LTQ_ES_ARP_REG_RAPPE_GET(val)   ((((val) & LTQ_ES_ARP_REG_RAPPE) >> 9) & 0x1)
++#define LTQ_ES_ARP_REG_RAPPE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RAPPE) | (((val) & 0x1) << 9))
++/* RARP/ARP Packet Priority (8:7) */
++#define LTQ_ES_ARP_REG_RAPP   (0x3 << 7)
++#define LTQ_ES_ARP_REG_RAPP_VAL(val)   (((val) & 0x3) << 7)
++#define LTQ_ES_ARP_REG_RAPP_GET(val)   ((((val) & LTQ_ES_ARP_REG_RAPP) >> 7) & 0x3)
++#define LTQ_ES_ARP_REG_RAPP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RAPP) | (((val) & 0x3) << 7))
++/* RARP/ARP Packet Output Tag Handle (6:5) */
++#define LTQ_ES_ARP_REG_RAPOTH   (0x3 << 5)
++#define LTQ_ES_ARP_REG_RAPOTH_VAL(val)   (((val) & 0x3) << 5)
++#define LTQ_ES_ARP_REG_RAPOTH_GET(val)   ((((val) & LTQ_ES_ARP_REG_RAPOTH) >> 5) & 0x3)
++#define LTQ_ES_ARP_REG_RAPOTH_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RAPOTH) | (((val) & 0x3) << 5))
++/* ARP Packet Treated as Cross _ VLAN Packet (4) */
++#define LTQ_ES_ARP_REG_APT   (0x1 << 4)
++#define LTQ_ES_ARP_REG_APT_VAL(val)   (((val) & 0x1) << 4)
++#define LTQ_ES_ARP_REG_APT_GET(val)   ((((val) & LTQ_ES_ARP_REG_APT) >> 4) & 0x1)
++#define LTQ_ES_ARP_REG_APT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_APT) | (((val) & 0x1) << 4))
++/* RARP/ARP Packet Treated as Management Packet (3) */
++#define LTQ_ES_ARP_REG_RAPTM   (0x1 << 3)
++#define LTQ_ES_ARP_REG_RAPTM_VAL(val)   (((val) & 0x1) << 3)
++#define LTQ_ES_ARP_REG_RAPTM_GET(val)   ((((val) & LTQ_ES_ARP_REG_RAPTM) >> 3) & 0x1)
++#define LTQ_ES_ARP_REG_RAPTM_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_RAPTM) | (((val) & 0x1) << 3))
++/* RARP/ARP Packet Treated as Span Packet (2) */
++#define LTQ_ES_ARP_REG_TAPTS   (0x1 << 2)
++#define LTQ_ES_ARP_REG_TAPTS_VAL(val)   (((val) & 0x1) << 2)
++#define LTQ_ES_ARP_REG_TAPTS_GET(val)   ((((val) & LTQ_ES_ARP_REG_TAPTS) >> 2) & 0x1)
++#define LTQ_ES_ARP_REG_TAPTS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_TAPTS) | (((val) & 0x1) << 2))
++/* Trap ARP Packet (1) */
++#define LTQ_ES_ARP_REG_TAP   (0x1 << 1)
++#define LTQ_ES_ARP_REG_TAP_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_ES_ARP_REG_TAP_GET(val)   ((((val) & LTQ_ES_ARP_REG_TAP) >> 1) & 0x1)
++#define LTQ_ES_ARP_REG_TAP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_TAP) | (((val) & 0x1) << 1))
++/* Trap RARP Packet (0) */
++#define LTQ_ES_ARP_REG_TRP   (0x1)
++#define LTQ_ES_ARP_REG_TRP_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_ES_ARP_REG_TRP_GET(val)   ((((val) & LTQ_ES_ARP_REG_TRP) >> 0) & 0x1)
++#define LTQ_ES_ARP_REG_TRP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ARP_REG_TRP) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * Storm control Register
++ ******************************************************************************/
++
++/* Reserved (31:29) */
++#define LTQ_ES_STRM_CTL_REG_RES   (0x7 << 29)
++#define LTQ_ES_STRM_CTL_REG_RES_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_RES) >> 29) & 0x7)
++/* 10M Threshold (28:16) */
++#define LTQ_ES_STRM_CTL_REG_STORM_10_TH   (0x1fff << 16)
++#define LTQ_ES_STRM_CTL_REG_STORM_10_TH_VAL(val)   (((val) & 0x1fff) << 16)
++#define LTQ_ES_STRM_CTL_REG_STORM_10_TH_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_STORM_10_TH) >> 16) & 0x1fff)
++#define LTQ_ES_STRM_CTL_REG_STORM_10_TH_SET(reg,val) (reg) = ((reg & ~LTQ_ES_STRM_CTL_REG_STORM_10_TH) | (((val) & 0x1fff) << 16))
++/* Storm Enable for Broadcast Packets (15) */
++#define LTQ_ES_STRM_CTL_REG_STORM_B   (0x1 << 15)
++#define LTQ_ES_STRM_CTL_REG_STORM_B_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_STRM_CTL_REG_STORM_B_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_STORM_B) >> 15) & 0x1)
++#define LTQ_ES_STRM_CTL_REG_STORM_B_SET(reg,val) (reg) = ((reg & ~LTQ_ES_STRM_CTL_REG_STORM_B) | (((val) & 0x1) << 15))
++/* Storm Enable for Multicast Packets (14) */
++#define LTQ_ES_STRM_CTL_REG_STORM_M   (0x1 << 14)
++#define LTQ_ES_STRM_CTL_REG_STORM_M_VAL(val)   (((val) & 0x1) << 14)
++#define LTQ_ES_STRM_CTL_REG_STORM_M_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_STORM_M) >> 14) & 0x1)
++#define LTQ_ES_STRM_CTL_REG_STORM_M_SET(reg,val) (reg) = ((reg & ~LTQ_ES_STRM_CTL_REG_STORM_M) | (((val) & 0x1) << 14))
++/* Storm Enable for Un-learned Unicast Packets (13) */
++#define LTQ_ES_STRM_CTL_REG_STORM_U   (0x1 << 13)
++#define LTQ_ES_STRM_CTL_REG_STORM_U_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_STRM_CTL_REG_STORM_U_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_STORM_U) >> 13) & 0x1)
++#define LTQ_ES_STRM_CTL_REG_STORM_U_SET(reg,val) (reg) = ((reg & ~LTQ_ES_STRM_CTL_REG_STORM_U) | (((val) & 0x1) << 13))
++/* 100M Threshold (12:0) */
++#define LTQ_ES_STRM_CTL_REG_STORM_100_TH   (0x1fff)
++#define LTQ_ES_STRM_CTL_REG_STORM_100_TH_VAL(val)   (((val) & 0x1fff) << 0)
++#define LTQ_ES_STRM_CTL_REG_STORM_100_TH_GET(val)   ((((val) & LTQ_ES_STRM_CTL_REG_STORM_100_TH) >> 0) & 0x1fff)
++#define LTQ_ES_STRM_CTL_REG_STORM_100_TH_SET(reg,val) (reg) = ((reg & ~LTQ_ES_STRM_CTL_REG_STORM_100_TH) | (((val) & 0x1fff) << 0))
++
++/*******************************************************************************
++ * RGMII/GMII Port Control Register
++ ******************************************************************************/
++
++/* Management Clock Select (31:24) */
++#define LTQ_ES_RGMII_CTL_REG_MCS   (0xff << 24)
++#define LTQ_ES_RGMII_CTL_REG_MCS_VAL(val)   (((val) & 0xff) << 24)
++#define LTQ_ES_RGMII_CTL_REG_MCS_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_MCS) >> 24) & 0xff)
++#define LTQ_ES_RGMII_CTL_REG_MCS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_MCS) | (((val) & 0xff) << 24))
++/* Interface Selection (19:18) */
++#define LTQ_ES_RGMII_CTL_REG_IS   (0x3 << 18)
++#define LTQ_ES_RGMII_CTL_REG_IS_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_IS) >> 18) & 0x3)
++/* Port 1 RGMII Rx Clock Delay (17:16) */
++#define LTQ_ES_RGMII_CTL_REG_P1RDLY   (0x3 << 16)
++#define LTQ_ES_RGMII_CTL_REG_P1RDLY_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_RGMII_CTL_REG_P1RDLY_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P1RDLY) >> 16) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P1RDLY_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P1RDLY) | (((val) & 0x3) << 16))
++/* Port 1 RGMII Tx Clock Delay (15:14) */
++#define LTQ_ES_RGMII_CTL_REG_P1TDLY   (0x3 << 14)
++#define LTQ_ES_RGMII_CTL_REG_P1TDLY_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_RGMII_CTL_REG_P1TDLY_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P1TDLY) >> 14) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P1TDLY_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P1TDLY) | (((val) & 0x3) << 14))
++/* Port 1 Speed (13:12) */
++#define LTQ_ES_RGMII_CTL_REG_P1SPD   (0x3 << 12)
++#define LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_RGMII_CTL_REG_P1SPD_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P1SPD) >> 12) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P1SPD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P1SPD) | (((val) & 0x3) << 12))
++/* Port 1 Duplex mode (11) */
++#define LTQ_ES_RGMII_CTL_REG_P1DUP   (0x1 << 11)
++#define LTQ_ES_RGMII_CTL_REG_P1DUP_VAL(val)   (((val) & 0x1) << 11)
++#define LTQ_ES_RGMII_CTL_REG_P1DUP_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P1DUP) >> 11) & 0x1)
++#define LTQ_ES_RGMII_CTL_REG_P1DUP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P1DUP) | (((val) & 0x1) << 11))
++/* Port 1 Flow Control Enable (10) */
++#define LTQ_ES_RGMII_CTL_REG_P1FCE   (0x1 << 10)
++#define LTQ_ES_RGMII_CTL_REG_P1FCE_VAL(val)   (((val) & 0x1) << 10)
++#define LTQ_ES_RGMII_CTL_REG_P1FCE_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P1FCE) >> 10) & 0x1)
++#define LTQ_ES_RGMII_CTL_REG_P1FCE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P1FCE) | (((val) & 0x1) << 10))
++/* Port 0 RGMII Rx Clock Delay (7:6) */
++#define LTQ_ES_RGMII_CTL_REG_P0RDLY   (0x3 << 6)
++#define LTQ_ES_RGMII_CTL_REG_P0RDLY_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_RGMII_CTL_REG_P0RDLY_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P0RDLY) >> 6) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P0RDLY_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P0RDLY) | (((val) & 0x3) << 6))
++/* Port 0 RGMII Tx Clock Delay (5:4) */
++#define LTQ_ES_RGMII_CTL_REG_P0TDLY   (0x3 << 4)
++#define LTQ_ES_RGMII_CTL_REG_P0TDLY_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_RGMII_CTL_REG_P0TDLY_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P0TDLY) >> 4) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P0TDLY_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P0TDLY) | (((val) & 0x3) << 4))
++/* Port 0 Speed (3:2) */
++#define LTQ_ES_RGMII_CTL_REG_P0SPD   (0x3 << 2)
++#define LTQ_ES_RGMII_CTL_REG_P0SPD_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_RGMII_CTL_REG_P0SPD_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P0SPD) >> 2) & 0x3)
++#define LTQ_ES_RGMII_CTL_REG_P0SPD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P0SPD) | (((val) & 0x3) << 2))
++/* Port 0 Duplex mode (1) */
++#define LTQ_ES_RGMII_CTL_REG_P0DUP   (0x1 << 1)
++#define LTQ_ES_RGMII_CTL_REG_P0DUP_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_ES_RGMII_CTL_REG_P0DUP_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P0DUP) >> 1) & 0x1)
++#define LTQ_ES_RGMII_CTL_REG_P0DUP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P0DUP) | (((val) & 0x1) << 1))
++/* Port 0 Flow Control Enable (0) */
++#define LTQ_ES_RGMII_CTL_REG_P0FCE   (0x1)
++#define LTQ_ES_RGMII_CTL_REG_P0FCE_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_ES_RGMII_CTL_REG_P0FCE_GET(val)   ((((val) & LTQ_ES_RGMII_CTL_REG_P0FCE) >> 0) & 0x1)
++#define LTQ_ES_RGMII_CTL_REG_P0FCE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RGMII_CTL_REG_P0FCE) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * 802.1p Priority Map Register
++ ******************************************************************************/
++
++/* Priority Queue 7 (15:14) */
++#define LTQ_ES_PRT_1P_REG_1PPQ7   (0x3 << 14)
++#define LTQ_ES_PRT_1P_REG_1PPQ7_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_PRT_1P_REG_1PPQ7_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ7) >> 14) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ7_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ7) | (((val) & 0x3) << 14))
++/* Priority Queue 6 (13:12) */
++#define LTQ_ES_PRT_1P_REG_1PPQ6   (0x3 << 12)
++#define LTQ_ES_PRT_1P_REG_1PPQ6_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_PRT_1P_REG_1PPQ6_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ6) >> 12) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ6_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ6) | (((val) & 0x3) << 12))
++/* Priority Queue 5 (11:10) */
++#define LTQ_ES_PRT_1P_REG_1PPQ5   (0x3 << 10)
++#define LTQ_ES_PRT_1P_REG_1PPQ5_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_PRT_1P_REG_1PPQ5_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ5) >> 10) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ5_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ5) | (((val) & 0x3) << 10))
++/* Priority Queue 4 (9:8) */
++#define LTQ_ES_PRT_1P_REG_1PPQ4   (0x3 << 8)
++#define LTQ_ES_PRT_1P_REG_1PPQ4_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_PRT_1P_REG_1PPQ4_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ4) >> 8) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ4_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ4) | (((val) & 0x3) << 8))
++/* Priority Queue 3 (7:6) */
++#define LTQ_ES_PRT_1P_REG_1PPQ3   (0x3 << 6)
++#define LTQ_ES_PRT_1P_REG_1PPQ3_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_PRT_1P_REG_1PPQ3_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ3) >> 6) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ3) | (((val) & 0x3) << 6))
++/* Priority Queue 2 (5:4) */
++#define LTQ_ES_PRT_1P_REG_1PPQ2   (0x3 << 4)
++#define LTQ_ES_PRT_1P_REG_1PPQ2_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_PRT_1P_REG_1PPQ2_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ2) >> 4) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ2) | (((val) & 0x3) << 4))
++/* Priority Queue 1 (3:2) */
++#define LTQ_ES_PRT_1P_REG_1PPQ1   (0x3 << 2)
++#define LTQ_ES_PRT_1P_REG_1PPQ1_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_PRT_1P_REG_1PPQ1_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ1) >> 2) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ1) | (((val) & 0x3) << 2))
++/* Priority Queue 0 (1:0) */
++#define LTQ_ES_PRT_1P_REG_1PPQ0   (0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ0_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_PRT_1P_REG_1PPQ0_GET(val)   ((((val) & LTQ_ES_PRT_1P_REG_1PPQ0) >> 0) & 0x3)
++#define LTQ_ES_PRT_1P_REG_1PPQ0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRT_1P_REG_1PPQ0) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * Global Bucket Size Base counter
++ ******************************************************************************/
++
++/* Reserved (31:18) */
++#define LTQ_ES_GBKT_SZBS_REG_REV   (0x3fff << 18)
++#define LTQ_ES_GBKT_SZBS_REG_REV_GET(val)   ((((val) & LTQ_ES_GBKT_SZBS_REG_REV) >> 18) & 0x3fff)
++/* Base[17:0] (17:0) */
++#define LTQ_ES_GBKT_SZBS_REG_BASE17_0   (0x3ffff)
++#define LTQ_ES_GBKT_SZBS_REG_BASE17_0_VAL(val)   (((val) & 0x3ffff) << 0)
++#define LTQ_ES_GBKT_SZBS_REG_BASE17_0_GET(val)   ((((val) & LTQ_ES_GBKT_SZBS_REG_BASE17_0) >> 0) & 0x3ffff)
++#define LTQ_ES_GBKT_SZBS_REG_BASE17_0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_GBKT_SZBS_REG_BASE17_0) | (((val) & 0x3ffff) << 0))
++
++/*******************************************************************************
++ * Global Bucket Size Extend Base Counter
++ ******************************************************************************/
++
++/* Reserved (31:18) */
++#define LTQ_ES_GBKT_SZEBS_REG_REV   (0x3fff << 18)
++#define LTQ_ES_GBKT_SZEBS_REG_REV_GET(val)   ((((val) & LTQ_ES_GBKT_SZEBS_REG_REV) >> 18) & 0x3fff)
++/* Extend Base[17:0] (17:0) */
++#define LTQ_ES_GBKT_SZEBS_REG_EBASE17_0   (0x3ffff)
++#define LTQ_ES_GBKT_SZEBS_REG_EBASE17_0_VAL(val)   (((val) & 0x3ffff) << 0)
++#define LTQ_ES_GBKT_SZEBS_REG_EBASE17_0_GET(val)   ((((val) & LTQ_ES_GBKT_SZEBS_REG_EBASE17_0) >> 0) & 0x3ffff)
++#define LTQ_ES_GBKT_SZEBS_REG_EBASE17_0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_GBKT_SZEBS_REG_EBASE17_0) | (((val) & 0x3ffff) << 0))
++
++/*******************************************************************************
++ * Buffer Threshold Register
++ ******************************************************************************/
++
++/* Port Unfull Offset 3 (31:30) */
++#define LTQ_ES_BF_TH_REG_PUO3   (0x3 << 30)
++#define LTQ_ES_BF_TH_REG_PUO3_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_BF_TH_REG_PUO3_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PUO3) >> 30) & 0x3)
++#define LTQ_ES_BF_TH_REG_PUO3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PUO3) | (((val) & 0x3) << 30))
++/* Port Unfull Offset 2 (29:28) */
++#define LTQ_ES_BF_TH_REG_PUO2   (0x3 << 28)
++#define LTQ_ES_BF_TH_REG_PUO2_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_BF_TH_REG_PUO2_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PUO2) >> 28) & 0x3)
++#define LTQ_ES_BF_TH_REG_PUO2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PUO2) | (((val) & 0x3) << 28))
++/* Port Unfull Offset 1 (27:26) */
++#define LTQ_ES_BF_TH_REG_PUO1   (0x3 << 26)
++#define LTQ_ES_BF_TH_REG_PUO1_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_BF_TH_REG_PUO1_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PUO1) >> 26) & 0x3)
++#define LTQ_ES_BF_TH_REG_PUO1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PUO1) | (((val) & 0x3) << 26))
++/* Port Unfull Offset 0 (25:24) */
++#define LTQ_ES_BF_TH_REG_PUO0   (0x3 << 24)
++#define LTQ_ES_BF_TH_REG_PUO0_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_BF_TH_REG_PUO0_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PUO0) >> 24) & 0x3)
++#define LTQ_ES_BF_TH_REG_PUO0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PUO0) | (((val) & 0x3) << 24))
++/* Port Full Offset 3 (23:22) */
++#define LTQ_ES_BF_TH_REG_PFO3   (0x3 << 22)
++#define LTQ_ES_BF_TH_REG_PFO3_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_BF_TH_REG_PFO3_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PFO3) >> 22) & 0x3)
++#define LTQ_ES_BF_TH_REG_PFO3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PFO3) | (((val) & 0x3) << 22))
++/* Port Full Offset 2 (21:20) */
++#define LTQ_ES_BF_TH_REG_PFO2   (0x3 << 20)
++#define LTQ_ES_BF_TH_REG_PFO2_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_BF_TH_REG_PFO2_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PFO2) >> 20) & 0x3)
++#define LTQ_ES_BF_TH_REG_PFO2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PFO2) | (((val) & 0x3) << 20))
++/* Port Full Offset 1 (19:18) */
++#define LTQ_ES_BF_TH_REG_PFO1   (0x3 << 18)
++#define LTQ_ES_BF_TH_REG_PFO1_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_BF_TH_REG_PFO1_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PFO1) >> 18) & 0x3)
++#define LTQ_ES_BF_TH_REG_PFO1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PFO1) | (((val) & 0x3) << 18))
++/* Port Full Offset 0 (17:16) */
++#define LTQ_ES_BF_TH_REG_PFO0   (0x3 << 16)
++#define LTQ_ES_BF_TH_REG_PFO0_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_BF_TH_REG_PFO0_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PFO0) >> 16) & 0x3)
++#define LTQ_ES_BF_TH_REG_PFO0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PFO0) | (((val) & 0x3) << 16))
++/* Reserved (15:14) */
++#define LTQ_ES_BF_TH_REG_RES   (0x3 << 14)
++#define LTQ_ES_BF_TH_REG_RES_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_RES) >> 14) & 0x3)
++/* Total Low Add (13) */
++#define LTQ_ES_BF_TH_REG_TLA   (0x1 << 13)
++#define LTQ_ES_BF_TH_REG_TLA_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_BF_TH_REG_TLA_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_TLA) >> 13) & 0x1)
++#define LTQ_ES_BF_TH_REG_TLA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_TLA) | (((val) & 0x1) << 13))
++/* Total High Add (12) */
++#define LTQ_ES_BF_TH_REG_THA   (0x1 << 12)
++#define LTQ_ES_BF_TH_REG_THA_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_BF_TH_REG_THA_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_THA) >> 12) & 0x1)
++#define LTQ_ES_BF_TH_REG_THA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_THA) | (((val) & 0x1) << 12))
++/* Total Low Offset (11:10) */
++#define LTQ_ES_BF_TH_REG_TLO   (0x3 << 10)
++#define LTQ_ES_BF_TH_REG_TLO_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_BF_TH_REG_TLO_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_TLO) >> 10) & 0x3)
++#define LTQ_ES_BF_TH_REG_TLO_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_TLO) | (((val) & 0x3) << 10))
++/* Total High Offset (9:8) */
++#define LTQ_ES_BF_TH_REG_THO   (0x3 << 8)
++#define LTQ_ES_BF_TH_REG_THO_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_BF_TH_REG_THO_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_THO) >> 8) & 0x3)
++#define LTQ_ES_BF_TH_REG_THO_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_THO) | (((val) & 0x3) << 8))
++/* Port Unfull Add (7:4) */
++#define LTQ_ES_BF_TH_REG_PUA   (0xf << 4)
++#define LTQ_ES_BF_TH_REG_PUA_VAL(val)   (((val) & 0xf) << 4)
++#define LTQ_ES_BF_TH_REG_PUA_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PUA) >> 4) & 0xf)
++#define LTQ_ES_BF_TH_REG_PUA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PUA) | (((val) & 0xf) << 4))
++/* Port Full Add (3:0) */
++#define LTQ_ES_BF_TH_REG_PFA   (0xf)
++#define LTQ_ES_BF_TH_REG_PFA_VAL(val)   (((val) & 0xf) << 0)
++#define LTQ_ES_BF_TH_REG_PFA_GET(val)   ((((val) & LTQ_ES_BF_TH_REG_PFA) >> 0) & 0xf)
++#define LTQ_ES_BF_TH_REG_PFA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_BF_TH_REG_PFA) | (((val) & 0xf) << 0))
++
++/*******************************************************************************
++ * PMAC Header Control Register
++ ******************************************************************************/
++
++/* Reserved (31:22) */
++#define LTQ_ES_PMAC_HD_CTL_RES   (0x3ff << 22)
++#define LTQ_ES_PMAC_HD_CTL_RES_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_RES) >> 22) & 0x3ff)
++/* Remove Layer-2 Header from Packets Going from PMAC to DMA (21) */
++#define LTQ_ES_PMAC_HD_CTL_RL2   (0x1 << 21)
++#define LTQ_ES_PMAC_HD_CTL_RL2_VAL(val)   (((val) & 0x1) << 21)
++#define LTQ_ES_PMAC_HD_CTL_RL2_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_RL2) >> 21) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_RL2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_RL2) | (((val) & 0x1) << 21))
++/* Remove CRC from Packets Going from PMAC to DMA (20) */
++#define LTQ_ES_PMAC_HD_CTL_RC   (0x1 << 20)
++#define LTQ_ES_PMAC_HD_CTL_RC_VAL(val)   (((val) & 0x1) << 20)
++#define LTQ_ES_PMAC_HD_CTL_RC_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_RC) >> 20) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_RC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_RC) | (((val) & 0x1) << 20))
++/* Status Header for Packets from PMAC to DMA (19) */
++#define LTQ_ES_PMAC_HD_CTL_AS   (0x1 << 19)
++#define LTQ_ES_PMAC_HD_CTL_AS_VAL(val)   (((val) & 0x1) << 19)
++#define LTQ_ES_PMAC_HD_CTL_AS_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_AS) >> 19) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_AS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_AS) | (((val) & 0x1) << 19))
++/* Add CRC for packets from DMA to PMAC (18) */
++#define LTQ_ES_PMAC_HD_CTL_AC   (0x1 << 18)
++#define LTQ_ES_PMAC_HD_CTL_AC_VAL(val)   (((val) & 0x1) << 18)
++#define LTQ_ES_PMAC_HD_CTL_AC_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_AC) >> 18) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_AC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_AC) | (((val) & 0x1) << 18))
++/* Contains the length/type value to the added to packets from DMA to PMAC (17:2) */
++#define LTQ_ES_PMAC_HD_CTL_TYPE_LEN   (0xffff << 2)
++#define LTQ_ES_PMAC_HD_CTL_TYPE_LEN_VAL(val)   (((val) & 0xffff) << 2)
++#define LTQ_ES_PMAC_HD_CTL_TYPE_LEN_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_TYPE_LEN) >> 2) & 0xffff)
++#define LTQ_ES_PMAC_HD_CTL_TYPE_LEN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_TYPE_LEN) | (((val) & 0xffff) << 2))
++/* Add TAG to Packets from DMA to PMAC (1) */
++#define LTQ_ES_PMAC_HD_CTL_TAG   (0x1 << 1)
++#define LTQ_ES_PMAC_HD_CTL_TAG_VAL(val)   (((val) & 0x1) << 1)
++#define LTQ_ES_PMAC_HD_CTL_TAG_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_TAG) >> 1) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_TAG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_TAG) | (((val) & 0x1) << 1))
++/* ADD Header to Packets from DMA to PMAC (0) */
++#define LTQ_ES_PMAC_HD_CTL_ADD   (0x1)
++#define LTQ_ES_PMAC_HD_CTL_ADD_VAL(val)   (((val) & 0x1) << 0)
++#define LTQ_ES_PMAC_HD_CTL_ADD_GET(val)   ((((val) & LTQ_ES_PMAC_HD_CTL_ADD) >> 0) & 0x1)
++#define LTQ_ES_PMAC_HD_CTL_ADD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_HD_CTL_ADD) | (((val) & 0x1) << 0))
++
++/*******************************************************************************
++ * PMAC Source Address Register 1
++ ******************************************************************************/
++
++/* Source Address to be inserted as a part of the Ethernet header. (15:0) */
++#define LTQ_ES_PMAC_SA1_SA_47_32   (0xffff)
++#define LTQ_ES_PMAC_SA1_SA_47_32_VAL(val)   (((val) & 0xffff) << 0)
++#define LTQ_ES_PMAC_SA1_SA_47_32_GET(val)   ((((val) & LTQ_ES_PMAC_SA1_SA_47_32) >> 0) & 0xffff)
++#define LTQ_ES_PMAC_SA1_SA_47_32_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_SA1_SA_47_32) | (((val) & 0xffff) << 0))
++
++/*******************************************************************************
++ * PMAC Source Address Register 2
++ ******************************************************************************/
++
++/* Source Address (31:0) */
++#define LTQ_ES_PMAC_SA2_SA_31_0   (0xFFFFFFFFL)
++#define LTQ_ES_PMAC_SA2_SA_31_0_VAL(val)   (((val) & 0xFFFFFFFFL) << 0)
++#define LTQ_ES_PMAC_SA2_SA_31_0_GET(val)   ((((val) & LTQ_ES_PMAC_SA2_SA_31_0) >> 0) & 0xFFFFFFFFL)
++#define LTQ_ES_PMAC_SA2_SA_31_0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_SA2_SA_31_0) | (((val) & 0xFFFFFFFFL) << 0))
++
++/*******************************************************************************
++ * PMAC Destination Address Register 1
++ ******************************************************************************/
++
++/* Destination Address (15:0) */
++#define LTQ_ES_PMAC_DA1_DA_47_32   (0xffff)
++#define LTQ_ES_PMAC_DA1_DA_47_32_VAL(val)   (((val) & 0xffff) << 0)
++#define LTQ_ES_PMAC_DA1_DA_47_32_GET(val)   ((((val) & LTQ_ES_PMAC_DA1_DA_47_32) >> 0) & 0xffff)
++#define LTQ_ES_PMAC_DA1_DA_47_32_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_DA1_DA_47_32) | (((val) & 0xffff) << 0))
++
++/*******************************************************************************
++ * PMAC Destination Address Register 2
++ ******************************************************************************/
++
++/* Destination Address to be inserted as a part of the Ethernet header. (31:0) */
++#define LTQ_ES_PMAC_DA2_DA_31_0   (0xFFFFFFFFL)
++#define LTQ_ES_PMAC_DA2_DA_31_0_VAL(val)   (((val) & 0xFFFFFFFFL) << 0)
++#define LTQ_ES_PMAC_DA2_DA_31_0_GET(val)   ((((val) & LTQ_ES_PMAC_DA2_DA_31_0) >> 0) & 0xFFFFFFFFL)
++#define LTQ_ES_PMAC_DA2_DA_31_0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_DA2_DA_31_0) | (((val) & 0xFFFFFFFFL) << 0))
++
++/*******************************************************************************
++ * PMAC VLAN Register
++ ******************************************************************************/
++
++/* Priority to be inserted as a part of VLAN tag (15:13) */
++#define LTQ_ES_PMAC_VLAN_PRI   (0x7 << 13)
++#define LTQ_ES_PMAC_VLAN_PRI_VAL(val)   (((val) & 0x7) << 13)
++#define LTQ_ES_PMAC_VLAN_PRI_GET(val)   ((((val) & LTQ_ES_PMAC_VLAN_PRI) >> 13) & 0x7)
++#define LTQ_ES_PMAC_VLAN_PRI_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_VLAN_PRI) | (((val) & 0x7) << 13))
++/* CFI bit to be inserted as a part of VLAN tag (12) */
++#define LTQ_ES_PMAC_VLAN_CFI   (0x1 << 12)
++#define LTQ_ES_PMAC_VLAN_CFI_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_PMAC_VLAN_CFI_GET(val)   ((((val) & LTQ_ES_PMAC_VLAN_CFI) >> 12) & 0x1)
++#define LTQ_ES_PMAC_VLAN_CFI_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_VLAN_CFI) | (((val) & 0x1) << 12))
++/* VLAN ID to be inserted as a part of VLAN tag (11:0) */
++#define LTQ_ES_PMAC_VLAN_VLAN_ID   (0xfff)
++#define LTQ_ES_PMAC_VLAN_VLAN_ID_VAL(val)   (((val) & 0xfff) << 0)
++#define LTQ_ES_PMAC_VLAN_VLAN_ID_GET(val)   ((((val) & LTQ_ES_PMAC_VLAN_VLAN_ID) >> 0) & 0xfff)
++#define LTQ_ES_PMAC_VLAN_VLAN_ID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_VLAN_VLAN ID) | (((val) & 0xfff) << 0))
++
++/*******************************************************************************
++ * PMAC TX IPG Counter Register
++ ******************************************************************************/
++
++/* IPG Counter (7:0) */
++#define LTQ_ES_PMAC_TX_IPG_IPG_CNT   (0xff)
++#define LTQ_ES_PMAC_TX_IPG_IPG_CNT_VAL(val)   (((val) & 0xff) << 0)
++#define LTQ_ES_PMAC_TX_IPG_IPG_CNT_GET(val)   ((((val) & LTQ_ES_PMAC_TX_IPG_IPG_CNT) >> 0) & 0xff)
++#define LTQ_ES_PMAC_TX_IPG_IPG_CNT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_TX_IPG_IPG_CNT) | (((val) & 0xff) << 0))
++
++/*******************************************************************************
++ * PMAC RX IPG Counter Register
++ ******************************************************************************/
++
++/* IPG Counter (7:0) */
++#define LTQ_ES_PMAC_RX_IPG_IPG_CNT   (0xff)
++#define LTQ_ES_PMAC_RX_IPG_IPG_CNT_VAL(val)   (((val) & 0xff) << 0)
++#define LTQ_ES_PMAC_RX_IPG_IPG_CNT_GET(val)   ((((val) & LTQ_ES_PMAC_RX_IPG_IPG_CNT) >> 0) & 0xff)
++#define LTQ_ES_PMAC_RX_IPG_IPG_CNT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PMAC_RX_IPG_IPG_CNT) | (((val) & 0xff) << 0))
++
++/*******************************************************************************
++ * Address Table Control 0 Register
++ ******************************************************************************/
++
++/* Address [31:0] (31:0) */
++#define LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0   (0xFFFFFFFFL)
++#define LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0_VAL(val)   (((val) & 0xFFFFFFFFL) << 0)
++#define LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0) >> 0) & 0xFFFFFFFFL)
++#define LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL0_REG_ADDR31_0) | (((val) & 0xFFFFFFFFL) << 0))
++
++/*******************************************************************************
++ * Address Table Control 1 Register
++ ******************************************************************************/
++
++/* Port Map (22:20) */
++#define LTQ_ES_ADR_TB_CTL1_REG_PMAP   (0x7 << 20)
++#define LTQ_ES_ADR_TB_CTL1_REG_PMAP_VAL(val)   (((val) & 0x7) << 20)
++#define LTQ_ES_ADR_TB_CTL1_REG_PMAP_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL1_REG_PMAP) >> 20) & 0x7)
++#define LTQ_ES_ADR_TB_CTL1_REG_PMAP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL1_REG_PMAP) | (((val) & 0x7) << 20))
++/* FID group (17:16) */
++#define LTQ_ES_ADR_TB_CTL1_REG_FID   (0x3 << 16)
++#define LTQ_ES_ADR_TB_CTL1_REG_FID_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_ADR_TB_CTL1_REG_FID_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL1_REG_FID) >> 16) & 0x3)
++#define LTQ_ES_ADR_TB_CTL1_REG_FID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL1_REG_FID) | (((val) & 0x3) << 16))
++/* Address [47:32] (15:0) */
++#define LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32   (0xffff)
++#define LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32_VAL(val)   (((val) & 0xffff) << 0)
++#define LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32) >> 0) & 0xffff)
++#define LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL1_REG_ADDR47_32) | (((val) & 0xffff) << 0))
++
++/*******************************************************************************
++ * Address Table Control 2 Register
++ ******************************************************************************/
++
++/* Command (22:20) */
++#define LTQ_ES_ADR_TB_CTL2_REG_CMD   (0x7 << 20)
++#define LTQ_ES_ADR_TB_CTL2_REG_CMD_VAL(val)   (((val) & 0x7) << 20)
++#define LTQ_ES_ADR_TB_CTL2_REG_CMD_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL2_REG_CMD) >> 20) & 0x7)
++#define LTQ_ES_ADR_TB_CTL2_REG_CMD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL2_REG_CMD) | (((val) & 0x7) << 20))
++/* Access Control (19:16) */
++#define LTQ_ES_ADR_TB_CTL2_REG_AC   (0xf << 16)
++#define LTQ_ES_ADR_TB_CTL2_REG_AC_VAL(val)   (((val) & 0xf) << 16)
++#define LTQ_ES_ADR_TB_CTL2_REG_AC_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL2_REG_AC) >> 16) & 0xf)
++#define LTQ_ES_ADR_TB_CTL2_REG_AC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL2_REG_AC) | (((val) & 0xf) << 16))
++/* Info Type: Static address (12) */
++#define LTQ_ES_ADR_TB_CTL2_REG_INFOT   (0x1 << 12)
++#define LTQ_ES_ADR_TB_CTL2_REG_INFOT_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_ADR_TB_CTL2_REG_INFOT_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL2_REG_INFOT) >> 12) & 0x1)
++#define LTQ_ES_ADR_TB_CTL2_REG_INFOT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL2_REG_INFOT) | (((val) & 0x1) << 12))
++/* Info_Ctrl/Age Timer (10:0) */
++#define LTQ_ES_ADR_TB_CTL2_REG_ITAT   (0x7ff)
++#define LTQ_ES_ADR_TB_CTL2_REG_ITAT_VAL(val)   (((val) & 0x7ff) << 0)
++#define LTQ_ES_ADR_TB_CTL2_REG_ITAT_GET(val)   ((((val) & LTQ_ES_ADR_TB_CTL2_REG_ITAT) >> 0) & 0x7ff)
++#define LTQ_ES_ADR_TB_CTL2_REG_ITAT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_ADR_TB_CTL2_REG_ITAT) | (((val) & 0x7ff) << 0))
++
++/*******************************************************************************
++ * Address Table Status 0 Register
++ ******************************************************************************/
++
++/* Address [31:0] (31:0) */
++#define LTQ_ES_ADR_TB_ST0_REG_ADDRS31_0   (0xFFFFFFFFL)
++#define LTQ_ES_ADR_TB_ST0_REG_ADDRS31_0_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST0_REG_ADDRS31_0) >> 0) & 0xFFFFFFFFL)
++
++/*******************************************************************************
++ * Address Table Status 1 Register
++ ******************************************************************************/
++
++/* Port Map (22:20) */
++#define LTQ_ES_ADR_TB_ST1_REG_PMAPS   (0x7 << 20)
++#define LTQ_ES_ADR_TB_ST1_REG_PMAPS_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST1_REG_PMAPS) >> 20) & 0x7)
++/* FID group (17:16) */
++#define LTQ_ES_ADR_TB_ST1_REG_FIDS   (0x3 << 16)
++#define LTQ_ES_ADR_TB_ST1_REG_FIDS_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST1_REG_FIDS) >> 16) & 0x3)
++/* Address [47:32] (15:0) */
++#define LTQ_ES_ADR_TB_ST1_REG_ADDRS47_32   (0xffff)
++#define LTQ_ES_ADR_TB_ST1_REG_ADDRS47_32_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST1_REG_ADDRS47_32) >> 0) & 0xffff)
++
++/*******************************************************************************
++ * Address Table Status 2 Register
++ ******************************************************************************/
++
++/* Busy (31) */
++#define LTQ_ES_ADR_TB_ST2_REG_BUSY   (0x1 << 31)
++#define LTQ_ES_ADR_TB_ST2_REG_BUSY_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_BUSY) >> 31) & 0x1)
++/* Result (30:28) */
++#define LTQ_ES_ADR_TB_ST2_REG_RSLT   (0x7 << 28)
++#define LTQ_ES_ADR_TB_ST2_REG_RSLT_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_RSLT) >> 28) & 0x7)
++/* Command (22:20) */
++#define LTQ_ES_ADR_TB_ST2_REG_CMD   (0x7 << 20)
++#define LTQ_ES_ADR_TB_ST2_REG_CMD_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_CMD) >> 20) & 0x7)
++/* Access Control (19:16) */
++#define LTQ_ES_ADR_TB_ST2_REG_AC   (0xf << 16)
++#define LTQ_ES_ADR_TB_ST2_REG_AC_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_AC) >> 16) & 0xf)
++/* Bad Status (14) */
++#define LTQ_ES_ADR_TB_ST2_REG_BAD   (0x1 << 14)
++#define LTQ_ES_ADR_TB_ST2_REG_BAD_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_BAD) >> 14) & 0x1)
++/* Occupy (13) */
++#define LTQ_ES_ADR_TB_ST2_REG_OCP   (0x1 << 13)
++#define LTQ_ES_ADR_TB_ST2_REG_OCP_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_OCP) >> 13) & 0x1)
++/* Info Type: Static address (12) */
++#define LTQ_ES_ADR_TB_ST2_REG_INFOTS   (0x1 << 12)
++#define LTQ_ES_ADR_TB_ST2_REG_INFOTS_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_INFOTS) >> 12) & 0x1)
++/* Info_Ctrl/Age Timer Status (10:0) */
++#define LTQ_ES_ADR_TB_ST2_REG_ITATS   (0x7ff)
++#define LTQ_ES_ADR_TB_ST2_REG_ITATS_GET(val)   ((((val) & LTQ_ES_ADR_TB_ST2_REG_ITATS) >> 0) & 0x7ff)
++
++/*******************************************************************************
++ * RMON Counter Control Register
++ ******************************************************************************/
++
++/* Reserved (31:12) */
++#define LTQ_ES_RMON_CTL_REG_RES   (0xfffff << 12)
++#define LTQ_ES_RMON_CTL_REG_RES_GET(val)   ((((val) & LTQ_ES_RMON_CTL_REG_RES) >> 12) & 0xfffff)
++/* Busy/Access Start (11) */
++#define LTQ_ES_RMON_CTL_REG_BAS   (0x1 << 11)
++#define LTQ_ES_RMON_CTL_REG_BAS_VAL(val)   (((val) & 0x1) << 11)
++#define LTQ_ES_RMON_CTL_REG_BAS_GET(val)   ((((val) & LTQ_ES_RMON_CTL_REG_BAS) >> 11) & 0x1)
++#define LTQ_ES_RMON_CTL_REG_BAS_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RMON_CTL_REG_BAS) | (((val) & 0x1) << 11))
++/* Command for access counter (10:9) */
++#define LTQ_ES_RMON_CTL_REG_CAC   (0x3 << 9)
++#define LTQ_ES_RMON_CTL_REG_CAC_VAL(val)   (((val) & 0x3) << 9)
++#define LTQ_ES_RMON_CTL_REG_CAC_GET(val)   ((((val) & LTQ_ES_RMON_CTL_REG_CAC) >> 9) & 0x3)
++#define LTQ_ES_RMON_CTL_REG_CAC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RMON_CTL_REG_CAC) | (((val) & 0x3) << 9))
++/* Port (8:6) */
++#define LTQ_ES_RMON_CTL_REG_PORTC   (0x7 << 6)
++#define LTQ_ES_RMON_CTL_REG_PORTC_VAL(val)   (((val) & 0x7) << 6)
++#define LTQ_ES_RMON_CTL_REG_PORTC_GET(val)   ((((val) & LTQ_ES_RMON_CTL_REG_PORTC) >> 6) & 0x7)
++#define LTQ_ES_RMON_CTL_REG_PORTC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RMON_CTL_REG_PORTC) | (((val) & 0x7) << 6))
++/* Counter Offset (5:0) */
++#define LTQ_ES_RMON_CTL_REG_OFFSET   (0x3f)
++#define LTQ_ES_RMON_CTL_REG_OFFSET_VAL(val)   (((val) & 0x3f) << 0)
++#define LTQ_ES_RMON_CTL_REG_OFFSET_GET(val)   ((((val) & LTQ_ES_RMON_CTL_REG_OFFSET) >> 0) & 0x3f)
++#define LTQ_ES_RMON_CTL_REG_OFFSET_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RMON_CTL_REG_OFFSET) | (((val) & 0x3f) << 0))
++
++/*******************************************************************************
++ * RMON Counter Status Register
++ ******************************************************************************/
++
++/* Counter [31:0] or Counter[63:32] for byte count (31:0) */
++#define LTQ_ES_RMON_ST_REG_COUNTER   (0xFFFFFFFFL)
++#define LTQ_ES_RMON_ST_REG_COUNTER_GET(val)   ((((val) & LTQ_ES_RMON_ST_REG_COUNTER) >> 0) & 0xFFFFFFFFL)
++
++/*******************************************************************************
++ * MDIO Indirect Access Control
++ ******************************************************************************/
++
++/* The Write Data to the MDIO register (31:16) */
++#define LTQ_ES_MDIO_CTL_REG_WD   (0xffff << 16)
++#define LTQ_ES_MDIO_CTL_REG_WD_VAL(val)   (((val) & 0xffff) << 16)
++#define LTQ_ES_MDIO_CTL_REG_WD_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_WD) >> 16) & 0xffff)
++#define LTQ_ES_MDIO_CTL_REG_WD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_MDIO_CTL_REG_WD) | (((val) & 0xffff) << 16))
++/* Busy state (15) */
++#define LTQ_ES_MDIO_CTL_REG_MBUSY   (0x1 << 15)
++#define LTQ_ES_MDIO_CTL_REG_MBUSY_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_MDIO_CTL_REG_MBUSY_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_MBUSY) >> 15) & 0x1)
++#define LTQ_ES_MDIO_CTL_REG_MBUSY_SET(reg,val) (reg) = ((reg & ~LTQ_ES_MDIO_CTL_REG_MBUSY) | (((val) & 0x1) << 15))
++/* Reserved (14:12) */
++#define LTQ_ES_MDIO_CTL_REG_RES   (0x7 << 12)
++#define LTQ_ES_MDIO_CTL_REG_RES_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_RES) >> 12) & 0x7)
++/* Operation Code (11:10) */
++#define LTQ_ES_MDIO_CTL_REG_OP   (0x3 << 10)
++#define LTQ_ES_MDIO_CTL_REG_OP_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_MDIO_CTL_REG_OP_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_OP) >> 10) & 0x3)
++#define LTQ_ES_MDIO_CTL_REG_OP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_MDIO_CTL_REG_OP) | (((val) & 0x3) << 10))
++/* PHY Address (9:5) */
++#define LTQ_ES_MDIO_CTL_REG_PHYAD   (0x1f << 5)
++#define LTQ_ES_MDIO_CTL_REG_PHYAD_VAL(val)   (((val) & 0x1f) << 5)
++#define LTQ_ES_MDIO_CTL_REG_PHYAD_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_PHYAD) >> 5) & 0x1f)
++#define LTQ_ES_MDIO_CTL_REG_PHYAD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_MDIO_CTL_REG_PHYAD) | (((val) & 0x1f) << 5))
++/* Register Address (4:0) */
++#define LTQ_ES_MDIO_CTL_REG_REGAD   (0x1f)
++#define LTQ_ES_MDIO_CTL_REG_REGAD_VAL(val)   (((val) & 0x1f) << 0)
++#define LTQ_ES_MDIO_CTL_REG_REGAD_GET(val)   ((((val) & LTQ_ES_MDIO_CTL_REG_REGAD) >> 0) & 0x1f)
++#define LTQ_ES_MDIO_CTL_REG_REGAD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_MDIO_CTL_REG_REGAD) | (((val) & 0x1f) << 0))
++
++/*******************************************************************************
++ * MDIO Indirect Read Data
++ ******************************************************************************/
++
++/* Reserved (31:16) */
++#define LTQ_ES_MDIO_DATA_REG_RES   (0xffff << 16)
++#define LTQ_ES_MDIO_DATA_REG_RES_GET(val)   ((((val) & LTQ_ES_MDIO_DATA_REG_RES) >> 16) & 0xffff)
++/* The Read Data (15:0) */
++#define LTQ_ES_MDIO_DATA_REG_RD   (0xffff)
++#define LTQ_ES_MDIO_DATA_REG_RD_GET(val)   ((((val) & LTQ_ES_MDIO_DATA_REG_RD) >> 0) & 0xffff)
++
++/*******************************************************************************
++ * Type Filter Action
++ ******************************************************************************/
++
++/* Destination Queue for Type Filter 7 (31:30) */
++#define LTQ_ES_TP_FLT_ACT_REG_QATF7   (0x3 << 30)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF7_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF7_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QATF7) >> 30) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF7_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QATF7) | (((val) & 0x3) << 30))
++/* Destination Queue for Type Filter 6 (29:28) */
++#define LTQ_ES_TP_FLT_ACT_REG_QATF6   (0x3 << 28)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF6_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF6_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QATF6) >> 28) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QATF6_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QATF6) | (((val) & 0x3) << 28))
++/* Destination Queue for Type Filter 5 (27:26) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF5   (0x3 << 26)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF5_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF5_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF5) >> 26) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF5_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF5) | (((val) & 0x3) << 26))
++/* Destination Queue for Type Filter 4 (25:24) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF4   (0x3 << 24)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF4_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF4_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF4) >> 24) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF4_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF4) | (((val) & 0x3) << 24))
++/* Destination Queue for Type Filter 3 (23:22) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF3   (0x3 << 22)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF3_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF3_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF3) >> 22) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF3) | (((val) & 0x3) << 22))
++/* Destination Queue for Type Filter 2 (21:20) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF2   (0x3 << 20)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF2_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF2_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF2) >> 20) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF2) | (((val) & 0x3) << 20))
++/* Destination Queue for Type Filter 1 (19:18) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF1   (0x3 << 18)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF1_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF1_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF1) >> 18) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF1) | (((val) & 0x3) << 18))
++/* Destination Queue for Type Filter 0 (17:16) */
++#define LTQ_ES_TP_FLT_ACT_REG_QTF0   (0x3 << 16)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF0_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF0_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_QTF0) >> 16) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_QTF0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_QTF0) | (((val) & 0x3) << 16))
++/* Action for Type Filter 7 (15:14) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF7   (0x3 << 14)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF7_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF7_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF7) >> 14) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF7_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF7) | (((val) & 0x3) << 14))
++/* Action for Type Filter 6 (13:12) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF6   (0x3 << 12)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF6_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF6_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF6) >> 12) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF6_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF6) | (((val) & 0x3) << 12))
++/* Action for Type Filter 5 (11:10) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF5   (0x3 << 10)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF5_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF5_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF5) >> 10) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF5_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF5) | (((val) & 0x3) << 10))
++/* Action for Type Filter 4 (9:8) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF4   (0x3 << 8)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF4_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF4_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF4) >> 8) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF4_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF4) | (((val) & 0x3) << 8))
++/* Action for Type Filter 3 (7:6) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF3   (0x3 << 6)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF3_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF3_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF3) >> 6) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF3) | (((val) & 0x3) << 6))
++/* Action for Type Filter 2 (5:4) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF2   (0x3 << 4)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF2_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF2_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF2) >> 4) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF2) | (((val) & 0x3) << 4))
++/* Action for Type Filter 1 (3:2) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF1   (0x3 << 2)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF1_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF1_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF1) >> 2) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF1) | (((val) & 0x3) << 2))
++/* Action for Type Filter 0 (1:0) */
++#define LTQ_ES_TP_FLT_ACT_REG_ATF0   (0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF0_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF0_GET(val)   ((((val) & LTQ_ES_TP_FLT_ACT_REG_ATF0) >> 0) & 0x3)
++#define LTQ_ES_TP_FLT_ACT_REG_ATF0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT_ACT_REG_ATF0) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * Protocol Filter Action
++ ******************************************************************************/
++
++/* Action for Protocol Filter 7 (15:14) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF7   (0x3 << 14)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF7_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF7_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF7) >> 14) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF7_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF7) | (((val) & 0x3) << 14))
++/* Action for Protocol Filter 6 (13:12) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF6   (0x3 << 12)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF6_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF6_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF6) >> 12) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF6_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF6) | (((val) & 0x3) << 12))
++/* Action for Protocol Filter 5 (11:10) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF5   (0x3 << 10)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF5_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF5_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF5) >> 10) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF5_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF5) | (((val) & 0x3) << 10))
++/* Action for Protocol Filter 4 (9:8) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF4   (0x3 << 8)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF4_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF4_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF4) >> 8) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF4_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF4) | (((val) & 0x3) << 8))
++/* Action for Protocol Filter 3 (7:6) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF3   (0x3 << 6)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF3_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF3_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF3) >> 6) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF3) | (((val) & 0x3) << 6))
++/* Action for Protocol Filter 2 (5:4) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF2   (0x3 << 4)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF2_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF2_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF2) >> 4) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF2) | (((val) & 0x3) << 4))
++/* Action for Protocol Filter 1 (3:2) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF1   (0x3 << 2)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF1_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF1_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF1) >> 2) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF1) | (((val) & 0x3) << 2))
++/* Action for Protocol Filter 0 (1:0) */
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF0   (0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF0_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF0_GET(val)   ((((val) & LTQ_ES_PRTCL_FLT_ACT_REG_APF0) >> 0) & 0x3)
++#define LTQ_ES_PRTCL_FLT_ACT_REG_APF0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_FLT_ACT_REG_APF0) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * VLAN Filter 0
++ ******************************************************************************/
++
++/* Res (31:24) */
++#define LTQ_ES_VLAN_FLT0_REG_RES   (0xff << 24)
++#define LTQ_ES_VLAN_FLT0_REG_RES_VAL(val)   (((val) & 0xff) << 24)
++#define LTQ_ES_VLAN_FLT0_REG_RES_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_RES) >> 24) & 0xff)
++#define LTQ_ES_VLAN_FLT0_REG_RES_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_RES) | (((val) & 0xff) << 24))
++/* FID (23:22) */
++#define LTQ_ES_VLAN_FLT0_REG_FID   (0x3 << 22)
++#define LTQ_ES_VLAN_FLT0_REG_FID_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_VLAN_FLT0_REG_FID_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_FID) >> 22) & 0x3)
++#define LTQ_ES_VLAN_FLT0_REG_FID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_FID) | (((val) & 0x3) << 22))
++/* Tagged Member (21:19) */
++#define LTQ_ES_VLAN_FLT0_REG_TM   (0x7 << 19)
++#define LTQ_ES_VLAN_FLT0_REG_TM_VAL(val)   (((val) & 0x7) << 19)
++#define LTQ_ES_VLAN_FLT0_REG_TM_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_TM) >> 19) & 0x7)
++#define LTQ_ES_VLAN_FLT0_REG_TM_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_TM) | (((val) & 0x7) << 19))
++/* Member (18:16) */
++#define LTQ_ES_VLAN_FLT0_REG_M   (0x7 << 16)
++#define LTQ_ES_VLAN_FLT0_REG_M_VAL(val)   (((val) & 0x7) << 16)
++#define LTQ_ES_VLAN_FLT0_REG_M_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_M) >> 16) & 0x7)
++#define LTQ_ES_VLAN_FLT0_REG_M_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_M) | (((val) & 0x7) << 16))
++/* VLAN_Valid (15) */
++#define LTQ_ES_VLAN_FLT0_REG_VV   (0x1 << 15)
++#define LTQ_ES_VLAN_FLT0_REG_VV_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_VLAN_FLT0_REG_VV_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_VV) >> 15) & 0x1)
++#define LTQ_ES_VLAN_FLT0_REG_VV_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_VV) | (((val) & 0x1) << 15))
++/* VLAN PRI (14:12) */
++#define LTQ_ES_VLAN_FLT0_REG_VP   (0x7 << 12)
++#define LTQ_ES_VLAN_FLT0_REG_VP_VAL(val)   (((val) & 0x7) << 12)
++#define LTQ_ES_VLAN_FLT0_REG_VP_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_VP) >> 12) & 0x7)
++#define LTQ_ES_VLAN_FLT0_REG_VP_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_VP) | (((val) & 0x7) << 12))
++/* VID (11:0) */
++#define LTQ_ES_VLAN_FLT0_REG_VID   (0xfff)
++#define LTQ_ES_VLAN_FLT0_REG_VID_VAL(val)   (((val) & 0xfff) << 0)
++#define LTQ_ES_VLAN_FLT0_REG_VID_GET(val)   ((((val) & LTQ_ES_VLAN_FLT0_REG_VID) >> 0) & 0xfff)
++#define LTQ_ES_VLAN_FLT0_REG_VID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_VLAN_FLT0_REG_VID) | (((val) & 0xfff) << 0))
++
++/*******************************************************************************
++ * Type Filter 10
++ ******************************************************************************/
++
++/* Value 1 Compared with Ether-Type (31:16) */
++#define LTQ_ES_TP_FLT10_REG_VCET1   (0xffff << 16)
++#define LTQ_ES_TP_FLT10_REG_VCET1_VAL(val)   (((val) & 0xffff) << 16)
++#define LTQ_ES_TP_FLT10_REG_VCET1_GET(val)   ((((val) & LTQ_ES_TP_FLT10_REG_VCET1) >> 16) & 0xffff)
++#define LTQ_ES_TP_FLT10_REG_VCET1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT10_REG_VCET1) | (((val) & 0xffff) << 16))
++/* Value 0 Compared with Ether-Type (15:0) */
++#define LTQ_ES_TP_FLT10_REG_VCET0   (0xffff)
++#define LTQ_ES_TP_FLT10_REG_VCET0_VAL(val)   (((val) & 0xffff) << 0)
++#define LTQ_ES_TP_FLT10_REG_VCET0_GET(val)   ((((val) & LTQ_ES_TP_FLT10_REG_VCET0) >> 0) & 0xffff)
++#define LTQ_ES_TP_FLT10_REG_VCET0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TP_FLT10_REG_VCET0) | (((val) & 0xffff) << 0))
++
++/*******************************************************************************
++ * DiffServMapping 0
++ ******************************************************************************/
++
++/* Priority Queue F (31:30) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQF   (0x3 << 30)
++#define LTQ_ES_DFSRV_MAP0_REG_PQF_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_DFSRV_MAP0_REG_PQF_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQF) >> 30) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQF_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQF) | (((val) & 0x3) << 30))
++/* Priority Queue E (29:28) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQE   (0x3 << 28)
++#define LTQ_ES_DFSRV_MAP0_REG_PQE_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_DFSRV_MAP0_REG_PQE_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQE) >> 28) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQE_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQE) | (((val) & 0x3) << 28))
++/* Priority Queue D (27:26) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQD   (0x3 << 26)
++#define LTQ_ES_DFSRV_MAP0_REG_PQD_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_DFSRV_MAP0_REG_PQD_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQD) >> 26) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQD_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQD) | (((val) & 0x3) << 26))
++/* Priority Queue C (25:24) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQC   (0x3 << 24)
++#define LTQ_ES_DFSRV_MAP0_REG_PQC_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_DFSRV_MAP0_REG_PQC_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQC) >> 24) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQC_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQC) | (((val) & 0x3) << 24))
++/* Priority Queue B (23:22) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQB   (0x3 << 22)
++#define LTQ_ES_DFSRV_MAP0_REG_PQB_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_DFSRV_MAP0_REG_PQB_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQB) >> 22) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQB_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQB) | (((val) & 0x3) << 22))
++/* Priority Queue A (21:20) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQA   (0x3 << 20)
++#define LTQ_ES_DFSRV_MAP0_REG_PQA_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_DFSRV_MAP0_REG_PQA_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQA) >> 20) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQA_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQA) | (((val) & 0x3) << 20))
++/* Priority Queue 9 (19:18) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ9   (0x3 << 18)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ9_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ9_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ9) >> 18) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ9_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ9) | (((val) & 0x3) << 18))
++/* Priority Queue 8 (17:16) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ8   (0x3 << 16)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ8_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ8_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ8) >> 16) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ8_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ8) | (((val) & 0x3) << 16))
++/* Priority Queue 7 (15:14) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ7   (0x3 << 14)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ7_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ7_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ7) >> 14) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ7_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ7) | (((val) & 0x3) << 14))
++/* Priority Queue 6 (13:12) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ6   (0x3 << 12)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ6_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ6_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ6) >> 12) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ6_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ6) | (((val) & 0x3) << 12))
++/* Priority Queue 5 (11:10) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ5   (0x3 << 10)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ5_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ5_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ5) >> 10) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ5_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ5) | (((val) & 0x3) << 10))
++/* Priority Queue 4 (9:8) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ4   (0x3 << 8)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ4_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ4_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ4) >> 8) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ4_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ4) | (((val) & 0x3) << 8))
++/* Priority Queue 3 (7:6) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ3   (0x3 << 6)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ3_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ3_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ3) >> 6) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ3) | (((val) & 0x3) << 6))
++/* Priority Queue 2 (5:4) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ2   (0x3 << 4)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ2_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ2_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ2) >> 4) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ2) | (((val) & 0x3) << 4))
++/* Priority Queue 1 (3:2) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ1   (0x3 << 2)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ1_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ1_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ1) >> 2) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ1) | (((val) & 0x3) << 2))
++/* Priority Queue 0 (1:0) */
++#define LTQ_ES_DFSRV_MAP0_REG_PQ0   (0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ0_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ0_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP0_REG_PQ0) >> 0) & 0x3)
++#define LTQ_ES_DFSRV_MAP0_REG_PQ0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP0_REG_PQ0) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * DiffServMapping 1
++ ******************************************************************************/
++
++/* Priority Queue 1F (31:30) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1F   (0x3 << 30)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1F_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1F_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1F) >> 30) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1F_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1F) | (((val) & 0x3) << 30))
++/* Priority Queue 1E (29:28) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1E   (0x3 << 28)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1E_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1E_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1E) >> 28) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1E_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1E) | (((val) & 0x3) << 28))
++/* Priority Queue 1D (27:26) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1D   (0x3 << 26)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1D_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1D_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1D) >> 26) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1D_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1D) | (((val) & 0x3) << 26))
++/* Priority Queue 1C (25:24) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1C   (0x3 << 24)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1C_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1C_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1C) >> 24) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1C_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1C) | (((val) & 0x3) << 24))
++/* Priority Queue 1B (23:22) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1B   (0x3 << 22)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1B_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1B_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1B) >> 22) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1B_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1B) | (((val) & 0x3) << 22))
++/* Priority Queue 1A (21:20) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1A   (0x3 << 20)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1A_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1A_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ1A) >> 20) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ1A_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ1A) | (((val) & 0x3) << 20))
++/* Priority Queue 19 (19:18) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ19   (0x3 << 18)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ19_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ19_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ19) >> 18) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ19_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ19) | (((val) & 0x3) << 18))
++/* Priority Queue 18 (17:16) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ18   (0x3 << 16)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ18_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ18_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ18) >> 16) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ18_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ18) | (((val) & 0x3) << 16))
++/* Priority Queue 17 (15:14) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ17   (0x3 << 14)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ17_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ17_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ17) >> 14) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ17_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ17) | (((val) & 0x3) << 14))
++/* Priority Queue 16 (13:12) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ16   (0x3 << 12)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ16_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ16_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ16) >> 12) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ16_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ16) | (((val) & 0x3) << 12))
++/* Priority Queue 15 (11:10) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ15   (0x3 << 10)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ15_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ15_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ15) >> 10) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ15_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ15) | (((val) & 0x3) << 10))
++/* Priority Queue 14 (9:8) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ14   (0x3 << 8)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ14_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ14_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ14) >> 8) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ14_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ14) | (((val) & 0x3) << 8))
++/* Priority Queue 13 (7:6) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ13   (0x3 << 6)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ13_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ13_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ13) >> 6) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ13_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ13) | (((val) & 0x3) << 6))
++/* Priority Queue 12 (5:4) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ12   (0x3 << 4)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ12_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ12_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ12) >> 4) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ12_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ12) | (((val) & 0x3) << 4))
++/* Priority Queue 11 (3:2) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ11   (0x3 << 2)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ11_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ11_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ11) >> 2) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ11_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ11) | (((val) & 0x3) << 2))
++/* Priority Queue 10 (1:0) */
++#define LTQ_ES_DFSRV_MAP1_REG_PQ10   (0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ10_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ10_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP1_REG_PQ10) >> 0) & 0x3)
++#define LTQ_ES_DFSRV_MAP1_REG_PQ10_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP1_REG_PQ10) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * DiffServMapping 2
++ ******************************************************************************/
++
++/* Priority Queue 2F (31:30) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2F   (0x3 << 30)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2F_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2F_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2F) >> 30) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2F_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2F) | (((val) & 0x3) << 30))
++/* Priority Queue 2E (29:28) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2E   (0x3 << 28)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2E_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2E_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2E) >> 28) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2E_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2E) | (((val) & 0x3) << 28))
++/* Priority Queue 2D (27:26) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2D   (0x3 << 26)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2D_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2D_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2D) >> 26) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2D_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2D) | (((val) & 0x3) << 26))
++/* Priority Queue 2C (25:24) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2C   (0x3 << 24)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2C_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2C_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2C) >> 24) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2C_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2C) | (((val) & 0x3) << 24))
++/* Priority Queue 2B (23:22) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2B   (0x3 << 22)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2B_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2B_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2B) >> 22) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2B_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2B) | (((val) & 0x3) << 22))
++/* Priority Queue 2A (21:20) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2A   (0x3 << 20)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2A_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2A_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ2A) >> 20) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ2A_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ2A) | (((val) & 0x3) << 20))
++/* Priority Queue 29 (19:18) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ29   (0x3 << 18)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ29_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ29_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ29) >> 18) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ29_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ29) | (((val) & 0x3) << 18))
++/* Priority Queue 28 (17:16) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ28   (0x3 << 16)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ28_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ28_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ28) >> 16) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ28_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ28) | (((val) & 0x3) << 16))
++/* Priority Queue 27 (15:14) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ27   (0x3 << 14)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ27_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ27_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ27) >> 14) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ27_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ27) | (((val) & 0x3) << 14))
++/* Priority Queue 26 (13:12) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ26   (0x3 << 12)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ26_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ26_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ26) >> 12) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ26_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ26) | (((val) & 0x3) << 12))
++/* Priority Queue 25 (11:10) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ25   (0x3 << 10)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ25_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ25_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ25) >> 10) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ25_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ25) | (((val) & 0x3) << 10))
++/* Priority Queue 24 (9:8) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ24   (0x3 << 8)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ24_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ24_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ24) >> 8) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ24_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ24) | (((val) & 0x3) << 8))
++/* Priority Queue 23 (7:6) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ23   (0x3 << 6)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ23_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ23_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ23) >> 6) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ23_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ23) | (((val) & 0x3) << 6))
++/* Priority Queue 22 (5:4) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ22   (0x3 << 4)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ22_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ22_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ22) >> 4) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ22_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ22) | (((val) & 0x3) << 4))
++/* Priority Queue 21 (3:2) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ21   (0x3 << 2)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ21_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ21_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ21) >> 2) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ21_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ21) | (((val) & 0x3) << 2))
++/* Priority Queue 20 (1:0) */
++#define LTQ_ES_DFSRV_MAP2_REG_PQ20   (0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ20_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ20_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP2_REG_PQ20) >> 0) & 0x3)
++#define LTQ_ES_DFSRV_MAP2_REG_PQ20_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP2_REG_PQ20) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * DiffServMapping 3
++ ******************************************************************************/
++
++/* Priority Queue 3F (31:30) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3F   (0x3 << 30)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3F_VAL(val)   (((val) & 0x3) << 30)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3F_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3F) >> 30) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3F_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3F) | (((val) & 0x3) << 30))
++/* Priority Queue 3E (29:28) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3E   (0x3 << 28)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3E_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3E_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3E) >> 28) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3E_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3E) | (((val) & 0x3) << 28))
++/* Priority Queue 3D (27:26) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3D   (0x3 << 26)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3D_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3D_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3D) >> 26) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3D_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3D) | (((val) & 0x3) << 26))
++/* Priority Queue 3C (25:24) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3C   (0x3 << 24)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3C_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3C_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3C) >> 24) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3C_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3C) | (((val) & 0x3) << 24))
++/* Priority Queue 3B (23:22) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3B   (0x3 << 22)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3B_VAL(val)   (((val) & 0x3) << 22)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3B_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3B) >> 22) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3B_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3B) | (((val) & 0x3) << 22))
++/* Priority Queue 3A (21:20) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3A   (0x3 << 20)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3A_VAL(val)   (((val) & 0x3) << 20)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3A_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ3A) >> 20) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ3A_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ3A) | (((val) & 0x3) << 20))
++/* Priority Queue 39 (19:18) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ39   (0x3 << 18)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ39_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ39_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ39) >> 18) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ39_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ39) | (((val) & 0x3) << 18))
++/* Priority Queue 38 (17:16) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ38   (0x3 << 16)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ38_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ38_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ38) >> 16) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ38_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ38) | (((val) & 0x3) << 16))
++/* Priority Queue 37 (15:14) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ37   (0x3 << 14)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ37_VAL(val)   (((val) & 0x3) << 14)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ37_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ37) >> 14) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ37_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ37) | (((val) & 0x3) << 14))
++/* Priority Queue 36 (13:12) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ36   (0x3 << 12)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ36_VAL(val)   (((val) & 0x3) << 12)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ36_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ36) >> 12) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ36_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ36) | (((val) & 0x3) << 12))
++/* Priority Queue 35 (11:10) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ35   (0x3 << 10)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ35_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ35_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ35) >> 10) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ35_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ35) | (((val) & 0x3) << 10))
++/* Priority Queue 34 (9:8) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ34   (0x3 << 8)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ34_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ34_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ34) >> 8) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ34_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ34) | (((val) & 0x3) << 8))
++/* Priority Queue 33 (7:6) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ33   (0x3 << 6)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ33_VAL(val)   (((val) & 0x3) << 6)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ33_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ33) >> 6) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ33_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ33) | (((val) & 0x3) << 6))
++/* Priority Queue 32 (5:4) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ32   (0x3 << 4)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ32_VAL(val)   (((val) & 0x3) << 4)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ32_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ32) >> 4) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ32_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ32) | (((val) & 0x3) << 4))
++/* Priority Queue 31 (3:2) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ31   (0x3 << 2)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ31_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ31_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ31) >> 2) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ31_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ31) | (((val) & 0x3) << 2))
++/* Priority Queue 30 (1:0) */
++#define LTQ_ES_DFSRV_MAP3_REG_PQ30   (0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ30_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ30_GET(val)   ((((val) & LTQ_ES_DFSRV_MAP3_REG_PQ30) >> 0) & 0x3)
++#define LTQ_ES_DFSRV_MAP3_REG_PQ30_SET(reg,val) (reg) = ((reg & ~LTQ_ES_DFSRV_MAP3_REG_PQ30) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * TCP/UDP Port Filter 0
++ ******************************************************************************/
++
++/* Reserved (31:30) */
++#define LTQ_ES_TCP_PF0_REG_RES   (0x3 << 30)
++#define LTQ_ES_TCP_PF0_REG_RES_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_RES) >> 30) & 0x3)
++/* Action for TCP/UDP Port Filter 0 (29:28) */
++#define LTQ_ES_TCP_PF0_REG_ATUF0   (0x3 << 28)
++#define LTQ_ES_TCP_PF0_REG_ATUF0_VAL(val)   (((val) & 0x3) << 28)
++#define LTQ_ES_TCP_PF0_REG_ATUF0_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_ATUF0) >> 28) & 0x3)
++#define LTQ_ES_TCP_PF0_REG_ATUF0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TCP_PF0_REG_ATUF0) | (((val) & 0x3) << 28))
++/* TCP/UDP PRI for TCP/UDP Port Filter 0 (27:26) */
++#define LTQ_ES_TCP_PF0_REG_TUPF0   (0x3 << 26)
++#define LTQ_ES_TCP_PF0_REG_TUPF0_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_TCP_PF0_REG_TUPF0_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_TUPF0) >> 26) & 0x3)
++#define LTQ_ES_TCP_PF0_REG_TUPF0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TCP_PF0_REG_TUPF0) | (((val) & 0x3) << 26))
++/* Compare TCP/UDP Source Port or Destination Port (25:24) */
++#define LTQ_ES_TCP_PF0_REG_COMP0   (0x3 << 24)
++#define LTQ_ES_TCP_PF0_REG_COMP0_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_TCP_PF0_REG_COMP0_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_COMP0) >> 24) & 0x3)
++#define LTQ_ES_TCP_PF0_REG_COMP0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TCP_PF0_REG_COMP0) | (((val) & 0x3) << 24))
++/* Port Range in TCP/UDP (23:16) */
++#define LTQ_ES_TCP_PF0_REG_PRANGE0   (0xff << 16)
++#define LTQ_ES_TCP_PF0_REG_PRANGE0_VAL(val)   (((val) & 0xff) << 16)
++#define LTQ_ES_TCP_PF0_REG_PRANGE0_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_PRANGE0) >> 16) & 0xff)
++#define LTQ_ES_TCP_PF0_REG_PRANGE0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TCP_PF0_REG_PRANGE0) | (((val) & 0xff) << 16))
++/* Base Port number 0 (15:0) */
++#define LTQ_ES_TCP_PF0_REG_BASEPT0   (0xffff)
++#define LTQ_ES_TCP_PF0_REG_BASEPT0_VAL(val)   (((val) & 0xffff) << 0)
++#define LTQ_ES_TCP_PF0_REG_BASEPT0_GET(val)   ((((val) & LTQ_ES_TCP_PF0_REG_BASEPT0) >> 0) & 0xffff)
++#define LTQ_ES_TCP_PF0_REG_BASEPT0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_TCP_PF0_REG_BASEPT0) | (((val) & 0xffff) << 0))
++
++/*******************************************************************************
++ * Reserved DA(0180C2000003~0180C2000000) control register
++ ******************************************************************************/
++
++/* Valid bit for 0180C2000003 (31) */
++#define LTQ_ES_RA_03_00_REG_RA03_VALID   (0x1 << 31)
++#define LTQ_ES_RA_03_00_REG_RA03_VALID_VAL(val)   (((val) & 0x1) << 31)
++#define LTQ_ES_RA_03_00_REG_RA03_VALID_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_VALID) >> 31) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA03_VALID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_VALID) | (((val) & 0x1) << 31))
++/* Span bit for 0180C2000003 (30) */
++#define LTQ_ES_RA_03_00_REG_RA03_SPAN   (0x1 << 30)
++#define LTQ_ES_RA_03_00_REG_RA03_SPAN_VAL(val)   (((val) & 0x1) << 30)
++#define LTQ_ES_RA_03_00_REG_RA03_SPAN_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_SPAN) >> 30) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA03_SPAN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_SPAN) | (((val) & 0x1) << 30))
++/* Management bit for 0180C2000003 (29) */
++#define LTQ_ES_RA_03_00_REG_RA03_MG   (0x1 << 29)
++#define LTQ_ES_RA_03_00_REG_RA03_MG_VAL(val)   (((val) & 0x1) << 29)
++#define LTQ_ES_RA_03_00_REG_RA03_MG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_MG) >> 29) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA03_MG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_MG) | (((val) & 0x1) << 29))
++/* Cross_VLAN bit for 0180C2000003 (28) */
++#define LTQ_ES_RA_03_00_REG_RA03_CV   (0x1 << 28)
++#define LTQ_ES_RA_03_00_REG_RA03_CV_VAL(val)   (((val) & 0x1) << 28)
++#define LTQ_ES_RA_03_00_REG_RA03_CV_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_CV) >> 28) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA03_CV_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_CV) | (((val) & 0x1) << 28))
++/* TXTAG bit for 0180C2000003 (27:26) */
++#define LTQ_ES_RA_03_00_REG_RA03_TXTAG   (0x3 << 26)
++#define LTQ_ES_RA_03_00_REG_RA03_TXTAG_VAL(val)   (((val) & 0x3) << 26)
++#define LTQ_ES_RA_03_00_REG_RA03_TXTAG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_TXTAG) >> 26) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA03_TXTAG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_TXTAG) | (((val) & 0x3) << 26))
++/* Action bit for 0180C2000003 (25:24) */
++#define LTQ_ES_RA_03_00_REG_RA03_ACT   (0x3 << 24)
++#define LTQ_ES_RA_03_00_REG_RA03_ACT_VAL(val)   (((val) & 0x3) << 24)
++#define LTQ_ES_RA_03_00_REG_RA03_ACT_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA03_ACT) >> 24) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA03_ACT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA03_ACT) | (((val) & 0x3) << 24))
++/* Valid bit for 0180C2000002 (23) */
++#define LTQ_ES_RA_03_00_REG_RA02_VALID   (0x1 << 23)
++#define LTQ_ES_RA_03_00_REG_RA02_VALID_VAL(val)   (((val) & 0x1) << 23)
++#define LTQ_ES_RA_03_00_REG_RA02_VALID_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_VALID) >> 23) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA02_VALID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_VALID) | (((val) & 0x1) << 23))
++/* Span bit for 0180C2000002 (22) */
++#define LTQ_ES_RA_03_00_REG_RA02_SPAN   (0x1 << 22)
++#define LTQ_ES_RA_03_00_REG_RA02_SPAN_VAL(val)   (((val) & 0x1) << 22)
++#define LTQ_ES_RA_03_00_REG_RA02_SPAN_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_SPAN) >> 22) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA02_SPAN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_SPAN) | (((val) & 0x1) << 22))
++/* Management bit for 0180C2000002 (21) */
++#define LTQ_ES_RA_03_00_REG_RA02_MG   (0x1 << 21)
++#define LTQ_ES_RA_03_00_REG_RA02_MG_VAL(val)   (((val) & 0x1) << 21)
++#define LTQ_ES_RA_03_00_REG_RA02_MG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_MG) >> 21) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA02_MG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_MG) | (((val) & 0x1) << 21))
++/* Cross_VLAN bit for 0180C2000002 (20) */
++#define LTQ_ES_RA_03_00_REG_RA02_CV   (0x1 << 20)
++#define LTQ_ES_RA_03_00_REG_RA02_CV_VAL(val)   (((val) & 0x1) << 20)
++#define LTQ_ES_RA_03_00_REG_RA02_CV_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_CV) >> 20) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA02_CV_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_CV) | (((val) & 0x1) << 20))
++/* TXTAG bit for 0180C2000002 (19:18) */
++#define LTQ_ES_RA_03_00_REG_RA02_TXTAG   (0x3 << 18)
++#define LTQ_ES_RA_03_00_REG_RA02_TXTAG_VAL(val)   (((val) & 0x3) << 18)
++#define LTQ_ES_RA_03_00_REG_RA02_TXTAG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_TXTAG) >> 18) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA02_TXTAG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_TXTAG) | (((val) & 0x3) << 18))
++/* Action bit for 0180C2000002 (17:16) */
++#define LTQ_ES_RA_03_00_REG_RA02_ACT   (0x3 << 16)
++#define LTQ_ES_RA_03_00_REG_RA02_ACT_VAL(val)   (((val) & 0x3) << 16)
++#define LTQ_ES_RA_03_00_REG_RA02_ACT_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA02_ACT) >> 16) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA02_ACT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA02_ACT) | (((val) & 0x3) << 16))
++/* Valid bit for 0180C2000001 (15) */
++#define LTQ_ES_RA_03_00_REG_RA01_VALID   (0x1 << 15)
++#define LTQ_ES_RA_03_00_REG_RA01_VALID_VAL(val)   (((val) & 0x1) << 15)
++#define LTQ_ES_RA_03_00_REG_RA01_VALID_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_VALID) >> 15) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA01_VALID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_VALID) | (((val) & 0x1) << 15))
++/* Span bit for 0180C2000001 (14) */
++#define LTQ_ES_RA_03_00_REG_RA01_SPAN   (0x1 << 14)
++#define LTQ_ES_RA_03_00_REG_RA01_SPAN_VAL(val)   (((val) & 0x1) << 14)
++#define LTQ_ES_RA_03_00_REG_RA01_SPAN_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_SPAN) >> 14) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA01_SPAN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_SPAN) | (((val) & 0x1) << 14))
++/* Management bit for 0180C2000001 (13) */
++#define LTQ_ES_RA_03_00_REG_RA01_MG   (0x1 << 13)
++#define LTQ_ES_RA_03_00_REG_RA01_MG_VAL(val)   (((val) & 0x1) << 13)
++#define LTQ_ES_RA_03_00_REG_RA01_MG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_MG) >> 13) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA01_MG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_MG) | (((val) & 0x1) << 13))
++/* Cross_VLAN bit for 0180C2000001 (12) */
++#define LTQ_ES_RA_03_00_REG_RA01_CV   (0x1 << 12)
++#define LTQ_ES_RA_03_00_REG_RA01_CV_VAL(val)   (((val) & 0x1) << 12)
++#define LTQ_ES_RA_03_00_REG_RA01_CV_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_CV) >> 12) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA01_CV_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_CV) | (((val) & 0x1) << 12))
++/* TXTAG bit for 0180C2000001 (11:10) */
++#define LTQ_ES_RA_03_00_REG_RA01_TXTAG   (0x3 << 10)
++#define LTQ_ES_RA_03_00_REG_RA01_TXTAG_VAL(val)   (((val) & 0x3) << 10)
++#define LTQ_ES_RA_03_00_REG_RA01_TXTAG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_TXTAG) >> 10) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA01_TXTAG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_TXTAG) | (((val) & 0x3) << 10))
++/* Action bit for 0180C2000001 (9:8) */
++#define LTQ_ES_RA_03_00_REG_RA01_ACT   (0x3 << 8)
++#define LTQ_ES_RA_03_00_REG_RA01_ACT_VAL(val)   (((val) & 0x3) << 8)
++#define LTQ_ES_RA_03_00_REG_RA01_ACT_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA01_ACT) >> 8) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA01_ACT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA01_ACT) | (((val) & 0x3) << 8))
++/* Valid bit for 0180C2000000 (7) */
++#define LTQ_ES_RA_03_00_REG_RA00_VALID   (0x1 << 7)
++#define LTQ_ES_RA_03_00_REG_RA00_VALID_VAL(val)   (((val) & 0x1) << 7)
++#define LTQ_ES_RA_03_00_REG_RA00_VALID_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_VALID) >> 7) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA00_VALID_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_VALID) | (((val) & 0x1) << 7))
++/* Span bit for 0180C2000000 (6) */
++#define LTQ_ES_RA_03_00_REG_RA00_SPAN   (0x1 << 6)
++#define LTQ_ES_RA_03_00_REG_RA00_SPAN_VAL(val)   (((val) & 0x1) << 6)
++#define LTQ_ES_RA_03_00_REG_RA00_SPAN_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_SPAN) >> 6) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA00_SPAN_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_SPAN) | (((val) & 0x1) << 6))
++/* Management bit for 0180C2000000 (5) */
++#define LTQ_ES_RA_03_00_REG_RA00_MG   (0x1 << 5)
++#define LTQ_ES_RA_03_00_REG_RA00_MG_VAL(val)   (((val) & 0x1) << 5)
++#define LTQ_ES_RA_03_00_REG_RA00_MG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_MG) >> 5) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA00_MG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_MG) | (((val) & 0x1) << 5))
++/* Cross_VLAN bit for 0180C2000000 (4) */
++#define LTQ_ES_RA_03_00_REG_RA00_CV   (0x1 << 4)
++#define LTQ_ES_RA_03_00_REG_RA00_CV_VAL(val)   (((val) & 0x1) << 4)
++#define LTQ_ES_RA_03_00_REG_RA00_CV_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_CV) >> 4) & 0x1)
++#define LTQ_ES_RA_03_00_REG_RA00_CV_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_CV) | (((val) & 0x1) << 4))
++/* TXTAG bit for 0180C2000000 (3:2) */
++#define LTQ_ES_RA_03_00_REG_RA00_TXTAG   (0x3 << 2)
++#define LTQ_ES_RA_03_00_REG_RA00_TXTAG_VAL(val)   (((val) & 0x3) << 2)
++#define LTQ_ES_RA_03_00_REG_RA00_TXTAG_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_TXTAG) >> 2) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA00_TXTAG_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_TXTAG) | (((val) & 0x3) << 2))
++/* Action bit for 0180C2000000 (1:0) */
++#define LTQ_ES_RA_03_00_REG_RA00_ACT   (0x3)
++#define LTQ_ES_RA_03_00_REG_RA00_ACT_VAL(val)   (((val) & 0x3) << 0)
++#define LTQ_ES_RA_03_00_REG_RA00_ACT_GET(val)   ((((val) & LTQ_ES_RA_03_00_REG_RA00_ACT) >> 0) & 0x3)
++#define LTQ_ES_RA_03_00_REG_RA00_ACT_SET(reg,val) (reg) = ((reg & ~LTQ_ES_RA_03_00_REG_RA00_ACT) | (((val) & 0x3) << 0))
++
++/*******************************************************************************
++ * Protocol Filter 0
++ ******************************************************************************/
++
++/* Value Compared with Protocol in IP Header (31:24) */
++#define LTQ_ES_PRTCL_F0_REG_PFR3   (0xff << 24)
++#define LTQ_ES_PRTCL_F0_REG_PFR3_VAL(val)   (((val) & 0xff) << 24)
++#define LTQ_ES_PRTCL_F0_REG_PFR3_GET(val)   ((((val) & LTQ_ES_PRTCL_F0_REG_PFR3) >> 24) & 0xff)
++#define LTQ_ES_PRTCL_F0_REG_PFR3_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_F0_REG_PFR3) | (((val) & 0xff) << 24))
++/* Value Compared with Protocol in IP Header (23:16) */
++#define LTQ_ES_PRTCL_F0_REG_PFR2   (0xff << 16)
++#define LTQ_ES_PRTCL_F0_REG_PFR2_VAL(val)   (((val) & 0xff) << 16)
++#define LTQ_ES_PRTCL_F0_REG_PFR2_GET(val)   ((((val) & LTQ_ES_PRTCL_F0_REG_PFR2) >> 16) & 0xff)
++#define LTQ_ES_PRTCL_F0_REG_PFR2_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_F0_REG_PFR2) | (((val) & 0xff) << 16))
++/* Value Compared with Protocol in IP Header (15:8) */
++#define LTQ_ES_PRTCL_F0_REG_PFR1   (0xff << 8)
++#define LTQ_ES_PRTCL_F0_REG_PFR1_VAL(val)   (((val) & 0xff) << 8)
++#define LTQ_ES_PRTCL_F0_REG_PFR1_GET(val)   ((((val) & LTQ_ES_PRTCL_F0_REG_PFR1) >> 8) & 0xff)
++#define LTQ_ES_PRTCL_F0_REG_PFR1_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_F0_REG_PFR1) | (((val) & 0xff) << 8))
++/* Value Compared with Protocol in IP Header (7:0) */
++#define LTQ_ES_PRTCL_F0_REG_PFR0   (0xff)
++#define LTQ_ES_PRTCL_F0_REG_PFR0_VAL(val)   (((val) & 0xff) << 0)
++#define LTQ_ES_PRTCL_F0_REG_PFR0_GET(val)   ((((val) & LTQ_ES_PRTCL_F0_REG_PFR0) >> 0) & 0xff)
++#define LTQ_ES_PRTCL_F0_REG_PFR0_SET(reg,val) (reg) = ((reg & ~LTQ_ES_PRTCL_F0_REG_PFR0) | (((val) & 0xff) << 0))
++
++#endif
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+--- a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
++++ b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+@@ -68,7 +68,8 @@ static inline int ltq_is_grx390(void)
+ #define LTQ_ASC0_BASE_ADDR			0x14100100
+ #define LTQ_ASC1_BASE_ADDR			0x14100200
+ #define LTQ_STATUS_BASE_ADDR		0x1E000500
+-#define LTQ_SYS1_BASE_ADDR			0x1C000800 
++#define LTQ_ES_BASE_ADDR			0x18000000
++#define LTQ_SYS1_BASE_ADDR			0x1C000800
+ 
+ #define SYS0_PLL1CR	0x0008
+ #define SYS0_PLL1CR_PLLDIV   (0x3)
+@@ -78,6 +79,7 @@ static inline int ltq_is_grx390(void)
+ #define SYS1_FPICR	0x0014
+ #define SYS1_CLKENR	0x0004
+ #define SYS1_CLKCLR	0x0008
++#define SYS1_CLKENR_ETHSW (0x1 << 7)
+ #define SYS1_CLKENR_DMA (0x1 << 9)
+ #define SYS1_CLKENR_ASC0 (0x1 << 14)
+ #define SYS1_CLKENR_ASC1 (0x1 << 15)
+diff --git a/arch/mips/lantiq/svip/Makefile b/arch/mips/lantiq/svip/Makefile
+--- a/arch/mips/lantiq/svip/Makefile
++++ b/arch/mips/lantiq/svip/Makefile
+@@ -1,1 +1,1 @@
+-obj-y := prom.o reset.o sysctrl.o dma.o
++obj-y := prom.o reset.o sysctrl.o dma.o switchip_setup.o
+diff --git a/arch/mips/lantiq/svip/switchip_setup.c b/arch/mips/lantiq/svip/switchip_setup.c
+new file mode 100644
+--- /dev/null
++++ b/arch/mips/lantiq/svip/switchip_setup.c
+@@ -0,0 +1,678 @@
++/******************************************************************************
++
++                               Copyright (c) 2012
++                            Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++*******************************************************************************
++   Module      : switchip_setup.c
++   Date        : 2007-11-09
++   Description : Basic setup of embedded ethernet switch "SwitchIP"
++   Remarks     : andreas.schmidt@infineon.com
++ *****************************************************************************/
++
++/* TODO: get rid of #ifdef CONFIG_LANTIQ_MACH_EASY336 */
++
++#include <linux/kernel.h>
++#include <linux/module.h>
++#include <linux/version.h>
++#include <linux/init.h>
++#include <linux/delay.h>
++#include <linux/workqueue.h>
++#include <linux/time.h>
++
++#include <lantiq_soc.h>
++#include <es.h>
++#if 0
++#include <sys1_reg.h>
++#endif
++#include <dma.h>
++
++static struct svip_reg_es *const es = (struct svip_reg_es *)(KSEG1 + LTQ_ES_BASE_ADDR);
++
++/* PHY Organizationally Unique Identifier (OUI) */
++#define PHY_OUI_PMC           0x00E004
++#define PHY_OUI_VITESSE       0x008083
++#define PHY_OUI_DEFAULT       0xFFFFFF
++
++unsigned short switchip_phy_read(unsigned int phyaddr, unsigned int regaddr);
++void switchip_phy_write(unsigned int phyaddr, unsigned int regaddr,
++			unsigned short data);
++
++static int phy_address[2] = {0, 1};
++static u32 phy_oui;
++static void switchip_mdio_poll_init(void);
++static void _switchip_mdio_poll(struct work_struct *work);
++
++/* struct workqueue_struct mdio_poll_task; */
++static struct workqueue_struct *mdio_poll_workqueue;
++DECLARE_DELAYED_WORK(mdio_poll_work, _switchip_mdio_poll);
++static int old_link_status[2] = {-1, -1};
++
++/**
++ * Autonegotiation check.
++ * This function checks for link changes. If a link change has occurred it will
++ * update certain switch registers.
++ */
++static void _switchip_check_phy_status(int port)
++{
++	int new_link_status;
++	unsigned short reg1;
++
++	reg1 = switchip_phy_read(phy_address[port], 1);
++	if ((reg1 == 0xFFFF) || (reg1 == 0x0000))
++		return; /* no PHY connected */
++
++	new_link_status = reg1 & 4;
++	if (old_link_status[port] ^ new_link_status) {
++		/* link status change */
++		if (!new_link_status) {
++			if (port == 0)
++				es_w32_mask(LTQ_ES_P0_CTL_REG_FLP, 0, p0_ctl);
++			else
++				es_w32_mask(LTQ_ES_P0_CTL_REG_FLP, 0, p1_ctl);
++
++			/* read again; link bit is latched low! */
++			reg1 = switchip_phy_read(phy_address[port], 1);
++			new_link_status = reg1 & 4;
++		}
++
++		if (new_link_status) {
++			unsigned short reg0, reg4, reg5, reg9, reg10;
++			int phy_pause, phy_speed, phy_duplex;
++			int aneg_enable, aneg_cmpt;
++
++			reg0 = switchip_phy_read(phy_address[port], 0);
++			reg4 = switchip_phy_read(phy_address[port], 4);
++			aneg_enable = reg0 & 0x1000;
++			aneg_cmpt = reg1 & 0x20;
++
++			if (aneg_enable && aneg_cmpt) {
++				reg5 = switchip_phy_read(phy_address[port], 5);
++				switch (phy_oui) {
++#ifdef CONFIG_LANTIQ_MACH_EASY336
++				case PHY_OUI_PMC:
++					/* PMC Sierra supports 1Gigabit FD,
++					 * only. On successful
++					 * auto-negotiation, we are sure this
++					 * is what the LP can. */
++					phy_pause = ((reg4 & reg5) & 0x0080) >> 7;
++					phy_speed = 2;
++					phy_duplex = 1;
++					break;
++#endif
++				case PHY_OUI_VITESSE:
++				case PHY_OUI_DEFAULT:
++					reg9 = switchip_phy_read(phy_address[port], 9);
++					reg10 = switchip_phy_read(phy_address[port], 10);
++
++					/* Check if advertise and partner
++					 * agree on pause */
++					phy_pause = ((reg4 & reg5) & 0x0400) >> 10;
++
++					/* Find the best mode both partners
++					 * support
++					 * Priority: 1GB-FD, 1GB-HD, 100MB-FD,
++					 * 100MB-HD, 10MB-FD, 10MB-HD */
++					phy_speed = ((((reg9<<2) & reg10)
++								& 0x0c00) >> 6) |
++						(((reg4 & reg5) & 0x01e0) >> 5);
++
++					if (phy_speed >= 0x0020) {
++						phy_speed = 2;
++						phy_duplex = 1;
++					} else if (phy_speed >= 0x0010) {
++						phy_speed = 2;
++						phy_duplex = 0;
++					} else if (phy_speed >= 0x0008) {
++						phy_speed = 1;
++						phy_duplex = 1;
++					} else if (phy_speed >= 0x0004) {
++						phy_speed = 1;
++						phy_duplex = 0;
++					} else if (phy_speed >= 0x0002) {
++						phy_speed = 0;
++						phy_duplex = 1;
++					} else {
++						phy_speed = 0;
++						phy_duplex = 0;
++					}
++					break;
++				default:
++					phy_pause = (reg4 & 0x0400) >> 10;
++					phy_speed = (reg0 & 0x40 ? 2 : (reg0 >> 13)&1);
++					phy_duplex = (reg0 >> 8)&1;
++					break;
++				}
++			} else {
++				/* parallel detection or fixed speed */
++				phy_pause = (reg4 & 0x0400) >> 10;
++				phy_speed = (reg0 & 0x40 ? 2 : (reg0 >> 13)&1);
++				phy_duplex = (reg0 >> 8)&1;
++			}
++
++			if (port == 0) {
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0SPD,
++						 LTQ_ES_RGMII_CTL_REG_P0SPD_VAL(phy_speed),
++						 rgmii_ctl);
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0DUP,
++						 LTQ_ES_RGMII_CTL_REG_P0DUP_VAL(phy_duplex),
++						 rgmii_ctl);
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0FCE,
++						 LTQ_ES_RGMII_CTL_REG_P0FCE_VAL(phy_pause),
++						 rgmii_ctl);
++
++				es_w32_mask(0, LTQ_ES_P0_CTL_REG_FLP, p0_ctl);
++			} else {
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1SPD,
++						 LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(phy_speed),
++						 rgmii_ctl);
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1DUP,
++						 LTQ_ES_RGMII_CTL_REG_P1DUP_VAL(phy_duplex),
++						 rgmii_ctl);
++				es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1FCE,
++						 LTQ_ES_RGMII_CTL_REG_P0FCE_VAL(phy_pause),
++						 rgmii_ctl);
++
++				es_w32_mask(1, LTQ_ES_P0_CTL_REG_FLP, p1_ctl);
++			}
++		}
++	}
++	old_link_status[port] = new_link_status;
++}
++
++static void _switchip_mdio_poll(struct work_struct *work)
++{
++	if (es_r32(sw_gctl0) & LTQ_ES_SW_GCTL0_REG_SE) {
++		_switchip_check_phy_status(0);
++		_switchip_check_phy_status(1);
++	}
++
++	queue_delayed_work(mdio_poll_workqueue, &mdio_poll_work, HZ/2);
++}
++
++static void switchip_mdio_poll_init(void)
++{
++	mdio_poll_workqueue = create_workqueue("SVIP MDIP poll");
++	INIT_DELAYED_WORK(&mdio_poll_work, _switchip_mdio_poll);
++
++	queue_delayed_work(mdio_poll_workqueue, &mdio_poll_work, HZ/2);
++
++}
++
++unsigned short switchip_phy_read(unsigned int phyaddr, unsigned int regaddr)
++{
++	/* TODO: protect MDIO access with semaphore */
++	es_w32(LTQ_ES_MDIO_CTL_REG_MBUSY
++			 | LTQ_ES_MDIO_CTL_REG_OP_VAL(2) /* read operation */
++			 | LTQ_ES_MDIO_CTL_REG_PHYAD_VAL(phyaddr)
++			 | LTQ_ES_MDIO_CTL_REG_REGAD_VAL(regaddr), mdio_ctl);
++	while (es_r32(mdio_ctl) & LTQ_ES_MDIO_CTL_REG_MBUSY);
++
++	return es_r32(mdio_data) & 0xFFFF;
++}
++EXPORT_SYMBOL(switchip_phy_read);
++
++void switchip_phy_write(unsigned int phyaddr, unsigned int regaddr,
++			unsigned short data)
++{
++	/* TODO: protect MDIO access with semaphore */
++	es_w32(LTQ_ES_MDIO_CTL_REG_WD_VAL(data)
++			 | LTQ_ES_MDIO_CTL_REG_MBUSY
++			 | LTQ_ES_MDIO_CTL_REG_OP_VAL(1) /* write operation */
++			 | LTQ_ES_MDIO_CTL_REG_PHYAD_VAL(phyaddr)
++			 | LTQ_ES_MDIO_CTL_REG_REGAD_VAL(regaddr), mdio_ctl);
++	while (es_r32(mdio_ctl) & LTQ_ES_MDIO_CTL_REG_MBUSY);
++
++	return;
++}
++EXPORT_SYMBOL(switchip_phy_write);
++
++const static u32 switch_reset_offset_000[] = {
++	/*b8000000:*/ 0xffffffff, 0x00000001, 0x00000001, 0x00000003,
++	/*b8000010:*/ 0x04070001, 0x04070001, 0x04070001, 0xffffffff,
++	/*b8000020:*/ 0x00001be8, 0x00001be8, 0x00001be8, 0xffffffff,
++	/*b8000030:*/ 0x00000000, 0x00000000, 0x00080004, 0x00020001,
++	/*b8000040:*/ 0x00000000, 0x00000000, 0x00080004, 0x00020001,
++	/*b8000050:*/ 0x00000000, 0x00000000, 0x00080004, 0x00020001,
++	/*b8000060:*/ 0x00000000, 0x00000000, 0x00081000, 0x001f7777,
++	/*b8000070:*/ 0x00000000, 0x00000000, 0x0c00ac2b, 0x0000fa50,
++	/*b8000080:*/ 0x00001000, 0x00001800, 0x00000000, 0x00000000,
++	/*b8000090:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b80000a0:*/ 0x00000000, 0x00000050, 0x00000010, 0x00000000,
++	/*b80000b0:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b80000c0:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b80000d0:*/ 0xffffffff, 0x00000000, 0x00000000
++};
++const static u32 switch_reset_offset_100[] = {
++	/*b8000100:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000110:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000120:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000130:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000140:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000150:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000160:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000170:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000180:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b8000190:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b80001a0:*/ 0x00000000, 0x00000000, 0x00000000, 0x00000000,
++	/*b80001b0:*/ 0x00000000, 0x00000000
++};
++
++/*
++ * Switch Reset.
++ */
++void switchip_reset(void)
++{
++	volatile unsigned int *reg;
++	volatile unsigned int rdreg;
++	int i;
++   struct clk *clk;
++
++   /*Enable Switch Power  */
++	clk = clk_get_sys("1e108000.eth", NULL);
++	clk_enable(clk);
++
++	/* disable P0 */
++	es_w32_mask(0, LTQ_ES_P0_CTL_REG_SPS_VAL(1), p0_ctl);
++	/* disable P1 */
++	es_w32_mask(0, LTQ_ES_P0_CTL_REG_SPS_VAL(1), p1_ctl);
++	/* disable P2 */
++	es_w32_mask(0, LTQ_ES_P0_CTL_REG_SPS_VAL(1), p2_ctl);
++
++	/**************************************
++	 * BEGIN: Procedure to clear MAC table
++	 **************************************/
++	for (i = 0; i < 3; i++) {
++		int result;
++
++		/* check if access engine is available */
++		while (es_r32(adr_tb_st2) & LTQ_ES_ADR_TB_ST2_REG_BUSY);
++
++		/* initialise to first address */
++		es_w32(LTQ_ES_ADR_TB_CTL2_REG_CMD_VAL(3)
++				 | LTQ_ES_ADR_TB_CTL2_REG_AC_VAL(0), adr_tb_ctl2);
++
++		/* wait while busy */
++		while (es_r32(adr_tb_st2) & LTQ_ES_ADR_TB_ST2_REG_BUSY);
++
++		/* setup the portmap */
++		es_w32_mask(0, LTQ_ES_ADR_TB_CTL1_REG_PMAP_VAL(1 << i),
++				 adr_tb_ctl1);
++
++		do {
++			/* search for addresses by port */
++			es_w32(LTQ_ES_ADR_TB_CTL2_REG_CMD_VAL(2)
++					 | LTQ_ES_ADR_TB_CTL2_REG_AC_VAL(9), adr_tb_ctl2);
++
++			/* wait while busy */
++			while (es_r32(adr_tb_st2) & LTQ_ES_ADR_TB_ST2_REG_BUSY);
++
++			result = LTQ_ES_ADR_TB_ST2_REG_RSLT_GET(es_r32(adr_tb_st2));
++			if (result == 0x101) {
++				printk(KERN_ERR "%s, cmd error\n", __func__);
++				return;
++			}
++			/* if Command OK, address found... */
++			if (result == 0) {
++				unsigned char mac[6];
++
++				mac[5] = (es_r32(adr_tb_st0) >> 0) & 0xff;
++				mac[4] = (es_r32(adr_tb_st0) >> 8) & 0xff;
++				mac[3] = (es_r32(adr_tb_st0) >> 16) & 0xff;
++				mac[2] = (es_r32(adr_tb_st0) >> 24) & 0xff;
++				mac[1] = (es_r32(adr_tb_st1) >> 0) & 0xff;
++				mac[0] = (es_r32(adr_tb_st1) >> 8) & 0xff;
++
++				/* setup address */
++				es_w32((mac[5] << 0) |
++						 (mac[4] << 8) |
++						 (mac[3] << 16) |
++						 (mac[2] << 24), adr_tb_ctl0);
++				es_w32(LTQ_ES_ADR_TB_CTL1_REG_PMAP_VAL(1<<i) |
++						 LTQ_ES_ADR_TB_CTL1_REG_FID_VAL(0) |
++						 (mac[0] << 8) |
++						 (mac[1] << 0), adr_tb_ctl1);
++				/* erase address */
++
++				es_w32(LTQ_ES_ADR_TB_CTL2_REG_CMD_VAL(1) |
++						 LTQ_ES_ADR_TB_CTL2_REG_AC_VAL(15),
++						 adr_tb_ctl2);
++
++				/* wait, while busy */
++				while (es_r32(adr_tb_st2) &
++						 LTQ_ES_ADR_TB_ST2_REG_BUSY);
++			}
++		} while (result == 0);
++	}
++	/**************************************
++	 * END: Procedure to clear MAC table
++	 **************************************/
++
++	/* reset RMON counters */
++	es_w32(LTQ_ES_RMON_CTL_REG_BAS | LTQ_ES_RMON_CTL_REG_CAC_VAL(3),
++			 rmon_ctl);
++
++	/* bring all registers to reset state */
++	reg = LTQ_ES_PS_REG;
++	for (i = 0; i < ARRAY_SIZE(switch_reset_offset_000); i++) {
++		if ((reg == LTQ_ES_PS_REG) ||
++			 (reg >= LTQ_ES_ADR_TB_CTL0_REG &&
++			  reg <= LTQ_ES_ADR_TB_ST2_REG))
++			continue;
++
++		if (switch_reset_offset_000[i] != 0xFFFFFFFF) {
++			/* write reset value to register */
++			*reg = switch_reset_offset_000[i];
++			/* read register value back */
++			rdreg = *reg;
++			if (reg == LTQ_ES_SW_GCTL1_REG)
++				rdreg &= ~LTQ_ES_SW_GCTL1_REG_BISTDN;
++			/* compare read value with written one */
++			if (rdreg != switch_reset_offset_000[i]) {
++				printk(KERN_ERR "%s,%d: reg %08x mismatch "
++						 "[has:%08x, expect:%08x]\n",
++						 __func__, __LINE__,
++						 (unsigned int)reg, rdreg,
++						 switch_reset_offset_000[i]);
++			}
++		}
++		reg++;
++	}
++
++	reg = LTQ_ES_VLAN_FLT0_REG;
++	for (i = 0; i < ARRAY_SIZE(switch_reset_offset_100); i++) {
++		*reg = switch_reset_offset_100[i];
++		rdreg = *reg;
++		if (rdreg != switch_reset_offset_100[i]) {
++			printk(KERN_ERR "%s,%d: reg %08x mismatch "
++					 "[has:%08x, expect:%08x]\n", __func__, __LINE__,
++					 (unsigned int)reg, rdreg,
++					 switch_reset_offset_100[i]);
++		}
++		reg++;
++	}
++}
++EXPORT_SYMBOL(switchip_reset);
++
++static u32 get_phy_oui(unsigned char phy_addr)
++{
++	u32 oui;
++	int i, bit, byte, shift, w;
++	u16 reg_id[2];
++
++	/* read PHY identifier registers 1 and 2 */
++	reg_id[0] = switchip_phy_read(phy_addr, 2);
++	reg_id[1] = switchip_phy_read(phy_addr, 3);
++
++	oui = 0;
++	w = 1;
++	shift = 7;
++	byte = 1;
++	for (i = 0, bit = 10; i <= 21; i++, bit++) {
++		oui |= ((reg_id[w] & (1<<bit)) ? 1 : 0) << shift;
++		if (!(shift % 8)) {
++			byte++;
++			if (byte == 2)
++				shift = 15;
++			else
++				shift = 21;
++		} else {
++			shift--;
++		}
++		if (w == 1 && bit == 15) {
++			bit = -1;
++			w = 0;
++		}
++	}
++	return oui;
++}
++
++/*
++ * Switch Initialization.
++ */
++int switchip_init(void)
++{
++	int eth_port, phy_present = 0;
++	u16 reg, mode;
++   struct clk *clk;
++
++   /*Enable Switch Power  */
++	clk = clk_get_sys("18000000.eth", NULL);
++	if (IS_ERR(clk))
++		panic("Failed to get ethernet switch clock");
++	clk_enable(clk);
++
++	/* Enable Switch, if not already done so */
++	if ((es_r32(sw_gctl0) & LTQ_ES_SW_GCTL0_REG_SE) == 0)
++		es_w32_mask(0, LTQ_ES_SW_GCTL0_REG_SE, sw_gctl0);
++	/* Wait for completion of MBIST */
++	while (LTQ_ES_SW_GCTL1_REG_BISTDN_GET(es_r32(sw_gctl1)) == 0);
++
++	switchip_reset();
++
++	mode = LTQ_ES_RGMII_CTL_REG_IS_GET(es_r32(rgmii_ctl));
++	eth_port = (mode == 2 ? 1 : 0);
++
++	/* Set the primary port(port toward backplane) as sniffer port,
++		changing from P2 which is the reset setting */
++	es_w32_mask(LTQ_ES_SW_GCTL0_REG_SNIFFPN,
++			 LTQ_ES_SW_GCTL0_REG_SNIFFPN_VAL(eth_port),
++			 sw_gctl0);
++
++	/* Point MDIO state machine to invalid PHY addresses 8 and 9 */
++	es_w32_mask(0, LTQ_ES_SW_GCTL0_REG_PHYBA, sw_gctl0);
++
++	/* Remove CRC for packets from PMAC to DMA.
++	 * Status header is added for packets from PMAC to DMA.
++	 * Add CRC for packets from DMA to PMAC.
++	 */
++	es_w32(LTQ_ES_PMAC_HD_CTL_RC | LTQ_ES_PMAC_HD_CTL_AS |
++		LTQ_ES_PMAC_HD_CTL_AC, pmac_hd_ctl);
++
++	phy_oui = get_phy_oui(0);
++	switch (phy_oui) {
++#ifdef CONFIG_LANTIQ_MACH_EASY336
++	case PHY_OUI_PMC:
++		phy_address[0] = (mode == 2 ? -1 : 2);
++		phy_address[1] = (mode == 2 ? 2 : -1);
++		break;
++#endif
++	case PHY_OUI_VITESSE:
++	default:
++		phy_oui = PHY_OUI_DEFAULT;
++		phy_address[0] = (mode == 2 ? 1 : 0);
++		phy_address[1] = (mode == 2 ? 0 : 1);
++		break;
++	}
++
++	/****** PORT 0 *****/
++	reg = switchip_phy_read(phy_address[0], 1);
++	if ((reg != 0x0000) && (reg != 0xffff)) {
++		/* PHY connected? */
++		phy_present |= 1;
++		/* Set Rx- and TxDelay in case of RGMII */
++		switch (mode) {
++		case 0: /* *RGMII,RGMII */
++		case 2: /* RGMII,*GMII */
++			/* program clock delay in PHY, not in SVIP */
++
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0RDLY, 0, rgmii_ctl);
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0TDLY, 0, rgmii_ctl);
++			if (phy_oui == PHY_OUI_VITESSE ||
++				 phy_oui == PHY_OUI_DEFAULT) {
++				switchip_phy_write(phy_address[0], 31, 0x0001);
++				switchip_phy_write(phy_address[0], 28, 0xA000);
++				switchip_phy_write(phy_address[0], 31, 0x0000);
++			}
++		default:
++			break;
++		}
++		if (phy_oui == PHY_OUI_VITESSE ||
++			 phy_oui == PHY_OUI_DEFAULT) {
++			/* Program PHY advertisements and
++			 * restart auto-negotiation */
++			switchip_phy_write(phy_address[0], 4, 0x05E1);
++			switchip_phy_write(phy_address[0], 9, 0x0300);
++			switchip_phy_write(phy_address[0], 0, 0x3300);
++		} else {
++			reg = switchip_phy_read(phy_address[1], 0);
++			reg |= 0x1000; /* auto-negotiation enable */
++			switchip_phy_write(phy_address[1], 0, reg);
++			reg |= 0x0200; /* auto-negotiation restart */
++			switchip_phy_write(phy_address[1], 0, reg);
++		}
++	} else {
++		/* Force SWITCH link with highest capability:
++		 * 100M FD for MII
++		 * 1G FD for GMII/RGMII
++		 */
++		switch (mode) {
++		case 1: /* *MII,MII */
++		case 3: /* *MII,RGMII */
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P0SPD_VAL(1),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P0DUP_VAL(1),
++					 rgmii_ctl);
++			break;
++		case 0: /* *RGMII,RGMII */
++		case 2: /* RGMII,*GMII */
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P0SPD_VAL(2),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P0DUP_VAL(1),
++					 rgmii_ctl);
++
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P0RDLY, 0, rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P0TDLY_VAL(2),
++					 rgmii_ctl);
++			break;
++		}
++
++		es_w32_mask(0, LTQ_ES_P0_CTL_REG_FLP, p0_ctl);
++	}
++
++	/****** PORT 1 *****/
++	reg = switchip_phy_read(phy_address[1], 1);
++	if ((reg != 0x0000) && (reg != 0xffff)) {
++		/* PHY connected? */
++		phy_present |= 2;
++		/* Set Rx- and TxDelay in case of RGMII */
++		switch (mode) {
++		case 0: /* *RGMII,RGMII */
++		case 3: /* *MII,RGMII */
++			/* program clock delay in PHY, not in SVIP */
++
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1RDLY, 0, rgmii_ctl);
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1TDLY, 0, rgmii_ctl);
++			if (phy_oui == PHY_OUI_VITESSE ||
++				 phy_oui == PHY_OUI_DEFAULT) {
++				switchip_phy_write(phy_address[1], 31, 0x0001);
++				switchip_phy_write(phy_address[1], 28, 0xA000);
++				switchip_phy_write(phy_address[1], 31, 0x0000);
++			}
++			break;
++		case 2: /* RGMII,*GMII */
++
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(2),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1DUP, rgmii_ctl);
++#ifdef CONFIG_LANTIQ_MACH_EASY336
++			if (phy_oui == PHY_OUI_PMC) {
++				switchip_phy_write(phy_address[1], 24, 0x0510);
++				switchip_phy_write(phy_address[1], 17, 0xA38C);
++				switchip_phy_write(phy_address[1], 17, 0xA384);
++			}
++#endif
++			break;
++		default:
++			break;
++		}
++		/* Program PHY advertisements and restart auto-negotiation */
++		if (phy_oui == PHY_OUI_VITESSE ||
++			 phy_oui == PHY_OUI_DEFAULT) {
++			switchip_phy_write(phy_address[1], 4, 0x05E1);
++			switchip_phy_write(phy_address[1], 9, 0x0300);
++			switchip_phy_write(phy_address[1], 0, 0x3300);
++		} else {
++			reg = switchip_phy_read(phy_address[1], 0);
++			reg |= 0x1000; /* auto-negotiation enable */
++			switchip_phy_write(phy_address[1], 0, reg);
++			reg |= 0x0200; /* auto-negotiation restart */
++			switchip_phy_write(phy_address[1], 0, reg);
++		}
++	} else {
++		/* Force SWITCH link with highest capability:
++		 * 100M FD for MII
++		 * 1G FD for GMII/RGMII
++		 */
++		switch (mode) {
++		case 1: /* *MII,MII */
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(1),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1DUP, rgmii_ctl);
++			break;
++		case 0: /* *RGMII,RGMII */
++		case 3: /* *MII,RGMII */
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(2),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1DUP, rgmii_ctl);
++			es_w32_mask(LTQ_ES_RGMII_CTL_REG_P1RDLY, 0, rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1TDLY_VAL(2),
++					 rgmii_ctl);
++			break;
++		case 2: /* RGMII,*GMII */
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1SPD_VAL(2),
++					 rgmii_ctl);
++			es_w32_mask(0, LTQ_ES_RGMII_CTL_REG_P1DUP, rgmii_ctl);
++			break;
++		}
++		es_w32_mask(0, LTQ_ES_P0_CTL_REG_FLP, p0_ctl);
++	}
++
++	/*
++	 * Allow unknown unicast/multicast and broadcasts
++	 * on all ports.
++	 */
++
++	es_w32_mask(0, LTQ_ES_SW_GCTL1_REG_UP_VAL(7), sw_gctl1);
++	es_w32_mask(0, LTQ_ES_SW_GCTL1_REG_BP_VAL(7), sw_gctl1);
++	es_w32_mask(0, LTQ_ES_SW_GCTL1_REG_MP_VAL(7), sw_gctl1);
++	es_w32_mask(0, LTQ_ES_SW_GCTL1_REG_RP_VAL(7), sw_gctl1);
++
++	/* user port priority as QoS */
++	/* set priority of port towards external network to lowest */
++	/* set priority of port towards private network to highest */
++	if (eth_port == 0)
++	{
++		es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PP_VAL(0), p0_vlan);
++		es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PP_VAL(2), p1_vlan);
++	}
++	else
++	{
++		es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PP_VAL(2), p0_vlan);
++		es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PP_VAL(0), p1_vlan);
++	}
++	es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PPE, p0_vlan);
++	es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PPE, p1_vlan);
++	/* set port 2 priority to medium high */
++	es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PP_VAL(1), p2_vlan);
++	es_w32_mask(0, LTQ_ES_P0_VLAN_REG_PPE, p2_vlan);
++
++	/* Enable LAN port(s) */
++	if (eth_port == 0)
++		es_w32_mask(LTQ_ES_P0_CTL_REG_SPS, 0, p0_ctl);
++	else
++		es_w32_mask(LTQ_ES_P0_CTL_REG_SPS, 0, p1_ctl);
++	/* Enable CPU Port (Forwarding State) */
++	es_w32_mask(LTQ_ES_P0_CTL_REG_SPS, 0, p2_ctl);
++
++	if (phy_present)
++		switchip_mdio_poll_init();
++
++	return 0;
++}
++EXPORT_SYMBOL(switchip_init);
++
++device_initcall(switchip_init);
+diff --git a/arch/mips/lantiq/svip/sysctrl.c b/arch/mips/lantiq/svip/sysctrl.c
+--- a/arch/mips/lantiq/svip/sysctrl.c
++++ b/arch/mips/lantiq/svip/sysctrl.c
+@@ -225,6 +225,7 @@ void __init ltq_soc_init(void)
+ 	clkdev_add_sys("14100100.serial0", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
+ 	clkdev_add_sys("14100200.serial1", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
+ 	clkdev_add_sys("14104000.dma", SYSCTL_SYS1, SYS1_CLKENR_DMA);
++	clkdev_add_sys("18000000.eth", SYSCTL_SYS1, SYS1_CLKENR_ETHSW);
+ }
+ 
+ /*
+diff --git a/drivers/net/ethernet/Kconfig b/drivers/net/ethernet/Kconfig
+--- a/drivers/net/ethernet/Kconfig
++++ b/drivers/net/ethernet/Kconfig
+@@ -91,6 +91,12 @@ config LANTIQ_XRX200
+ 	---help---
+ 	  Support for the MII0 inside the Lantiq VDSL SoC
+ 
++config LANTIQ_SVIP_ETH_DRV
++	tristate "Lantiq SoC SVIP ethernet driver using Lantiq DMA API"
++	depends on SOC_SVIP
++	---help---
++	  Support for the ethernet switch inside the Lantiq SoC SVIP
++
+ source "drivers/net/ethernet/marvell/Kconfig"
+ source "drivers/net/ethernet/mellanox/Kconfig"
+ source "drivers/net/ethernet/micrel/Kconfig"
+diff --git a/drivers/net/ethernet/Makefile b/drivers/net/ethernet/Makefile
+--- a/drivers/net/ethernet/Makefile
++++ b/drivers/net/ethernet/Makefile
+@@ -37,6 +37,7 @@ obj-$(CONFIG_JME) += jme.o
+ obj-$(CONFIG_KORINA) += korina.o
+ obj-$(CONFIG_LANTIQ_ETOP) += lantiq_etop.o
+ obj-$(CONFIG_LANTIQ_XRX200) += lantiq_xrx200.o
++obj-$(CONFIG_LANTIQ_SVIP_ETH_DRV) += lantiq_svip.o
+ obj-$(CONFIG_NET_VENDOR_MARVELL) += marvell/
+ obj-$(CONFIG_NET_VENDOR_MELLANOX) += mellanox/
+ obj-$(CONFIG_NET_VENDOR_MICREL) += micrel/
+diff --git a/drivers/net/ethernet/lantiq_svip.c b/drivers/net/ethernet/lantiq_svip.c
+new file mode 100644
+--- /dev/null
++++ b/drivers/net/ethernet/lantiq_svip.c
+@@ -0,0 +1,931 @@
++/******************************************************************************
++ **
++ ** FILE NAME    : lantiq_svip.c
++ ** PROJECT      : Lantiq voice co
++ ** MODULES      : Lantiq VINETIC-SVIP ethernet driver
++ ** DATE         : 16 May 2014
++ ** AUTHOR       : Martins Pukitis
++ ** DESCRIPTION  : Lantiq SVIP ethernet device driver
++ ** COPYRIGHT    : Copyright (c) 2014
++ **                Lantiq Deutschland
++ **
++ **    This program is free software; you can redistribute it and/or modify
++ **    it under the terms of the GNU General Public License as published by
++ **    the Free Software Foundation; either version 2 of the License, or
++ **    (at your option) any later version.
++ **
++ **    Adapted from Lantiq Cross-Platform ethernet device driver written by
++ **    Reddy Mallikarjuna, Kishore Kankipati and Suresh Nagaraj.
++ **
++ ** HISTORY
++ ** $Date                $Author              $Comment
++ ** 16 May 2014          Martins Pukitis		Initial version (no ethtool support)
++ *******************************************************************************/
++
++#include <linux/version.h>
++#include <linux/module.h>
++#include <linux/platform_device.h>
++#include <linux/interrupt.h>
++#include <linux/kernel.h> /* printk() */
++#include <linux/types.h>  /* size_t */
++#include <linux/etherdevice.h>
++#include <linux/ethtool.h>
++#include <linux/proc_fs.h>
++#include <linux/etherdevice.h> /* eth_type_trans */
++#include <asm/delay.h>
++#include <linux/init.h>
++#include <linux/clk.h>
++
++#include <linux/of_net.h>
++#include <linux/of_mdio.h>
++#include <linux/of_gpio.h>
++
++#define CONFIG_LTQMIPS_DMA
++#define CONFIG_SW_ROUTING_MODE
++
++#ifdef CONFIG_LTQMIPS_DMA
++#include <lantiq_soc.h>
++#include <es.h>
++#include <svip/dma.h>
++#endif /* CONFIG_LTQMIPS_DMA */
++
++#include "lantiq_svip.h"
++
++#ifdef CONFIG_NAPI_ENABLED
++#define CONFIG_IFX_NAPI               1
++#endif
++
++#define DRV_MODULE_NAME             "lantiq_svip"
++#define DRV_MODULE_VERSION          "1.1.1"
++static char version[] =
++DRV_MODULE_NAME ".c:v" DRV_MODULE_VERSION;
++
++#define SVIP_ETH_DRV_PROC_ENTRY_NAME   "3port_sw"
++
++/* length of time before we decide the hardware is borked,
++ * and dev->eth_tx_timeout() should be called to fix the problem
++ */
++#define LTQ_TX_TIMEOUT                  (10 * HZ)
++
++#define DMA_TX_BURST_LEN                DMA_BURSTL_4DW
++#define DMA_RX_BURST_LEN                DMA_BURSTL_4DW
++#define ETH_PKT_BUF_SIZE                1568
++#define NUM_ETH_INF                     2
++
++/** todo: use device tree */
++static struct svip_reg_es * const es = (struct svip_reg_es *) (KSEG1
++	+ LTQ_ES_BASE_ADDR);
++
++#ifdef CONFIG_LTQMIPS_DMA
++struct dma_device_info *g_dma_device = NULL;
++#endif
++
++static struct net_device *eth_dev[NUM_ETH_INF];
++static int g_pmac_dma;
++/* /proc file to debug */
++//#define SNMP_COUNTERS_DEBUG
++#undef SNMP_COUNTERS_DEBUG
++struct proc_dir_entry* g_eth_proc_dir;
++/* Start the  network device interface queue */
++static int ltq_eth_open(struct net_device *dev);
++/* Stop the  network device interface queue */
++static int ltq_eth_close(struct net_device *dev);
++/* Transmit packet from Tx Queue to MAC */
++static int ltq_start_xmit(struct sk_buff *skb, struct net_device *dev);
++/* Hardware specific IOCTL's  */
++static int ltq_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
++/* Get the network device statistics */
++static struct net_device_stats *ltq_get_stats(struct net_device *dev);
++/* change MTU values */
++static int ltq_change_mtu(struct net_device *dev, int new_mtu);
++/*  Set mac address*/
++static int ltq_set_mac_address(struct net_device *dev, void *p);
++/* Transmit timeout*/
++static void ltq_tx_timeout(struct net_device *dev);
++/* select the tx dma channel */
++static int select_tx_chan(struct sk_buff *skb, struct net_device *dev);
++/*open dma rx channel*/
++static void enable_dma_channel(void);
++/*close the dma rx channel*/
++static void disable_dma_channel(void);
++/* Init of the network device */
++static int ltq_switch_init(struct net_device *dev);
++/* Get the ether addr from u-boot */
++static unsigned char my_ethaddr[MAX_ADDR_LEN];
++#ifdef  CONFIG_IFX_NAPI
++static ltq_eth_fw_poll_ret_t ltq_switch_poll(struct net_device *poll_dev, int work_to_do, int *work_done);
++#endif
++
++static int rversion_open(struct inode *inode, struct file *file);
++static int eth_proc_version(struct seq_file *m, void *v);
++
++struct file_operations rversion_ops = { .open = rversion_open, .read = seq_read,
++			.llseek = seq_lseek, .release = single_release, };
++
++static struct net_device_ops ltq_eth_drv_ops = { .ndo_init = ltq_switch_init,
++			.ndo_open = ltq_eth_open, .ndo_stop = ltq_eth_close,
++			.ndo_start_xmit = ltq_start_xmit, .ndo_set_mac_address =
++				ltq_set_mac_address, .ndo_change_mtu =
++				ltq_change_mtu, .ndo_get_stats = ltq_get_stats,
++			.ndo_do_ioctl = ltq_ioctl, .ndo_tx_timeout =
++				ltq_tx_timeout,
++#ifdef  CONFIG_IFX_NAPI
++	.poll = ltq_switch_poll,
++	.weight = 25,
++#endif
++};
++
++/* #define DUMP_PACKET */
++
++#ifdef DUMP_PACKET
++/*
++ * \brief	dump skb data
++ * \param[in] len length of the data buffer
++ * \param[in] pData Pointer to data to dump
++ *
++ * \return void No Value
++ */
++static inline void dump_skb(u32 len, char *pData)
++{
++	int i;
++	for (i = 0; i < len; i++) {
++		printk("%2.2x ", (u8) (pData[i]));
++		if (i % 16 == 15)
++			printk("\n");
++	}
++	printk("\n");
++}
++#endif
++
++/* Get the driver information, used by ethtool_ops  */
++static void get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)
++{
++	/* driver driver short name (Max 32 characters) */
++	strcpy(info->driver, DRV_MODULE_NAME);
++	/* driver version (Max 32 characters) */
++	strcpy(info->version, DRV_MODULE_VERSION);
++}
++
++/* Get the network device interface number */
++static int get_network_dev_num(struct net_device *dev)
++{
++	int dev_index = ((!strcmp(dev->name, "eth0")) ? 0 :
++			 (!strcmp(dev->name, "eth1")) ? 1 :
++			 (!strcmp(dev->name, "eth2")) ? 2 : -1);
++	return dev_index;
++}
++
++/* Get the network device settings  */
++static int get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
++{
++	int dev_index = get_network_dev_num(dev);
++
++	if (dev_index == -1) {
++		/* should not be here*/
++		printk(KERN_ERR "%s[%d]: Dev index error(%d)!!!\n", __func__,
++			__LINE__, dev_index);
++		return -ENODEV;
++	}
++	return -EOPNOTSUPP;
++}
++
++/* Set the network device settings */
++static int set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
++{
++	int dev_index = get_network_dev_num(dev);
++
++	if (dev_index == -1) {
++		/* should not be here*/
++		printk(KERN_ERR "%s[%d]: Dev index error(%d)!!!\n", __func__,
++			__LINE__, dev_index);
++		return -ENODEV;
++	}
++	if (cmd->autoneg == AUTONEG_ENABLE) {
++		/*TODO*/
++		return 0;
++	} else if ((cmd->speed != SPEED_100 && cmd->speed != SPEED_10
++		&& cmd->speed != SPEED_1000)
++		|| (cmd->duplex != DUPLEX_HALF && cmd->duplex != DUPLEX_FULL)) {
++		return -EINVAL;
++	}
++
++	return -EOPNOTSUPP;
++}
++
++/* Reset the device */
++static int nway_reset(struct net_device *dev)
++{
++	/*TODO*/
++	return 0;
++}
++
++/* Structure of the ether tool operation  */
++static const struct ethtool_ops ethtool_ops = { .get_drvinfo = get_drvinfo,
++			.get_settings = get_settings, .set_settings =
++				set_settings, .nway_reset = nway_reset,
++			.get_link = ethtool_op_get_link,
++#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,32)
++	.get_perm_addr = ethtool_op_get_perm_addr,
++#endif
++};
++
++#ifndef MODULE
++/* Get the ether addr from u-boot */
++static int __init ethaddr_setup(char *line)
++{
++	char *ep;
++	int i;
++	memset(my_ethaddr, 0, MAX_ADDR_LEN);
++	/* there should really be routines to do this stuff */
++	for (i = 0; i < 6; i++) {
++		my_ethaddr[i] = line ? simple_strtoul(line, &ep, 16) : 0;
++		if (line)
++			line = (*ep) ? ep + 1 : ep;
++	}
++	printk("mac address %2x-%2x-%2x-%2x-%2x-%2x \n", my_ethaddr[0],
++		my_ethaddr[1], my_ethaddr[2], my_ethaddr[3], my_ethaddr[4],
++		my_ethaddr[5]);
++	return 0;
++}
++__setup("ethaddr=", ethaddr_setup);
++#endif
++
++/* Turn On RX DMA channels */
++static void enable_dma_channel(void)
++{
++#ifdef CONFIG_LTQMIPS_DMA
++	struct dma_device_info* dma_dev = g_dma_device;
++	int i;
++
++	for (i = 0; i < dma_dev->max_rx_chan_num; i++) {
++		if ((dma_dev->rx_chan[i])->control == IFX_DMA_CH_ON)
++			dma_dev->rx_chan[i]->open(dma_dev->rx_chan[i]);
++	}
++#endif
++}
++
++/* Turn Off RX DMA channels */
++static void disable_dma_channel()
++{
++#ifdef CONFIG_LTQMIPS_DMA
++	struct dma_device_info* dma_dev = g_dma_device;
++	int i;
++
++	for (i = 0; i < dma_dev->max_rx_chan_num; i++)
++		dma_dev->rx_chan[i]->close(dma_dev->rx_chan[i]);
++#endif
++}
++
++static int index = 0;
++/* open the network device interface*/
++static int ltq_eth_open(struct net_device *dev)
++{
++	if (index == 0) {
++		enable_dma_channel();
++	}
++	index++;
++	return 0;
++}
++
++/* Close the network device interface*/
++static int ltq_eth_close(struct net_device *dev)
++{
++	if (index)
++		index--;
++	if (index == 0) {
++		disable_dma_channel();
++	}
++	return 0;
++}
++
++/* Send the packet to netwrok rx queue, used by  switch_hw_receive function */
++static void eth_rx(struct net_device *dev, int len, struct sk_buff* skb)
++{
++	ltq_switch_priv_t *priv = netdev_priv(dev);
++
++	skb->dev = dev;
++	skb->protocol = eth_type_trans(skb, dev);
++#ifdef  CONFIG_IFX_NAPI
++	netif_receive_skb(skb);
++#else
++	netif_rx(skb);
++#endif
++	priv->stats.rx_packets++;
++	priv->stats.rx_bytes += len;
++}
++
++/*
++ * This function is called in dma intr handler (DMA RCV_INT interrupt).
++ * This function get the data from the DMA device.
++ *   if the packet is valid then it will send to upper layer based on  criteria.
++ *       The switch CPU port PMAC status header is enabled, then remove the header and
++ *           look from which port the packet comes and send to relative network device.
++ if PMAC status header is not enabled, then send the packets eth0 interafce
++ */
++static void switch_hw_receive(struct dma_device_info* dma_dev)
++{
++	unsigned char* buf = NULL;
++	int len = 0;
++	struct sk_buff *skb = NULL;
++	struct net_device *dev;
++
++#ifdef CONFIG_LTQMIPS_DMA
++	len = dma_device_read(dma_dev, &buf, (void**) &skb);
++#endif
++	if ((len >= 0x600) || (len < ETH_ZLEN)) {
++		printk(KERN_ERR "%s[%d]: Packet is too large/small (%d)!!!\n",
++			__func__,
++			__LINE__, len);
++		goto rx_err_exit;
++	}
++	if (skb == NULL) {
++		printk(KERN_ERR "%s[%d]: Can't restore the Packet !!!\n",
++			__func__,
++			__LINE__);
++		goto rx_err_exit;
++	}
++	if (len > (skb->end - skb->tail)) {
++		printk(KERN_ERR "%s[%d]: len:%d end:%p tail:%p Err!!!\n",
++			__func__,
++			__LINE__, len, skb->end, skb->tail);
++		goto rx_err_exit;
++	}
++#ifdef DUMP_PACKET
++	if (buf) {
++		printk("rx len:%d\n", len);
++		dump_skb(len, (char *) buf);
++	}
++#endif
++	skb_put(skb, len);
++	if (g_pmac_dma) {
++		int sourcePortId;
++
++		len -= 8; /*Remove PMAC to DMA header */
++		skb_pull(skb, 8);
++		memcpy(&sourcePortId, buf + 2, sizeof(sourcePortId));
++		sourcePortId = le32_to_cpu(sourcePortId) >> 22 & 3;
++		switch (sourcePortId) {
++		case 0:
++		case 2:
++			dev = eth_dev[0];
++			break;
++		case 1:
++			dev = eth_dev[1];
++			break;
++		default:
++			/* printk("%s[%d], SPID:%d ERROR!!! \n", __FUNCTION__, __LINE__,
++			 * pkt_hdr.SPID); */
++			goto rx_err_exit;
++		}
++	} else
++		dev = eth_dev[0];
++
++	skb->dev = dev;
++	eth_rx(dev, len, skb);
++	return;
++
++	rx_err_exit: if (skb)
++		dev_kfree_skb_any(skb);
++	return;
++}
++
++/* Get the network device stats information */
++static struct net_device_stats *ltq_get_stats(struct net_device *dev)
++{
++	ltq_switch_priv_t *priv = netdev_priv(dev);
++	return &priv->stats;
++}
++
++/* Trasmit timeout */
++static void ltq_tx_timeout(struct net_device *dev)
++{
++	ltq_switch_priv_t *priv = netdev_priv(dev);
++#ifdef CONFIG_LTQMIPS_DMA
++	struct dma_device_info* dma_dev = g_dma_device;
++	int i;
++
++	printk(KERN_ERR "%s: transmit timed out, disable the dma channel irq\n",
++		dev->name);
++
++	priv->stats.tx_errors++;
++
++	for (i = 0; i < dma_dev->max_tx_chan_num; i++) {
++		dma_dev->tx_chan[i]->disable_irq(dma_dev->tx_chan[i]);
++	}
++	netif_wake_queue(dev);
++#endif
++}
++
++/* Set the MAC address */
++static int ltq_set_mac_address(struct net_device *dev, void *p)
++{
++	struct sockaddr *addr = p;
++
++	if (netif_running(dev))
++		return -EBUSY;
++
++	if (!is_valid_ether_addr(addr->sa_data))
++		return -EINVAL;
++
++	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
++	return 0;
++}
++
++/* Change the MTU value of the netwrok device interfaces */
++static int ltq_change_mtu(struct net_device *dev, int new_mtu)
++{
++	if (new_mtu < ETH_ZLEN || new_mtu > ETH_DATA_LEN)
++		return -EINVAL;
++	dev->mtu = new_mtu;
++	return 0;
++}
++
++/* select the DMA channel numbers, refer dma_setup_init function */
++static int select_tx_chan(struct sk_buff *skb, struct net_device *dev)
++{
++	int chan_no;
++	/*TODO: select the channel based on criteria*/
++	int dev_index = (!strcmp(dev->name, "eth0") ? 0 : 1);
++	if (dev_index)
++		chan_no = 1;
++	else
++		chan_no = 0;
++	return chan_no;
++}
++
++/*
++ * Transmit packet over DMA, which comes from the Tx Queue
++ * Note: Tx packet based on the interface queue.
++ *       if packet comes from eth0, then sendout the packet over Tx DMA channel 0
++ *       if packet comes from eth1, then sendout the packet over Tx DMA channel 1
++ * refer the function "select_tx_chan" selection of dma channels
++ * if switch CPU port PMAC status header is enabled, then set the status header
++ *   based on criteria and push the status header infront of header.
++ * if head room is not availabe for status header(4-bytes), reallocate the head room
++ *   and push status header  infront of the header
++ */
++static int ltq_start_xmit(struct sk_buff *skb, struct net_device *dev)
++{
++	ltq_switch_priv_t *priv = netdev_priv(dev);
++	int len, rc = NETDEV_TX_OK;
++	char *data;
++#ifdef CONFIG_LTQMIPS_DMA
++	struct dma_device_info* dma_dev = g_dma_device;
++#endif
++
++	len = skb->len < ETH_ZLEN ? ETH_ZLEN : skb->len;
++
++	data = skb->data;
++	priv->skb = skb;
++	dev->trans_start = jiffies;
++
++#ifdef CONFIG_LTQMIPS_DMA
++	/*select the tx channel*/
++	dma_dev->current_tx_chan = select_tx_chan(skb, dev);
++
++	if (dma_device_write(dma_dev, data, len, skb) != len) {
++		if (skb)
++			dev_kfree_skb_any(skb);
++		priv->stats.tx_errors++;
++		priv->stats.tx_dropped++;
++		/*      rc = NETDEV_TX_BUSY;  */
++	} else {
++		priv->stats.tx_packets++;
++		priv->stats.tx_bytes += len;
++	}
++#endif
++	return rc;
++}
++
++/* Platform specific IOCTL's handler */
++static int ltq_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
++{
++	/* TODO*/
++
++	return -EOPNOTSUPP;
++}
++
++#ifdef CONFIG_LTQMIPS_DMA
++/*
++ * DMA Pseudo Interrupt handler.
++ * This function handle the DMA pseudo interrupts to handle the data packets Rx/Tx over DMA channels
++ * It will handle the following interrupts
++ *   RCV_INT: DMA receive the packet interrupt,So get from the PPE peripheral
++ *   TX_BUF_FULL_INT: TX channel descriptors are not availabe, so, stop the transmission
++ and enable the Tx channel interrupt.
++ *   TRANSMIT_CPT_INT: Tx channel descriptors are availabe and resume the transmission and
++ disable the Tx channel interrupt.
++ */
++int dma_intr_handler(struct dma_device_info* dma_dev, int status)
++{
++	struct net_device* dev;
++	int i;
++
++	switch (status) {
++	case RCV_INT:
++		switch_hw_receive(dma_dev);
++		break;
++	case TX_BUF_FULL_INT:
++		for (i = 0; i < NUM_ETH_INF; i++) {
++			dev = eth_dev[i];
++			netif_stop_queue(dev);
++		}
++		for (i = 0; i < dma_dev->max_tx_chan_num; i++) {
++			if ((dma_dev->tx_chan[i])->control == IFX_DMA_CH_ON)
++				dma_dev->tx_chan[i]->enable_irq(
++					dma_dev->tx_chan[i]);
++		}
++		break;
++	case TRANSMIT_CPT_INT:
++		for (i = 0; i < dma_dev->max_tx_chan_num; i++) {
++			dma_dev->tx_chan[i]->disable_irq(dma_dev->tx_chan[i]);
++		}
++		for (i = 0; i < NUM_ETH_INF; i++) {
++			dev = eth_dev[i];
++			netif_wake_queue(dev);
++		}
++		break;
++	}
++	return 0;
++}
++#endif /* CONFIG_LTQMIPS_DMA */
++
++/*
++ * Allocates the buffer for ethernet packet.
++ * This function is invoke when DMA callback function to be called
++ *   to allocate a new buffer for Rx packets.*/
++unsigned char* sw_dma_buffer_alloc(int len, int* byte_offset, void** opt)
++{
++	unsigned char *buffer = NULL;
++	struct sk_buff *skb = NULL;
++	int offset = 0;
++#ifdef CONFIG_LTQMIPS_DMA
++	int dma_burst_len = g_dma_device->rx_burst_len << 2;
++#else
++	int dma_burst_len = 4;
++#endif
++	/* for reserving 2 bytes in skb buffer, so, set offset 2 bytes infront of data pointer */
++	*byte_offset = 2;
++	skb = dev_alloc_skb(ETH_PKT_BUF_SIZE + dma_burst_len);
++	if (skb == NULL) {
++		printk(KERN_ERR "%s[%d]: Buffer allocation failed!!!\n",
++			__func__, __LINE__);
++		return NULL;
++	}
++	if (likely(skb)) {
++		if (((u32) skb->data & (dma_burst_len - 1)) != 0) {
++			offset = ~((u32) skb->data + (dma_burst_len - 1))
++				& (dma_burst_len - 1);
++		}
++		if (offset != 0) {
++			buffer = (unsigned char *) (skb->data + offset);
++			skb_reserve(skb, offset);
++		} else {
++			buffer = (unsigned char*) (skb->data);
++		}
++		skb_reserve(skb, 2);
++		*(int*) opt = (int) skb;
++	}
++	return buffer;
++}
++
++/* Free skb buffer
++ * This function frees a buffer previously allocated by the DMA buffer
++ *   alloc callback function. */
++int sw_dma_buffer_free(unsigned char* dataptr, void* opt)
++{
++	struct sk_buff *skb = NULL;
++
++	if (opt == NULL) {
++		if (dataptr)
++			kfree(dataptr);
++	} else {
++		skb = (struct sk_buff*) opt;
++		if (skb)
++			dev_kfree_skb_any(skb);
++	}
++
++	return 0;
++}
++
++#ifdef  CONFIG_IFX_NAPI
++/* This function scheduled from upper layer when the NAPI is enabled*/
++//static int eth_switch_poll(struct net_device *poll_dev, int *budget)
++//{
++//    int work_to_do, work_done, ret;
++//    struct dma_device_info* dma_dev=g_dma_device;
++//
++//    work_to_do = min(*budget, poll_dev->quota);
++//    work_done = 0;
++//    ret = dma_device_poll(dma_dev, work_to_do, &work_done);
++//    *budget -= work_done;
++//    poll_dev->quota -= work_done;
++//    return ret;
++//}
++static eth_fw_poll_ret_t ltq_switch_poll(struct net_device *poll_dev, int work_to_do, int *work_done)
++{
++	int ret;
++
++#ifdef CONFIG_LTQMIPS_DMA
++	ret = dma_device_poll(g_dma_device, work_to_do, work_done);
++	return ret == 0 ? IFX_ETH_FW_POLL_COMPLETE : IFX_ETH_FW_POLL_CONTINUE;
++#endif
++}
++
++static void switch_activate_poll(struct dma_device_info* dma_dev)
++{
++	struct net_device *dev;
++	int i;
++
++	for(i=0; i < NUM_ETH_INF; i++)
++	{
++		dev = eth_dev[i];
++//        if ( netif_rx_schedule_prep(dev) )
++//        __netif_rx_schedule(dev);
++		ltq_eth_fw_poll_schedule(dev);
++	}
++}
++
++static void switch_inactivate_poll(struct dma_device_info* dma_dev)
++{
++	struct net_device *dev;
++	int i;
++
++	for(i=0; i < NUM_ETH_INF; i++)
++	{
++		dev = eth_dev[i];
++//        if(netif_running(dev) )
++//            netif_rx_complete(dev);
++		ltq_eth_fw_poll_complete(dev);
++	}
++}
++#endif
++
++/* Unregister with DMA device core driver */
++static void dma_setup_uninit(void)
++{
++#ifdef CONFIG_LTQMIPS_DMA
++	struct dma_device_info* dma_dev = g_dma_device;
++	if (dma_dev) {
++		dma_device_unregister(dma_dev);
++		dma_device_release(dma_dev);
++	}
++#endif
++}
++
++#ifdef CONFIG_LTQMIPS_DMA
++static int ltqco_dma_setup_init(void)
++{
++	int i, ret = 0;
++
++	g_dma_device = dma_device_reserve("SW");
++	if (!g_dma_device) {
++		printk(
++			KERN_ERR "%s[%d]: Reserved with DMA core driver failed!!!\n",
++			__func__, __LINE__);
++		return -ENODEV;
++	}
++	g_dma_device->buffer_alloc = &sw_dma_buffer_alloc;
++	g_dma_device->buffer_free = &sw_dma_buffer_free;
++	g_dma_device->intr_handler = &dma_intr_handler;
++	g_dma_device->num_rx_chan = 4;
++	g_dma_device->num_tx_chan = 2;
++	g_dma_device->tx_burst_len = DMA_TX_BURST_LEN;
++	g_dma_device->rx_burst_len = DMA_RX_BURST_LEN;
++#ifdef CONFIG_CPU_LITTLE_ENDIAN
++	g_dma_device->tx_endianness_mode = IFX_DMA_ENDIAN_TYPE0;
++	g_dma_device->rx_endianness_mode = IFX_DMA_ENDIAN_TYPE0;
++#else
++	g_dma_device->tx_endianness_mode = IFX_DMA_ENDIAN_TYPE3;
++	g_dma_device->rx_endianness_mode = IFX_DMA_ENDIAN_TYPE3;
++#endif
++	g_dma_device->port_packet_drop_enable = 0;
++
++	for (i = 0; i < g_dma_device->num_rx_chan; i++) {
++		g_dma_device->rx_chan[i]->packet_size = ETH_PKT_BUF_SIZE;
++		g_dma_device->rx_chan[i]->control = IFX_DMA_CH_ON;
++	}
++	for (i = 0; i < g_dma_device->num_tx_chan; i++) {
++		if ((i == 0) || (i == 1)) /* eth0 --> DMA Tx0 channel, eth1--> DMA Tx1 channel*/
++			g_dma_device->tx_chan[i]->control = IFX_DMA_CH_ON;
++		else
++			g_dma_device->tx_chan[i]->control = IFX_DMA_CH_OFF;
++	}
++#ifdef  CONFIG_IFX_NAPI
++	g_dma_device->activate_poll = switch_activate_poll;
++	g_dma_device->inactivate_poll = switch_inactivate_poll;
++#endif
++	ret = dma_device_register(g_dma_device);
++	if (ret != 0)
++		printk(
++			KERN_ERR "%s[%d]: Register with DMA core driver Failed!!!\n",
++			__func__, __LINE__);
++
++	return ret;
++}
++#endif /* CONFIG_LTQMIPS_DMA */
++
++/* init of the network device */
++static int ltq_switch_init(struct net_device *dev)
++{
++	u64 retval;
++	static int macVal = 0;
++	int i;
++
++	/*printk("%s up\n", dev->name); */
++
++	SET_ETHTOOL_OPS(dev, &ethtool_ops);
++
++	for (i = 0, retval = 0; i < 6; i++)
++		retval += dev->dev_addr[i];
++	if (retval == 0) {
++		/*read the mac address from the mac table and put them into the mac table.*/
++		for (i = 0; i < 6; i++) {
++			retval += my_ethaddr[i];
++		}
++		/* if ethaddr not set in u-boot, then use default one */
++		if (retval == 0) {
++			dev->dev_addr[0] = 0x00;
++			dev->dev_addr[1] = 0x20;
++			dev->dev_addr[2] = 0xda;
++			dev->dev_addr[3] = 0x86;
++			dev->dev_addr[4] = 0x23;
++			dev->dev_addr[5] = 0x74 + macVal;
++		} else {
++			for (i = 0; i < 6; i++) {
++				dev->dev_addr[i] = my_ethaddr[i];
++			}
++			dev->dev_addr[5] += +macVal;
++		}
++		macVal++;
++	}
++	return 0;
++}
++
++/* Driver version info */
++static inline int eth_drv_ver(char *buf)
++{
++	return sprintf(buf,
++		"Lantiq ethernet driver for SVIP, version %s,(c) 2014 "
++			"Lantiq Deutschland GmbH\n", version);
++}
++
++/* Displays the version of ETH module via proc file */
++static int eth_proc_version(struct seq_file *m, void *v)
++{
++	/* No sanity check cos length is smaller than one page */
++	seq_printf(m, "Lantiq ethernet driver for SVIP, version %s, "
++		"(c) 2014 Lantiq Deutschland GmbH\n", version);
++	return 0;
++}
++
++static int rversion_open(struct inode *inode, struct file *file)
++{
++	return single_open(file, eth_proc_version, NULL);
++}
++
++/* create proc for debug  info, eth_module_init */
++static int eth_proc_create(void)
++{
++	/* procfs */
++	g_eth_proc_dir = proc_mkdir("driver/" SVIP_ETH_DRV_PROC_ENTRY_NAME,
++		NULL);
++	if (g_eth_proc_dir == NULL) {
++		printk(KERN_ERR "%s: Create proc directory (/driver/"
++		SVIP_ETH_DRV_PROC_ENTRY_NAME ") failed!!!\n", __func__);
++		return -EIO;
++	}
++	proc_create("version", S_IRUGO, g_eth_proc_dir, &rversion_ops);
++	return 0;
++}
++
++/* remove of the proc entries, eth_module_exit */
++static void eth_proc_delete(void)
++{
++	remove_proc_entry("version", g_eth_proc_dir);
++	remove_proc_entry("driver/" SVIP_ETH_DRV_PROC_ENTRY_NAME, NULL);
++}
++
++/* Initialization of Ethernet module */
++static int svip_eth_drv_init(void)
++{
++	int i, err;
++	unsigned int reg;
++	char ver_str[128] = { 0 };
++	ltq_switch_priv_t* priv;
++	g_pmac_dma = 0;
++
++#ifdef CONFIG_LTQMIPS_DMA
++	/* Register with DM core driver */
++	err = ltqco_dma_setup_init();
++#endif /* CONFIG_LTQMIPS_DMA */
++
++	/* todo: read configuration from device tree */
++	for (i = 0; i < NUM_ETH_INF; i++) {
++		char name[16];
++
++		sprintf(name, "eth%d", i);
++		eth_dev[i] = alloc_etherdev(sizeof(ltq_switch_priv_t));
++		if (!eth_dev[i]) {
++			printk(KERN_ERR "%s[%d]: no memory for eth_dev!!!\n",
++				__func__, __LINE__);
++			err = -ENOMEM;
++			goto err_out_free_res;
++		}
++
++		/* setup the network device */
++		strcpy(eth_dev[i]->name, name);
++		eth_dev[i]->netdev_ops = &ltq_eth_drv_ops;
++		eth_dev[i]->watchdog_timeo = LTQ_TX_TIMEOUT;
++
++		/* setup the private data */
++		priv = netdev_priv(eth_dev[i]);
++		priv->phy_addr = i;
++
++		/* By default, advertise supported  speed/duplex settings. */
++		priv->flags |= (FLAG_ADV_10HALF | FLAG_ADV_10FULL
++			| FLAG_ADV_100HALF | FLAG_ADV_100FULL
++			| FLAG_ADV_1000HALF | FLAG_ADV_1000FULL);
++
++		/* By default, auto-negotiate PAUSE. */
++		priv->flags |= FLAG_PAUSE_AUTO;
++		spin_lock_init(&priv->lock);
++		err = register_netdev(eth_dev[i]);
++		if (err) {
++			printk(
++				KERN_ERR "%s[%d]: Register with network device failed!!!\n",
++				__func__, __LINE__);
++			goto err_out_free_res;
++		}
++	}
++
++	reg = es_r32(pmac_hd_ctl);
++	if (reg & LTQ_ES_PMAC_HD_CTL_AS)
++		g_pmac_dma = 1;
++
++	if (eth_proc_create() != 0)
++		goto err_out_free_res;
++	/* Print the driver version info */
++	eth_drv_ver(ver_str);
++	printk(KERN_INFO "%s", ver_str);
++	printk("g_pmac_dma:%d, reg:0x%08x !!!\n", g_pmac_dma, reg);
++	return 0;
++
++	err_out_free_res:
++	/* Unregister with DMA core driver */
++	dma_setup_uninit();
++	/* unregister the network devices */
++	for (i = 0; i < NUM_ETH_INF; i++) {
++		if (eth_dev[i])
++			free_netdev(eth_dev[i]);
++	}
++	return err;
++}
++
++static void svip_eth_drv_exit(void)
++{
++	int i;
++
++	/* unregister the network devices */
++	for (i = 0; i < NUM_ETH_INF; i++) {
++		unregister_netdev(eth_dev[i]);
++		free_netdev(eth_dev[i]);
++	}
++	/*Unregister with DMA core driver */
++	dma_setup_uninit();
++	/* remove of the proc entries */
++	eth_proc_delete();
++}
++
++static int svip_eth_drv_probe(struct platform_device *pdev)
++{
++	/* Just do the init */
++	svip_eth_drv_init();
++
++	return 0;
++}
++
++static int svip_eth_drv_remove(struct platform_device *pdev)
++{
++	/* Just do the exit */
++	svip_eth_drv_exit();
++	return 0;
++}
++
++static const struct of_device_id svip_eth_drv_match[] = { { .compatible =
++	"lantiq,svip-net" }, { }, };
++MODULE_DEVICE_TABLE(of, ltq_eth_drv_match);
++
++static struct platform_driver svip_eth_driver = { .probe = svip_eth_drv_probe,
++			.remove = svip_eth_drv_remove, .driver = { .name =
++				"svip-net",
++						.of_match_table =
++							svip_eth_drv_match,
++						.owner = THIS_MODULE, }, };
++
++module_platform_driver(svip_eth_driver);
++
++MODULE_AUTHOR("Martins Pukitis");
++MODULE_DESCRIPTION("SVIP ethernet driver");
++MODULE_LICENSE("GPL");
++MODULE_VERSION(DRV_MODULE_VERSION);
+diff --git a/drivers/net/ethernet/lantiq_svip.h b/drivers/net/ethernet/lantiq_svip.h
+new file mode 100644
+--- /dev/null
++++ b/drivers/net/ethernet/lantiq_svip.h
+@@ -0,0 +1,146 @@
++/******************************************************************************
++**
++** FILE NAME    : lantiq_svip.h
++** PROJECT      : Lantiq voice co
++** MODULES      : VINETIC-SVIP ETH module
++** DATE         : 16 May 2014
++** AUTHOR       : Martins Pukitis
++** DESCRIPTION  : Lantiq SVIP ethernet device driver
++** COPYRIGHT    : Copyright (c) 2014
++**                Lantiq Deutschland
++**
++**    This program is free software; you can redistribute it and/or modify
++**    it under the terms of the GNU General Public License as published by
++**    the Free Software Foundation; either version 2 of the License, or
++**    (at your option) any later version.
++**
++**    Adapted from Lantiq Cross-Platform ethernet device driver written by
++**    Reddy Mallikarjuna, Kishore Kankipati and Suresh Nagaraj.
++**
++** HISTORY
++** $Date                $Author              $Comment
++** 16 May 2014          Martins Pukitis		Initial version (no ethtool support)
++*******************************************************************************/
++
++#ifndef _LANTIQ_ETH_DRV_H_
++#define _LANTIQ_ETH_DRV_H_
++
++/* MII MAC mode*/
++#define MII_MAC_MODE                0x0001
++/* MII internl PHY mode */
++#define MII_PHY_MODE                0x0002
++#define EPHY_MODE                   MII_PHY_MODE
++/* Reverse MII MAC mode */
++#define REV_MII_MAC_MODE            0x0004
++/* Turbo Rev Mii MAC mode */
++#define TURBO_REV_MAC_MII_MODE      0x0008
++/* Reduced MII mode */
++#define RED_MII_PHY_MODE            0x0010
++#define RMII_PHY_MODE               RED_MII_PHY_MODE
++/* Reduce MII MAC mode with Input ref clock */
++#define RED_MII_MAC_MODE            0x0020
++#define RMII_MAC_MODE               RED_MII_MAC_MODE
++/* Reduce MII MAC mode with Output Ref clock */
++#define RED_MII_MAC_MODE_OC         0x0040
++#define RED_MII_MODE                RMII_MAC_MODE
++/* RGMII MAC mode */
++#define RGMII_MODE                  0x0100
++/* RGMII MAC with 100MB mode */
++#define RGMII_MODE_100MB            0x0200
++/*GMII MAC mode */
++#define GMII_MAC_MODE               0x0400
++
++/* hardware minimum and maximum for a single frame's data payload */
++/* Minimum MTU value */
++#define MIN_MTU                 64
++/*Max MTU value*/
++#define MAX_MTU                 1500
++/* Eth Packet buffer size to allocate for DMA descriptor*/
++#define RX_PKT_BUF_SZ           (1536 +  64)
++#define TX_PKT_BUF_SZ           (MAX_MTU + ETH_HLEN + 8)
++
++/* Set VLAN CoS value */
++#define SET_VLAN_COS                SIOCDEVPRIVATE
++/* Set DSCP CoS value*/
++#define SET_DSCP_COS                SIOCDEVPRIVATE+1
++/* Enable VLAN CoS */
++#define ENABLE_VLAN_COS             SIOCDEVPRIVATE+2
++/* Disable VLAN CoS */
++#define DISABLE_VLAN_COS            SIOCDEVPRIVATE+3
++/* Select first VLAN Class */
++#define VLAN_CLASS_FIRST            SIOCDEVPRIVATE+4
++/* Select second VLAN class if device supports*/
++#define VLAN_CLASS_SECOND           SIOCDEVPRIVATE+5
++/* Enable DSCP CoS */
++#define ENABLE_DSCP_COS             SIOCDEVPRIVATE+6
++/*Disable DSCP CoS */
++#define DISABLE_DSCP_COS            SIOCDEVPRIVATE+7
++/* Enable Unicast packet */
++#define PASS_UNICAST_PACKETS        SIOCDEVPRIVATE+8
++/* Disable Unicase packet */
++#define FILTER_UNICAST_PACKETS      SIOCDEVPRIVATE+9
++/* Enable broadcast packet */
++#define KEEP_BROADCAST_PACKETS      SIOCDEVPRIVATE+10
++/* Disable broadcast packet */
++#define DROP_BROADCAST_PACKETS      SIOCDEVPRIVATE+11
++/* Enable Multicast packet */
++#define KEEP_MULTICAST_PACKETS      SIOCDEVPRIVATE+12
++/* Disable Multicast packet */
++#define DROP_MULTICAST_PACKETS      SIOCDEVPRIVATE+13
++
++/*
++ This structure is used internal purpose
++ */
++typedef struct switch_priv
++{
++	struct net_device_stats stats; /*!< network device interface Statistics */
++	struct dma_device_info *dma_device; /*!< structure of dma device information */
++	struct sk_buff *skb; /*!< skb buffer structure*/
++	spinlock_t lock; /*!< spin lock */
++	int phy_addr; /*!< interface mdio phy address*/
++	int current_speed; /*!< interface current speed*/
++	int full_duplex; /*!< duplex mode*/
++	int current_duplex; /*!< current interface duplex mode*/
++	void __iomem *base_addr; /*!< Base address */
++	unsigned int flags; /*!< flags */
++#define FLAG_PAUSE_AUTO         0x00000001
++#define FLAG_FULL_DUPLEX        0x00000002
++#define FLAG_10_BASE_T          0x00000010
++#define FLAG_100_BASE_T         0x00000020
++#define FLAG_1000_BASE_T        0x00000040
++#define FLAG_TX_PAUSE           0x00000100
++#define FLAG_RX_PAUSE           0x00000200
++#define FLAG_FORCE_LINK         0x00000400
++#define FLAG_ADV_10HALF         0x00001000
++#define FLAG_ADV_10FULL         0x00002000
++#define FLAG_ADV_100HALF        0x00004000
++#define FLAG_ADV_100FULL        0x08008000
++#define FLAG_ADV_1000HALF       0x00010000
++#define FLAG_ADV_1000FULL       0x00020000
++#define FLAG_INTERNAL_PHY       0x00100000
++} ltq_switch_priv_t;
++
++/*
++ This structure is used internal purpose
++ */
++typedef struct
++{
++	u32 CRC_GEN :1; /* CRC Generated */
++	u32 CRC_ERR :1; /* CRC Error */
++	u32 res1: 6; /* reserved */
++	u32 SPID: 2; /* Source Port Id */
++	u32 MRR: 1; /* Mirrored */
++	u32 res2: 2; /* reserved */
++	u32 TO_TAG: 1; /* Output Tagged */
++	u32 QID: 2; /* Queue ID */
++	u32 TAG: 1; /* Input Tagged */
++	u32 MPC: 1; /* Management Packet */
++	u32 res3: 14; /* reserved */
++
++	u32 res4: 2; /* reserved */
++	u32 PKT_LEN: 14; /* Original packet length */
++	u32 PRI: 3; /* Priority of the packet */
++	u32 CFI: 1; /* CFI Bit in VLAN Tag */
++	u32 VLAN_ID: 12; /* VLAN ID */
++} cpu_pkt_header_t;
++#endif /* _LANTIQ_ETH_DRV_H_ */
diff --git a/target/linux/lantiq/patches-3.10/2225-lantiq-add-veth-drv.patch b/target/linux/lantiq/patches-3.10/2225-lantiq-add-veth-drv.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2225-lantiq-add-veth-drv.patch
@@ -0,0 +1,355 @@
+# HG changeset patch
+# Parent 5e38b53024f0dd204af399e19bc38a1cb3fc67e4
+
+diff --git a/drivers/net/ethernet/Kconfig b/drivers/net/ethernet/Kconfig
+--- a/drivers/net/ethernet/Kconfig
++++ b/drivers/net/ethernet/Kconfig
+@@ -97,6 +97,11 @@ config LANTIQ_SVIP_ETH_DRV
+ 	---help---
+ 	  Support for the ethernet switch inside the Lantiq SoC SVIP
+ 
++config LANTIQ_SVIP_VIRTUAL_ETH
++	default y
++	tristate "Lantiq SoC SVIP Virtual Ethernet driver"
++	depends on SOC_SVIP
++
+ source "drivers/net/ethernet/marvell/Kconfig"
+ source "drivers/net/ethernet/mellanox/Kconfig"
+ source "drivers/net/ethernet/micrel/Kconfig"
+diff --git a/drivers/net/ethernet/Makefile b/drivers/net/ethernet/Makefile
+--- a/drivers/net/ethernet/Makefile
++++ b/drivers/net/ethernet/Makefile
+@@ -38,6 +38,7 @@ obj-$(CONFIG_KORINA) += korina.o
+ obj-$(CONFIG_LANTIQ_ETOP) += lantiq_etop.o
+ obj-$(CONFIG_LANTIQ_XRX200) += lantiq_xrx200.o
+ obj-$(CONFIG_LANTIQ_SVIP_ETH_DRV) += lantiq_svip.o
++obj-$(CONFIG_LANTIQ_SVIP_VIRTUAL_ETH) += lantiq_svip_virtual_eth.o
+ obj-$(CONFIG_NET_VENDOR_MARVELL) += marvell/
+ obj-$(CONFIG_NET_VENDOR_MELLANOX) += mellanox/
+ obj-$(CONFIG_NET_VENDOR_MICREL) += micrel/
+diff --git a/drivers/net/ethernet/lantiq_svip_virtual_eth.c b/drivers/net/ethernet/lantiq_svip_virtual_eth.c
+new file mode 100644
+--- /dev/null
++++ b/drivers/net/ethernet/lantiq_svip_virtual_eth.c
+@@ -0,0 +1,321 @@
++/******************************************************************************
++**
++** FILE NAME    : lantiq_svip_virtual_eth.c
++** PROJECT      : Lantiq voice co
++** MODULES      : Lantiq VINETIC-SVIP virtual ethernet driver
++** DATE         : 12 August 2014
++** AUTHOR       : Martins Pukitis
++** DESCRIPTION  : Lantiq SVIP virtual ethernet device driver
++** COPYRIGHT    : Copyright (c) 2014
++**                Lantiq Deutschland
++**
++**    This program is free software; you can redistribute it and/or modify
++**    it under the terms of the GNU General Public License as published by
++**    the Free Software Foundation; either version 2 of the License, or
++**    (at your option) any later version.
++**
++**    Adapted from Lantiq VINETIC-SVIP virtual ethernet driver written by
++**    Risto Minev.
++**
++** HISTORY
++** $Date             $Author              $Comment
++** 12 Aug 2014       Martins Pukitis	  Initial version (no ethtool support)
++*******************************************************************************/
++#include <linux/module.h>
++#include <linux/kernel.h>
++#include <linux/netdevice.h>
++#include <linux/platform_device.h>
++#include <linux/etherdevice.h>
++#include <linux/init.h>
++
++#define DRV_MODULE_NAME             "lantiq_svip_virtual_eth"
++#define DRV_MODULE_VERSION          "3.0"
++static char version[] =
++	DRV_MODULE_NAME ".c:v" DRV_MODULE_VERSION;
++
++/******************************************************************************
++ * Local define/macro definitions
++ ******************************************************************************/
++struct svip_ve_priv {
++	struct net_device_stats stats;
++};
++
++/******************************************************************************
++ * Global function declarations
++ ******************************************************************************/
++
++/******************************************************************************
++ * Local variable declarations
++ ******************************************************************************/
++static struct net_device *svip_ve_dev;
++static int watchdog_timeout = 10*HZ;
++static int (*svip_ve_mps_xmit)(struct sk_buff *skb);
++
++
++/******************************************************************************
++ * Global function declarations
++ ******************************************************************************/
++
++/**
++ * Called by MPS driver to register a transmit routine called for each outgoing
++ * VoFW0 message.
++ *
++ * \param   mps_xmit    pointer to transmit routine
++ *
++ * \return  none
++ *
++ * \ingroup Internal
++ */
++void register_mps_xmit_routine(int (*mps_xmit)(struct sk_buff *skb))
++{
++	svip_ve_mps_xmit = mps_xmit;
++}
++EXPORT_SYMBOL(register_mps_xmit_routine);
++
++/**
++ * Called by MPS driver upon receipt of a new message from VoFW0 module in
++ * the data inbox. The packet is pushed up the IP module for further processing.
++ *
++ * \param   skb            pointer to skb containing the incoming message
++ *
++ * \return  0 on success
++ * \return  non-zero on error
++ *
++ * \ingroup Internal
++ */
++int svip_ve_rx(struct sk_buff *skb)
++{
++	int err;
++	struct svip_ve_priv *priv = netdev_priv(svip_ve_dev);
++	struct net_device_stats *stats = &priv->stats;
++
++	skb->dev = svip_ve_dev;
++	skb->protocol = eth_type_trans(skb, svip_ve_dev);
++
++	stats->rx_packets++;
++	stats->rx_bytes += skb->len;
++
++	err = netif_rx(skb);
++	switch (err) {
++	case NET_RX_SUCCESS:
++		return 0;
++		break;
++	case NET_RX_DROP:
++	default:
++		stats->rx_dropped++;
++		break;
++	}
++
++	return 1;
++}
++EXPORT_SYMBOL(svip_ve_rx);
++
++/**
++ * Returns a pointer to the routine used to deliver an incoming packet/message
++ * from the MPS mailbox to the networking layer. This routine is called by MPS
++ * driver during initialisation time.
++ *
++ * \param   skb         pointer to incoming socket buffer
++ *
++ * \return  svip_ve_rx  pointer to incoming messages delivering routine
++ *
++ * \ingroup Internal
++ */
++int (*register_mps_recv_routine(void)) (struct sk_buff *skb)
++{
++	return svip_ve_rx;
++}
++EXPORT_SYMBOL(register_mps_recv_routine);
++
++/**
++ * Used to deliver outgoing packets to VoFW0 module through the MPS driver.
++ * Upon loading/initialisation the MPS driver is registering a transmitting
++ * routine, which is called here to deliver the packet to the VoFW0 module.
++ *
++ * \param   skb            pointer to skb containing outgoing data
++ * \param   dev            pointer to this networking device's data
++ *
++ * \return  0 on success
++ * \return  non-zero on error
++ *
++ * \ingroup Internal
++ */
++static int svip_ve_xmit(struct sk_buff *skb, struct net_device *dev)
++{
++	int err;
++	struct svip_ve_priv *priv = netdev_priv(dev);
++	struct net_device_stats *stats = &priv->stats;
++
++	stats->tx_packets++;
++	stats->tx_bytes += skb->len;
++
++	if (svip_ve_mps_xmit) {
++		err = svip_ve_mps_xmit(skb);
++		if (err)
++			stats->tx_errors++;
++		dev->trans_start = jiffies;
++		return err;
++	} else
++		netdev_err(dev, "MPS driver not registered, outgoing packet not delivered\n");
++
++	dev_kfree_skb(skb);
++
++	return -1;
++}
++
++
++/**
++ * Returns a pointer to the device's networking statistics data
++ *
++ * \param   dev            pointer to this networking device's data
++ *
++ * \return  stats          pointer to this network device's statistics data
++ *
++ * \ingroup Internal
++ */
++static struct net_device_stats *svip_ve_get_stats(struct net_device *dev)
++{
++	struct svip_ve_priv *priv = netdev_priv(dev);
++
++	return &priv->stats;
++}
++
++static void svip_ve_tx_timeout(struct net_device *dev)
++{
++	struct svip_ve_priv *priv = netdev_priv(dev);
++
++	priv->stats.tx_errors++;
++	netif_wake_queue(dev);
++}
++
++/**
++ * Device open routine. Called e.g. upon setting of an IP address using,
++ * 'ifconfig veth0 YYY.YYY.YYY.YYY netmask ZZZ.ZZZ.ZZZ.ZZZ' or
++ * 'ifconfig veth0 up'
++ *
++ * \param   dev            pointer to this network device's data
++ *
++ * \return  0 on success
++ * \return  non-zero on error
++ *
++ * \ingroup Internal
++ */
++int svip_ve_open(struct net_device *dev)
++{
++	netif_start_queue(dev);
++	return 0;
++}
++
++/**
++ * Device close routine. Called e.g. upon calling
++ * 'ifconfig veth0 down'
++ *
++ * \param   dev            pointer to this network device's data
++ *
++ * \return  0 on success
++ * \return  non-zero on error
++ *
++ * \ingroup Internal
++ */
++
++int svip_ve_release(struct net_device *dev)
++{
++	netif_stop_queue(dev);
++	return 0;
++}
++
++static int svip_ve_dev_init(struct net_device *dev);
++
++static const struct net_device_ops svip_virtual_eth_netdev_ops = {
++	.ndo_init = svip_ve_dev_init,
++	.ndo_open = svip_ve_open,
++	.ndo_stop = svip_ve_release,
++	.ndo_start_xmit = svip_ve_xmit,
++	.ndo_get_stats = svip_ve_get_stats,
++	.ndo_tx_timeout = svip_ve_tx_timeout,
++};
++
++
++/**
++ * Device initialisation routine which registers device interface routines.
++ * It is called upon execution of 'register_netdev' routine.
++ *
++ * \param   dev            pointer to this network device's data
++ *
++ * \return  0 on success
++ * \return  non-zero on error
++ *
++ * \ingroup Internal
++ */
++static int svip_ve_dev_init(struct net_device *dev)
++{
++	ether_setup(dev); /* assign some of the fields */
++
++	dev->watchdog_timeo  = watchdog_timeout;
++	memset(netdev_priv(dev), 0, sizeof(struct svip_ve_priv));
++	dev->flags |= IFF_NOARP|IFF_PROMISC;
++	dev->flags &= ~IFF_MULTICAST;
++
++	/* dedicated MAC address to veth0, 00:03:19:00:15:80 */
++	dev->dev_addr[0] = 0x00;
++	dev->dev_addr[1] = 0x03;
++	dev->dev_addr[2] = 0x19;
++	dev->dev_addr[3] = 0x00;
++	dev->dev_addr[4] = 0x15;
++	dev->dev_addr[5] = 0x80;
++
++	return 0;
++}
++
++static int svip_ve_probe(struct platform_device *dev)
++{
++	int result = 0;
++
++	svip_ve_dev = alloc_etherdev(sizeof(struct svip_ve_priv));
++	svip_ve_dev->netdev_ops = &svip_virtual_eth_netdev_ops;
++
++	strcpy(svip_ve_dev->name, "veth%d");
++
++	result = register_netdev(svip_ve_dev);
++	if (result) {
++		netdev_info(svip_ve_dev, "error %i registering device\n",
++			result);
++		goto out;
++	}
++
++	netdev_info(svip_ve_dev, "Lantiq virtual ethernet driver for SVIP, version %s,(c) 2014 Lantiq Deutschland GmbH\n",
++		version);
++
++out:
++	return result;
++}
++
++static int svip_ve_remove(struct platform_device *dev)
++{
++	unregister_netdev(svip_ve_dev);
++	free_netdev(svip_ve_dev);
++
++	return 0;
++}
++
++static const struct of_device_id svip_ve_drv_match[] = {
++	{.compatible = "lantiq,svip-ve-net"},
++	{},
++};
++MODULE_DEVICE_TABLE(of, svip_ve_drv_match);
++
++static struct platform_driver svip_ve_driver = {
++	.probe = svip_ve_probe,
++	.remove = svip_ve_remove,
++	.driver = {
++		.name = "svip-ve-net",
++		.of_match_table = svip_ve_drv_match,
++		.owner = THIS_MODULE,
++	},
++};
++module_platform_driver(svip_ve_driver);
++
++MODULE_AUTHOR("Martins Pukitis");
++MODULE_DESCRIPTION("SVIP virtual ethernet driver");
++MODULE_LICENSE("GPL");
++MODULE_VERSION(DRV_MODULE_VERSION);
diff --git a/target/linux/lantiq/patches-3.10/2230-lantiq-add-spi-drv.patch b/target/linux/lantiq/patches-3.10/2230-lantiq-add-spi-drv.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2230-lantiq-add-spi-drv.patch
@@ -0,0 +1,27 @@
+# HG changeset patch
+# Parent ec7e316cef5ea89c98e90bdde4a0027e421e9911
+
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+--- a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
++++ b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+@@ -80,6 +80,9 @@ static inline int ltq_is_grx390(void)
+ #define SYS1_CLKCLR	0x0008
+ #define SYS1_CLKENR_ETHSW (0x1 << 7)
+ #define SYS1_CLKENR_DMA (0x1 << 9)
++#define SYS1_CLKENR_SSC0 (0x1 << 11)
++#define SYS1_CLKENR_SSC1 (0x1 << 12)
++#define SYS1_CLKENR_SSC2 (0x1 << 13)
+ #define SYS1_CLKENR_ASC0 (0x1 << 14)
+ #define SYS1_CLKENR_ASC1 (0x1 << 15)
+ #define SYS1_FPICR_FPIDIV   (0x1)
+diff --git a/arch/mips/lantiq/svip/sysctrl.c b/arch/mips/lantiq/svip/sysctrl.c
+--- a/arch/mips/lantiq/svip/sysctrl.c
++++ b/arch/mips/lantiq/svip/sysctrl.c
+@@ -241,6 +241,7 @@ void __init ltq_soc_init(void)
+ 		svip_io_region_clock(), 0);
+ 	clkdev_add_sys("14100100.serial0", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
+ 	clkdev_add_sys("14100200.serial1", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
++	clkdev_add_sys("14100300.spi0", SYSCTL_SYS1, SYS1_CLKENR_SSC0);
+ 	clkdev_add_sys("14104000.dma", SYSCTL_SYS1, SYS1_CLKENR_DMA);
+ 	clkdev_add_sys("18000000.eth", SYSCTL_SYS1, SYS1_CLKENR_ETHSW);
+ }
diff --git a/target/linux/lantiq/patches-3.10/2240-svip-add-sysctrl.patch b/target/linux/lantiq/patches-3.10/2240-svip-add-sysctrl.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2240-svip-add-sysctrl.patch
@@ -0,0 +1,190 @@
+# HG changeset patch
+# Parent 4f3963ddf3165a0fd90835b46ed7eca70ed51f13
+
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h b/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h
+--- a/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h
++++ b/arch/mips/include/asm/mach-lantiq/svip/ebu_reg.h
+@@ -16,10 +16,6 @@
+ #define ebu_w32_mask(clear, set, reg)	\
+ 	ltq_w32_mask(clear, set, ltq_ebu_membase + (reg))
+ 
+-#define ltq_ebu_w32(x, y) 		ebu_w32(x, y)
+-#define ltq_ebu_r32(x) 		ebu_r32(x)
+-#define ltq_ebu_w32_mask(clear, set, reg)	ebu_w32_mask(clear, set, reg)
+-
+ /** EBU register structure */
+ struct svip_reg_ebu {
+ 	volatile unsigned long  clc;  /*  0x0000 */
+diff --git a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+--- a/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
++++ b/arch/mips/include/asm/mach-lantiq/svip/lantiq_soc.h
+@@ -64,21 +64,48 @@ static inline int ltq_is_grx390(void)
+ 	return (ltq_get_soc_type() == SOC_TYPE_GRX390);
+ }
+ 
++static inline int ltq_is_svip(void)
++{
++	return (ltq_get_soc_type() == SOC_TYPE_SVIP);
++}
++
+ #define LTQ_EBU_BASE				0x14102000
+ #define LTQ_ASC0_BASE_ADDR			0x14100100
+ #define LTQ_ASC1_BASE_ADDR			0x14100200
+ #define LTQ_STATUS_BASE_ADDR		0x1E000500
+ #define LTQ_ES_BASE_ADDR			0x18000000
+ #define LTQ_SYS1_BASE_ADDR			0x1C000800
++#define LTQ_L2_SPRAM_BASE_ADDR			0x1F1E8000
++#define LTQ_SWINT_BASE_ADDR			0x1E000100
+ 
+ #define SYS0_PLL1CR	0x0008
+ #define SYS0_PLL1CR_PLLDIV   (0x3)
+-#define SYS0_PLL1CR_PLLDIV_GET(val)   ((((val) & SYS0_PLL1CR_PLLDIV) >> 0) & 0x3) 
++#define SYS0_PLL1CR_PLLDIV_GET(val)   ((((val) & SYS0_PLL1CR_PLLDIV) >> 0) & 0x3)
++#define SYS0_BCR	0x0004
+ 
+ #define SYS1_CLKSR	0x0000
+ #define SYS1_FPICR	0x0014
+ #define SYS1_CLKENR	0x0004
+ #define SYS1_CLKCLR	0x0008
++#define SYS1_CPUCR0	0x0020
++#define SYS1_CPUCR1	0x0024
++#define SYS1_CPUCR2	0x0028
++#define SYS1_CPUCR3	0x002C
++#define SYS1_CPUCR4	0x0030
++#define SYS1_CPUCR5	0x0034
++#define SYS1_RREQR	0x0044
++#define SYS1_RRLSR	0x0048
++#define SYS1_CPU0RSR	0x0060
++#define SYS1_CPU1RSR	0x0064
++#define SYS1_CPU2RSR	0x0068
++#define SYS1_CPU3RSR	0x006C
++#define SYS1_CPU4RSR	0x0070
++#define SYS1_CPU5RSR	0x0074
++#define SYS1_CLKENR_PORT0 (0x1 << 0)
++#define SYS1_CLKENR_PORT1 (0x1 << 1)
++#define SYS1_CLKENR_PORT2 (0x1 << 2)
++#define SYS1_CLKENR_PORT3 (0x1 << 3)
++#define SYS1_CLKENR_EBU (0x1 << 6)
+ #define SYS1_CLKENR_ETHSW (0x1 << 7)
+ #define SYS1_CLKENR_DMA (0x1 << 9)
+ #define SYS1_CLKENR_SSC0 (0x1 << 11)
+@@ -116,8 +143,6 @@ static inline int ltq_is_grx390(void)
+ #define STATUS_CONFIG_CLK_MODE   (0x1 << 1)
+ #define STATUS_CONFIG_CLK_MODE_GET(val)   ((((val) & STATUS_CONFIG_CLK_MODE) >> 4) & 0x1) 
+ 
+-#define LTQ_EBU_PCC_ISTAT   (LTQ_EBU_BASE + 0x00A0)
+-
+ /*
+  * during early_printk no ioremap possible at this early stage
+  * lets use KSEG1 instead
+@@ -158,5 +183,13 @@ extern __iomem void *ltq_sys1_membase;
+ #define ltq_sys1_w32_mask(clear, set, reg)   \
+ 	ltq_sys1_w32((ltq_sys1_r32(reg) & ~(clear)) | (set), reg)
+ 
++extern void ltq_sysctl_chipid_get(unsigned int *chipid);
++
++/*
++ * to keep the irq code generic we need to define this to 0 as SVIP
++ * has no EBU_PCC_ISTAT register
++ */
++#define LTQ_EBU_PCC_ISTAT	0
++
+ #endif /* CONFIG_SOC_SVIP */
+ #endif /* _LTQ_SVIP_H__ */
+diff --git a/arch/mips/lantiq/svip/sysctrl.c b/arch/mips/lantiq/svip/sysctrl.c
+--- a/arch/mips/lantiq/svip/sysctrl.c
++++ b/arch/mips/lantiq/svip/sysctrl.c
+@@ -16,6 +16,9 @@
+ #include <asm/delay.h>
+ 
+ #include <lantiq_soc.h>
++#include <svip/sys2_reg.h>
++#include <svip/ebu_reg.h>
++
+ /*
+ static struct svip_reg_status *const status =
+ (struct svip_reg_status *)LTQ_STATUS_BASE_ADDR;
+@@ -32,7 +35,8 @@ static struct svip_reg_sys1 *const sys1 
+ #define status_w32(x, y)	ltq_w32((x), status_membase + (y))
+ #define status_r32(x)		ltq_r32(status_membase + (x))
+ 
+-static void __iomem *sysctl_membase[SYSCTL_NUM_OF_SYS], *status_membase;
++void __iomem *sysctl_membase[SYSCTL_NUM_OF_SYS], *status_membase;
++EXPORT_SYMBOL(sysctl_membase);
+ void __iomem *ltq_sys1_membase, *ltq_ebu_membase;
+ 
+ static inline void sysctl_wait(struct clk *clk,
+@@ -225,10 +229,73 @@ void __init ltq_soc_init(void)
+ 	clkdev_add_sys("14100100.serial0", SYSCTL_SYS1, SYS1_CLKENR_ASC0);
+ 	clkdev_add_sys("14100200.serial1", SYSCTL_SYS1, SYS1_CLKENR_ASC1);
+ 	clkdev_add_sys("14100300.spi0", SYSCTL_SYS1, SYS1_CLKENR_SSC0);
++	clkdev_add_sys("14100600.port", SYSCTL_SYS1, SYS1_CLKENR_PORT0);
++	clkdev_add_sys("14108100.port", SYSCTL_SYS1, SYS1_CLKENR_PORT1);
++	clkdev_add_sys("14100800.port", SYSCTL_SYS1, SYS1_CLKENR_PORT2);
++	clkdev_add_sys("14100900.port", SYSCTL_SYS1, SYS1_CLKENR_PORT3);
+ 	clkdev_add_sys("14104000.dma", SYSCTL_SYS1, SYS1_CLKENR_DMA);
+ 	clkdev_add_sys("18000000.eth", SYSCTL_SYS1, SYS1_CLKENR_ETHSW);
++
++	clkdev_add_sys("1e000400.port", SYSCTL_SYS2, SYS2_CLKENR_PORT4);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_HWSYNC);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_MBS);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SWINT);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_HWACC3);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_HWACC2);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_HWACC1);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_HWACC0);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF7);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF6);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF5);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF4);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF3);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF2);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF1);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_SIF0);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV7);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV6);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV5);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV4);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV3);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV2);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV1);
++	ltq_sysctl_clken(SYSCTL_SYS2, SYS2_CLKENR_DFEV0);
++
++	/* EBU configuration */
++	ebu_w32(0x120000f1, LTQ_EBU_ADDR_SEL_2);
++#ifdef CONFIG_CPU_LITTLE_ENDIAN
++	ebu_w32(LTQ_EBU_CON_0_SETUP |
++		LTQ_EBU_CON_0_BCGEN_VAL(0x02) |
++		LTQ_EBU_CON_0_WAITWRC_VAL(7) |
++		LTQ_EBU_CON_0_WAITRDC_VAL(3) |
++		LTQ_EBU_CON_0_HOLDC_VAL(3) |
++		LTQ_EBU_CON_0_RECOVC_VAL(3) |
++		LTQ_EBU_CON_0_CMULT_VAL(3), LTQ_EBU_CON_2);
++#else
++	ebu_w32(LTQ_EBU_CON_0_ADSWP |
++		LTQ_EBU_CON_0_SETUP |
++		LTQ_EBU_CON_0_BCGEN_VAL(0x02) |
++		LTQ_EBU_CON_0_WAITWRC_VAL(7) |
++		LTQ_EBU_CON_0_WAITRDC_VAL(3) |
++		LTQ_EBU_CON_0_HOLDC_VAL(3) |
++		LTQ_EBU_CON_0_RECOVC_VAL(3) |
++		LTQ_EBU_CON_0_CMULT_VAL(3), LTQ_EBU_CON_2);
++#endif
+ }
+ 
++void ltq_sysctl_clken(int module, unsigned int mask)
++{
++	struct clk clk = { .module = module, .bits = mask };
++	sysctl_clken(&clk);
++}
++EXPORT_SYMBOL(ltq_sysctl_clken);
++
++void ltq_sysctl_clkdis(int module, unsigned int mask)
++{
++	struct clk clk = { .module = module, .bits = mask };
++	sysctl_clkdis(&clk);
++}
++EXPORT_SYMBOL(ltq_sysctl_clkdis);
+ /*
+  * for compatibility to external drivers from Lantiq
+  * see arch/mips/include/asm/mach-lantiq/svip/sysctrl.h
diff --git a/target/linux/lantiq/patches-3.10/2250-svip-add-nat.patch b/target/linux/lantiq/patches-3.10/2250-svip-add-nat.patch
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/patches-3.10/2250-svip-add-nat.patch
@@ -0,0 +1,2928 @@
+# HG changeset patch
+# Parent f9608b0943d8477fb6ad5bcf7fc75b75b34d3076
+
+diff --git a/include/linux/svip_nat.h b/include/linux/svip_nat.h
+new file mode 100644
+--- /dev/null
++++ b/include/linux/svip_nat.h
+@@ -0,0 +1,23 @@
++/******************************************************************************
++
++	Copyright (c) 2014
++	Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++#ifndef _SVIP_NAT_H
++#define _SVIP_NAT_H
++
++/*  The declarations here have to be in a header file, because
++ *  they need to be known both to the kernel module
++ *  (in chardev.c) and the process calling ioctl (ioctl.c)
++ */
++#include <linux/svip_nat_io.h>
++
++#define SVIP_NAT_VERSION "3.4"
++extern int do_SVIP_NAT(struct sk_buff *);
++extern int ipv6_do_SVIP_NAT(struct sk_buff *);
++
++#endif
+diff --git a/include/linux/svip_nat_io.h b/include/linux/svip_nat_io.h
+new file mode 100644
+--- /dev/null
++++ b/include/linux/svip_nat_io.h
+@@ -0,0 +1,15 @@
++/******************************************************************************
++
++	Copyright (c) 2014
++	Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++#ifndef _SVIP_NAT_IO_H_
++#define _SVIP_NAT_IO_H_
++
++#include <uapi/linux/svip_nat_io.h>
++
++#endif
+diff --git a/include/uapi/linux/Kbuild b/include/uapi/linux/Kbuild
+--- a/include/uapi/linux/Kbuild
++++ b/include/uapi/linux/Kbuild
+@@ -360,6 +360,7 @@ header-y += stat.h
+ header-y += stddef.h
+ header-y += string.h
+ header-y += suspend_ioctls.h
++header-y += svip_nat_io.h
+ header-y += swab.h
+ header-y += switch.h
+ header-y += synclink.h
+diff --git a/include/uapi/linux/svip_nat_io.h b/include/uapi/linux/svip_nat_io.h
+new file mode 100644
+--- /dev/null
++++ b/include/uapi/linux/svip_nat_io.h
+@@ -0,0 +1,157 @@
++/******************************************************************************
++
++	Copyright (c) 2014
++	Lantiq Deutschland GmbH
++
++  For licensing information, see the file 'LICENSE' in the root folder of
++  this software module.
++
++******************************************************************************/
++#ifndef _UAPI__SVIP_NAT_IO_H_
++#define _UAPI__SVIP_NAT_IO_H_
++
++#include <linux/types.h>
++#include <linux/ioctl.h>
++
++#define SVIP_NAT_DEVICE_NAME        "svip_nat"
++#define PATH_SVIP_NAT_DEVICE_NAME   "/dev/"SVIP_NAT_DEVICE_NAME
++
++#define MAJOR_NUM_SVIP_NAT          10
++#define MINOR_NUM_SVIP_NAT          120
++
++/** @defgroup SVIP_NATAPI  SVIP Custom NAT ioctl interface.
++  An ioctl interface is provided to add a rule into the SVIP NAT table and
++  to respectively remove the rule form it. The ioctl interface is accessible
++  using the fd issued upon opening the special device node /dev/svip_nat.
++  @{  */
++
++/** Used to add a new rule to the SVIP Custom NAT table. If a rule already
++  exists for the target UDP port, that rule shall be overwritten.
++
++  \param SVIP_NAT_IO_Rule_t* The parameter points to a
++  \ref SVIP_NAT_IO_Rule_t structure.
++  */
++#define FIO_SVIP_NAT_RULE_ADD \
++	_IOW(MAJOR_NUM_SVIP_NAT, 1, SVIP_NAT_IO_Rule_t)
++
++/** Used to remove a rule from the SVIP Custom NAT table. No check is
++  performed whether the rule already exists or not. The remove operation is
++  performed as long as the target UDP port is within the defined port range.
++
++  \param SVIP_NAT_IO_Rule_t* The parameter points to a
++  \ref SVIP_NAT_IO_Rule_t structure.
++  */
++#define FIO_SVIP_NAT_RULE_REMOVE \
++	_IOW(MAJOR_NUM_SVIP_NAT, 2, SVIP_NAT_IO_Rule_t)
++
++/** Used to list all rules in the SVIP Custom NAT table.
++
++  \param <none>
++  */
++#define FIO_SVIP_NAT_RULE_LIST \
++	_IO(MAJOR_NUM_SVIP_NAT, 3)
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++/** Used to add a valid IP for NAT. NAT is performed only on packets whose
++ * destination IPs have been added with this IO control.
++
++  \param SVIP_IP_ADDR_t* The parameter points to a
++  \ref SVIP_NAT_DESTIP_t structure.
++  */
++#define FIO_SVIP_NAT_DESTIP_CHECK_ADD \
++	_IOW(MAJOR_NUM_SVIP_NAT, 4, SVIP_NAT_DESTIP_t)
++
++/** Used to remove a valid IP for NAT.
++
++  \param SVIP_IP_ADDR_t* The parameter points to a
++  \ref SVIP_NAT_DESTIP_t structure.
++  */
++#define FIO_SVIP_NAT_DESTIP_CHECK_REMOVE \
++	_IOW(MAJOR_NUM_SVIP_NAT, 5, SVIP_NAT_DESTIP_t)
++
++/** Used to list all IPs valid for NAT.
++
++  \param <none>
++  */
++#define FIO_SVIP_NAT_DESTIP_LIST_GET \
++	_IO(MAJOR_NUM_SVIP_NAT, 6)
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++
++/** Used to write the base UDP port number of SVIP Custom NAT.
++
++  \param SVIP_UDP_PORT_t* The parameter points to a
++  \ref SVIP_UDP_PORT_t structure.
++  */
++#define FIO_SVIP_NAT_UDP_PORT_BASE_SET \
++	_IOW(MAJOR_NUM_SVIP_NAT, 7, SVIP_UDP_PORT_t)
++
++/** Used to read the base UDP port number of SVIP Custom NAT.
++
++  \param SVIP_UDP_PORT_t* The parameter points to a
++  \ref SVIP_UDP_PORT_t structure.
++  */
++#define FIO_SVIP_NAT_UDP_PORT_BASE_GET \
++	_IOWR(MAJOR_NUM_SVIP_NAT, 8, SVIP_UDP_PORT_t)
++
++#ifndef ETH_ALEN
++#define ETH_ALEN         6 /* Octets in one ethernet address */
++#endif
++
++/** Type to identify IPv4 and IPv6 addresses */
++typedef enum {
++	SVIP_IPV4_ADDR_TYPE,
++	SVIP_IPV6_ADDR_TYPE
++} SVIP_IP_ADDR_TYPE_t;
++
++typedef __u32 SVIP_IPV4_ADDR_t;
++
++/** IPv6 address type, style taken over from Linux */
++typedef struct SVIP_IPV6_ADDR {
++	union {
++		__u8 addr8[16];
++		__u16 addr16[8];
++		__u32 addr32[4];
++	} ipv6_u;
++#define ipv6_addr8 ipv6_u.addr8
++#define ipv6_addr16 ipv6_u.addr16
++#define ipv6_addr32 ipv6_u.addr32
++} SVIP_IPV6_ADDR_t;
++
++/** union of IPv4 and IPv6 addresses */
++typedef union {
++	SVIP_IPV4_ADDR_t v4;
++	SVIP_IPV6_ADDR_t v6;
++} SVIP_IP_ADDR_UNION_t;
++
++/** UDP port in network-byte order */
++typedef __u16 SVIP_UDP_PORT_t;
++
++/** NAT parameters part of the NAT table.
++  These parameters are configurable through the NAT API. */
++typedef struct SVIP_NAT_IO_Rule {
++	/** IP address type */
++	SVIP_IP_ADDR_TYPE_t typeIP;
++	/** Remote peer IP address */
++	SVIP_IP_ADDR_UNION_t remIP;
++	/** Remote peer, MAC address */
++	__u8 remMAC[ETH_ALEN];
++	/** Target SVIP, IP address (local peer) */
++	SVIP_IP_ADDR_UNION_t locIP;
++	/** Target SVIP, MAC address */
++	__u8 locMAC[ETH_ALEN];
++	/** Target SVIP, UDP port number */
++	SVIP_UDP_PORT_t locUDP;
++} SVIP_NAT_IO_Rule_t;
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++/** SVIP NAT IP filter structure */
++typedef struct SVIP_NAT_DESTIP {
++	/** address type */
++	SVIP_IP_ADDR_TYPE_t typeIP;
++	/** IP address */
++	SVIP_IP_ADDR_UNION_t IP;
++} SVIP_NAT_DESTIP_t;
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++
++/** @} */
++#endif
+diff --git a/net/Kconfig b/net/Kconfig
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -87,6 +87,71 @@ config IFX_UDP_REDIRECT
+ 	  UDP redirection.
+ 
+ if INET
++
++menu "VINETIC-SVIP NAT options"
++
++config LTQ_SVIP_NAT
++	default y
++	bool "VINETIC-SVIP NAT"
++	depends on SOC_SVIP
++	---help---
++	Performs MAC and IP address translation of incoming and ougoing
++	IP packets relative the address mapping details provided by the
++	SVIP NAT rules. The packets will be intercept in the IP module and
++	when an appropriate NAT rule exists the source and destination address
++	details are replaced, and the packets are sent out the destined Ethernet
++	interface.
++	This NAT solution is tailored for Lantiq VINETIC-SVIP VoIP applications.
++
++config LTQ_SVIP_NAT_DESTIP_CHECK
++	bool "SVIP NAT destination IP filter"
++	depends on LTQ_SVIP_NAT
++	default y
++	---help---
++	Enables NAT filtering for LTQ_SVIP_NAT depending on IP address. NAT is
++	performed only on packets whose destination IP addresses have been added to
++	the allowed address list. Addition to and removal from the list is done
++	using svip_nat application.
++
++config LTQ_SVIP_NAT_DESTIP_LIST_SIZE
++	int "SVIP NAT destination IP filter list length"
++	depends on LTQ_SVIP_NAT_DESTIP_CHECK
++	default 10
++	---help---
++	Length of SVIP NAT destination IP filter list. Specifies the number of IP
++	addresses that can be stored in the allowed address list.
++
++config LTQ_SVIP_NAT_RULES_TOTAL
++	int "SVIP NAT table total rules"
++	depends on LTQ_SVIP_NAT
++	default 192
++	---help---
++	Defines the size of the VINETIC-SVIP NAT table i.e. the total number of NAT
++	rules. In order to select a correct size for the table, one has to consider
++	the number of VINETIC-SVIP devices available on the system and the total
++	number of analog channels available on the devices.
++	For example: for a system consisting of one VINETIC-SVIP16 device and one
++	VINETIC-SVIP8, one would need (16+8)*3=72 rules.
++	Explanation: for each analog channel there are twice as many coder channels
++	available where for each coder one rule applies. That is, for a VINETIC-SVIP16,
++	a device with 16 analog channels one needs 16*2=32 rules.
++	Additionally, one needs to allocate a rule per analog channel if one uses
++	T.38 and for these streams one assigns a unique UDP port. That is how we
++	come to a total number equaling 3 times the number of analog channels.
++
++config LTQ_SVIP_NAT_UDP_PORT_BASE
++	int "SVIP NAT UDP port base number"
++	depends on LTQ_SVIP_NAT
++	default 50000
++	---help---
++	Defines the base UDP port number to be used in the rules.
++	The VINETIC-SVIP NAT table is a simple array of rules. The rules are simply
++	indexed relative the locUDP number. E.g. rule which locUDP port equals the base
++	UDP port is stored at index zero, the rule which locUDP port equals the base
++	UDP port number plus one is stored at index 1, etc.
++
++endmenu
++
+ source "net/ipv4/Kconfig"
+ source "net/ipv6/Kconfig"
+ source "net/netlabel/Kconfig"
+diff --git a/net/ipv4/Makefile b/net/ipv4/Makefile
+--- a/net/ipv4/Makefile
++++ b/net/ipv4/Makefile
+@@ -58,3 +58,5 @@ obj-$(CONFIG_NETLABEL) += cipso_ipv4.o
+ 
+ obj-$(CONFIG_XFRM) += xfrm4_policy.o xfrm4_state.o xfrm4_input.o \
+ 		      xfrm4_output.o
++
++obj-$(CONFIG_LTQ_SVIP_NAT) += svip_nat.o
+diff --git a/net/ipv4/ip_input.c b/net/ipv4/ip_input.c
+--- a/net/ipv4/ip_input.c
++++ b/net/ipv4/ip_input.c
+@@ -145,6 +145,9 @@
+ #include <net/xfrm.h>
+ #include <linux/mroute.h>
+ #include <linux/netlink.h>
++#ifdef CONFIG_LTQ_SVIP_NAT
++#include <linux/svip_nat.h>
++#endif
+ 
+ /*
+  *	Process Router Attention IP option (RFC 2113)
+@@ -442,6 +445,13 @@ int ip_rcv(struct sk_buff *skb, struct n
+ 	/* Must drop socket now because of tproxy. */
+ 	skb_orphan(skb);
+ 
++#ifdef CONFIG_LTQ_SVIP_NAT
++	if (do_SVIP_NAT(skb)) {
++		/* SVIP NAT performed, receving successful */
++		return NET_RX_SUCCESS;
++	}
++#endif
++
+ 	return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL,
+ 		       ip_rcv_finish);
+ 
+diff --git a/net/ipv4/svip_nat.c b/net/ipv4/svip_nat.c
+new file mode 100644
+--- /dev/null
++++ b/net/ipv4/svip_nat.c
+@@ -0,0 +1,2561 @@
++/******************************************************************************
++**
++** FILE NAME	: svip_nat.c
++** PROJECT	: Lantiq voice co
++** MODULES	: Lantiq VINETIC-SVIP NAT
++** DATE		: 13 August 2014
++** AUTHOR	: Martins Pukitis
++** DESCRIPTION	: Lantiq VINETIC-SVIP NAT implementation
++** COPYRIGHT	: Copyright (c) 2014
++**		  Lantiq Deutschland
++**
++**	This program is free software; you can redistribute it and/or modify
++**	it under the terms of the GNU General Public License as published by
++**	the Free Software Foundation; either version 2 of the License, or
++**	(at your option) any later version.
++**
++**	Adapted from VINETIC-SVIP NAT written by Risto Minev
++**
++** HISTORY
++** $Date                $Author            $Comment
++** 13 Aug 2014          Martins Pukitis    Initial version (no ethtool support)
++*******************************************************************************/
++
++#include <linux/module.h>
++#include <linux/netfilter_ipv4.h>
++#include <linux/if_ether.h>
++#include <linux/netdevice.h>
++#include <linux/inetdevice.h>
++#include <linux/in.h>
++#include <linux/ip.h>
++#include <linux/ipv6.h>
++#include <net/ipv6.h>
++#include <net/addrconf.h>
++#include <linux/if_vlan.h>
++#include <linux/udp.h>
++#include <linux/kernel.h>
++#include <linux/version.h>
++#include <linux/proc_fs.h>
++#include <linux/seq_file.h>
++#include <linux/in6.h>
++#include <linux/miscdevice.h>
++#include <asm/checksum.h>
++#include "../8021q/vlan.h"
++
++#include <linux/svip_nat.h>
++
++MODULE_AUTHOR("Lantiq Deutschland GmbH");
++MODULE_DESCRIPTION("SVIP Network Address Translation module");
++MODULE_LICENSE("GPL");
++
++#define SVIP_NAT_INFO_STR "@(#)SVIP NAT, version "SVIP_NAT_VERSION
++
++#define SVIP_UDP_FROM  (nat_udp_port_base)
++#define SVIP_UDP_TO    ((nat_udp_port_base)+(CONFIG_LTQ_SVIP_NAT_RULES_TOTAL)-1)
++
++#define SVIP_PORT_INRANGE(port) \
++	((port) >= (SVIP_UDP_FROM) && (port) <= (SVIP_UDP_TO))
++
++#define SVIP_PORT_INDEX(port)   (port - SVIP_UDP_FROM)
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++#define SVIP_NAT_DESTIP_LIST_SIZE   (CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE)
++#endif
++
++#define SVIP_NET_DEV_ETH0_IDX       0
++#define SVIP_NET_DEV_ETH1_IDX       1
++#define SVIP_NET_DEV_VETH0_IDX      2
++#define SVIP_NET_DEV_LO_IDX         3
++
++#define SVIP_NET_DEV_ETH0_NAME      "eth0"
++#define SVIP_NET_DEV_ETH1_NAME      "eth1"
++#define SVIP_NET_DEV_VETH0_NAME     "veth0"
++#define SVIP_NET_DEV_LO_NAME        "lo"
++
++#define SVIP_NAT_STATS_LOC2REM      0
++#define SVIP_NAT_STATS_REM2LOC      1
++#define SVIP_NAT_STATS_TYPES        2
++
++#if !(defined(CONFIG_VLAN_8021Q) || defined(CONFIG_VLAN_8021Q_MODULE))
++#define VLAN_8021Q_UNUSED
++#endif
++
++struct SVIP_NAT_stats {
++	unsigned long in_packets;
++	unsigned long out_packets;
++	unsigned long out_errors;
++};
++
++struct SVIP_NAT_table_entry {
++	SVIP_NAT_IO_Rule_t nat_rule;
++	struct SVIP_NAT_stats nat_stats[SVIP_NAT_STATS_TYPES];
++};
++
++/* pointer to the SVIP NAT table */
++static struct SVIP_NAT_table_entry *nat_table;
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++/* IP address arrays */
++static SVIP_IPV4_ADDR_t nat_dest_IPv4[CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE];
++static int nat_dest_IPv4_size;
++static SVIP_IPV6_ADDR_t nat_dest_IPv6[CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE];
++static int nat_dest_IPv6_size;
++#endif
++/* base UDP port number */
++static SVIP_UDP_PORT_t nat_udp_port_base = CONFIG_LTQ_SVIP_NAT_UDP_PORT_BASE;
++
++struct net_device *net_devs[3];
++static u32 *paddr_eth0;
++static u32 *paddr_eth1;
++static u32 *pmask_eth1;
++static u32 *paddr_veth0;
++static u32 *pmask_veth0;
++
++struct in6_addr *paddr_eth0_ip6 = NULL, *paddr_eth1_ip6 = NULL,
++	*paddr_veth0_ip6 = NULL;
++static u32 prefix_len_eth1;
++static u32 prefix_len_veth0;
++
++static struct semaphore *sem_nat_tbl_access;
++
++static int device_open;
++
++static long SVIP_NAT_device_ioctl(struct file *file, unsigned int ioctl_num,
++	unsigned long ioctl_param);
++static int SVIP_NAT_device_release(struct inode *, struct file *);
++static int SVIP_NAT_device_open(struct inode *, struct file *);
++
++/* This structure holds the interface functions supported by
++ * the SVIP NAT configuration device.
++ */
++const struct file_operations SVIP_NAT_fops = {
++	.owner = THIS_MODULE, .llseek = NULL, /* seek */
++	.read = NULL, .write = NULL, .readdir = NULL, /* readdir */
++	.poll = NULL, /* select */
++	.unlocked_ioctl = SVIP_NAT_device_ioctl, .mmap = NULL, /* mmap */
++	.open = SVIP_NAT_device_open, /* open, */
++	.flush = NULL, /* flush */
++	.release = SVIP_NAT_device_release /* close */
++};
++
++/** Structure holding MISC module operations */
++static struct miscdevice SVIP_NAT_miscdev = {
++	.minor = MINOR_NUM_SVIP_NAT,
++	.name = SVIP_NAT_DEVICE_NAME,
++	.fops = &SVIP_NAT_fops
++};
++
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++int SVIP_NAT_sniffer;
++unsigned char SVIP_NAT_sniffer_MAC[ETH_ALEN];
++int SVIP_NAT_sniffer_MAC_set;
++#endif
++
++/* single call read proc entry callback function */
++typedef void (*proc_single_callback_t)(struct seq_file *);
++/* multiple call read proc entry callback function */
++typedef int (*proc_callback_t)(struct seq_file *, int);
++/* write function */
++typedef ssize_t (*proc_write_t)(struct file *file, const char __user *buffer,
++	size_t count, loff_t *data);
++
++static void *SVIP_NAT_seq_start(struct seq_file *s, loff_t *pos);
++static void *SVIP_NAT_seq_next(struct seq_file *s, void *v, loff_t *pos);
++static void SVIP_NAT_seq_stop(struct seq_file *s, void *v);
++static int SVIP_NAT_seq_show(struct seq_file *s, void *v);
++
++struct proc_file_entry {
++	/* function used for data output */
++	proc_callback_t callback;
++	/* current output position */
++	int pos;
++	/* maximum output position */
++	int max_pos;
++};
++
++struct proc_entry {
++	char *name;
++	proc_single_callback_t single_callback;
++	proc_callback_t callback;
++	int max_pos;
++	proc_write_t write_function;
++	struct file_operations ops;
++};
++
++static const struct seq_operations SVIP_NAT_seq_ops = {
++	.start = SVIP_NAT_seq_start,
++	.next = SVIP_NAT_seq_next,
++	.stop = SVIP_NAT_seq_stop,
++	.show = SVIP_NAT_seq_show
++};
++
++static int SVIP_NAT_proc_read_NAT(struct seq_file *s, int pos);
++static int SVIP_NAT_read_NAT(char *buf, int count, int pos);
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++static int SVIP_NAT_proc_read_dest_IP(struct seq_file *s, int pos);
++static int SVIP_NAT_read_dest_IP(char *buf, int count, int pos);
++#endif
++static void SVIP_NAT_proc_read_port_base(struct seq_file *s);
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++static void SVIP_NAT_proc_read_sniffer_MAC(struct seq_file *s);
++static ssize_t SVIP_NAT_proc_write_sniffer_MAC(struct file *file,
++	const char __user *buffer,
++	size_t count, loff_t *data);
++static void SVIP_NAT_proc_read_sniffer_on_off(struct seq_file *s);
++static ssize_t SVIP_NAT_proc_write_sniffer_on_off(struct file *file,
++	const char __user *buffer, size_t count, loff_t *data);
++#endif
++
++static struct proc_entry proc_entries[] = {
++	{ "nat", NULL, SVIP_NAT_proc_read_NAT,
++		CONFIG_LTQ_SVIP_NAT_RULES_TOTAL - 1, NULL },
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	{ "destip", NULL, SVIP_NAT_proc_read_dest_IP,
++		(SVIP_NAT_DESTIP_LIST_SIZE << 1) - 1, NULL },
++#endif
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++	{"snifferMAC", SVIP_NAT_proc_read_sniffer_MAC, NULL, 0,
++		SVIP_NAT_proc_write_sniffer_MAC},
++	{"snifferOnOff", SVIP_NAT_proc_read_sniffer_on_off, NULL, 0,
++		SVIP_NAT_proc_write_sniffer_on_off},
++#endif
++	{ "portbase", SVIP_NAT_proc_read_port_base, NULL, 0, NULL }
++};
++
++/**
++
++ Read NAT table from the driver.
++
++ \param s
++ \param pos  NAT entry to print
++
++ \return
++ zero or an error code
++ */
++static int SVIP_NAT_proc_read_NAT(struct seq_file *s, int pos)
++{
++	return SVIP_NAT_read_NAT((char *) s, 0, pos);
++}
++
++/******************************************************************************/
++/**
++ Function for reading /proc/net/svip_nat/nat
++
++ \arguments
++ buf   - pointer to read buffer or seq_file structure
++ count - size of read buffer. If size is 0 then buf is pointer to seq_file
++ structure
++ pos   - NAT entry to print
++
++ \return
++ if buf is pointer to buffer, length of read data into buffer, else 0 or error
++ code
++
++*******************************************************************************/
++static int SVIP_NAT_read_NAT(char *buf, int count, int pos)
++{
++	int j;
++	int slen;
++	SVIP_NAT_IO_Rule_t *nat_rule;
++
++	if (count == 0) {
++		/* write to seq_file */
++		struct seq_file *s = (struct seq_file *) buf;
++		char tmp[40];
++
++		if (pos == 0) {
++			seq_puts(s, "rem MAC            " /* 19 char */
++				"rem IP                                   "
++				/* 41 char */
++				"loc MAC            " /* 19 char */
++				"loc IP                                   "
++				/* 41 char */
++				"loc UDP  " /* 9 char */
++				"loc->rem(in/out/err)  " /* 22 char */
++				"rem->loc(in/out/err)\n\r"); /* 22 char  */
++			/* 173 total */
++
++			seq_puts(s, "-----------------  " /* 19 char */
++				"---------------------------------------  "
++				/* 41 char */
++				"-----------------  " /* 19 char */
++				"---------------------------------------  "
++				/* 41 char */
++				"-------  " /* 9 char */
++				"--------------------  " /* 22 char */
++				"--------------------\n\r");
++		}
++
++		nat_rule = &nat_table[pos].nat_rule;
++
++		if (nat_rule->locUDP == 0)
++			return 0;
++
++		/* remMAC */
++		slen = 0;
++		for (j = 0; j < ETH_ALEN; j++) {
++			slen += sprintf(tmp + slen, "%02x%s",
++				nat_rule->remMAC[j],
++				j < ETH_ALEN - 1 ? ":" : " ");
++		}
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (19 - slen); j++)
++			seq_puts(s, " ");
++
++		/* remIP */
++		if (nat_rule->typeIP == SVIP_IPV4_ADDR_TYPE) {
++			slen = sprintf(tmp, "%d.%d.%d.%d",
++				(ntohl(nat_rule->remIP.v4) >> 24) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 16) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 8) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 0) & 0xff);
++		} else {
++			slen = sprintf(tmp,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x",
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[0]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[1]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[2]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[3]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[4]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[5]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[6]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[7]));
++		}
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (41 - slen); j++)
++			seq_puts(s, " ");
++
++		/* locMAC */
++		slen = 0;
++		for (j = 0; j < ETH_ALEN; j++) {
++			slen += sprintf(tmp + slen, "%02x%s",
++				nat_rule->locMAC[j],
++				j < ETH_ALEN - 1 ? ":" : " ");
++		}
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (19 - slen); j++)
++			seq_puts(s, " ");
++
++		/* locIP */
++		if (nat_rule->typeIP == SVIP_IPV4_ADDR_TYPE) {
++			slen = sprintf(tmp, "%d.%d.%d.%d",
++				(ntohl(nat_rule->locIP.v4) >> 24) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 16) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 8) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 0) & 0xff);
++		} else {
++			slen = sprintf(tmp,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x",
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[0]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[1]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[2]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[3]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[4]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[5]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[6]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[7]));
++		}
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (41 - slen); j++)
++			seq_puts(s, " ");
++
++		/* locUDP */
++		slen = sprintf(tmp, "%d", nat_rule->locUDP);
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (9 - slen); j++)
++			seq_puts(s, " ");
++
++		/* NAT statistics, Local to Remote translation */
++		slen = sprintf(tmp, "(%ld/%ld/%ld)",
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			in_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			out_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			out_errors);
++		seq_printf(s, "%s", tmp);
++		for (j = 0; j < (22 - slen); j++)
++			seq_puts(s, " ");
++
++		/* NAT statistics, Remote to Local translation */
++		seq_printf(s, "(%ld/%ld/%ld)\n\r",
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			in_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			out_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			out_errors);
++	} else {
++		/* write to memory buffer */
++		int len = 0;
++
++		if (pos == 0) {
++			len = sprintf(buf + len,
++				"rem MAC            " /* 19 char */
++				"rem IP                                   "
++				/* 41 char */
++				"loc MAC            " /* 19 char */
++				"loc IP                                   "
++				/* 41 char */
++				"loc UDP  " /* 9 char */
++				"loc->rem(in/out/err)  " /* 22 char */
++				"rem->loc(in/out/err)\n\r"); /* 22 char */
++			/* 173 total */
++
++			len += sprintf(buf + len,
++				"-----------------  " /* 19 char */
++				"---------------------------------------  "
++				/* 41 char */
++				"-----------------  " /* 19 char */
++				"---------------------------------------  "
++				/* 41 char */
++				"-------  " /* 9 char */
++				"--------------------  " /* 22 char */
++				"--------------------\n\r");
++		}
++
++		nat_rule = &nat_table[pos].nat_rule;
++
++		if (nat_rule->locUDP == 0)
++			return len;
++
++		/* make sure not to overwrite the buffer */
++		if (count < len + 173)
++			return len;
++
++		/* remMAC */
++		slen = 0;
++		for (j = 0; j < ETH_ALEN; j++) {
++			slen += sprintf(buf + len + slen, "%02x%s",
++				nat_rule->remMAC[j],
++				j < ETH_ALEN - 1 ? ":" : " ");
++		}
++		len += slen;
++		for (j = 0; j < (19 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* remIP */
++		if (nat_rule->typeIP == SVIP_IPV4_ADDR_TYPE) {
++			slen = sprintf(buf + len, "%d.%d.%d.%d",
++				(ntohl(nat_rule->remIP.v4) >> 24) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 16) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 8) & 0xff,
++				(ntohl(nat_rule->remIP.v4) >> 0) & 0xff);
++			len += slen;
++		} else {
++			slen = sprintf(buf + len,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x",
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[0]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[1]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[2]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[3]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[4]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[5]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[6]),
++				ntohs(nat_rule->remIP.v6.ipv6_addr16[7]));
++			len += slen;
++		}
++		for (j = 0; j < (41 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* locMAC */
++		slen = 0;
++		for (j = 0; j < ETH_ALEN; j++) {
++			slen += sprintf(buf + len + slen, "%02x%s",
++				nat_rule->locMAC[j],
++				j < ETH_ALEN - 1 ? ":" : " ");
++		}
++		len += slen;
++		for (j = 0; j < (19 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* locIP */
++		if (nat_rule->typeIP == SVIP_IPV4_ADDR_TYPE) {
++			slen = sprintf(buf + len, "%d.%d.%d.%d",
++				(ntohl(nat_rule->locIP.v4) >> 24) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 16) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 8) & 0xff,
++				(ntohl(nat_rule->locIP.v4) >> 0) & 0xff);
++			len += slen;
++		} else {
++			slen = sprintf(buf + len,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x",
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[0]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[1]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[2]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[3]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[4]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[5]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[6]),
++				ntohs(nat_rule->locIP.v6.ipv6_addr16[7]));
++			len += slen;
++		}
++		for (j = 0; j < (41 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* locUDP */
++		slen = sprintf(buf + len, "%d", nat_rule->locUDP);
++		len += slen;
++		for (j = 0; j < (9 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* NAT statistics, Local to Remote translation */
++		slen = sprintf(buf + len, "(%ld/%ld/%ld)",
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			in_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			out_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_LOC2REM].
++			out_errors);
++		len += slen;
++		for (j = 0; j < (22 - slen); j++)
++			len += sprintf(buf + len, " ");
++
++		/* NAT statistics, Remote to Local translation */
++		len += sprintf(buf + len, "(%ld/%ld/%ld)\n\r",
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			in_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			out_packets,
++			nat_table[pos].nat_stats[SVIP_NAT_STATS_REM2LOC].
++			out_errors);
++
++		return len;
++	}
++
++	return 0;
++}
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++
++/**
++
++ Read allowed destination IPs from the driver.
++
++ \param s
++ \param pos  IP entry to print
++
++ \return
++ zero or an error code
++ */
++static int SVIP_NAT_proc_read_dest_IP(struct seq_file *s, int pos)
++{
++	if (pos < nat_dest_IPv4_size + nat_dest_IPv6_size)
++		return SVIP_NAT_read_dest_IP((char *) s, 0, pos);
++	else
++		return 0;
++}
++
++/******************************************************************************/
++/**
++ Function for reading /proc/net/svip_nat/destip
++
++ \arguments
++ buf   - pointer to read buffer or seq_file structure
++ count - size of read buffer. If size is 0 then buf is pointer to seq_file
++ structure
++ pos   - IP entry to print
++
++ \return
++ if buf is pointer to buffer, length of read data into buffer, else 0 or error
++ code
++*******************************************************************************/
++static int SVIP_NAT_read_dest_IP(char *buf, int count, int pos)
++{
++	if (count == 0) {
++		/* write to seq_file */
++		struct seq_file *s = (struct seq_file *) buf;
++
++		if (pos == 0)
++			seq_printf(s,
++				"Allowed NAT destination IP addresses:\n\r");
++		if (pos < nat_dest_IPv4_size)
++		{
++			seq_printf(s, "%d.%d.%d.%d\n\r",
++				(ntohl(nat_dest_IPv4[pos]) >> 24) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 16) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 8) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 0) & 0xff);
++		}
++		else
++			seq_printf(s,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n\r",
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[0]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[1]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[2]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[3]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[4]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[5]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[6]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[7]));
++	} else {
++		int len = 0;
++
++		if (pos == 0)
++			len = sprintf(buf + len,
++				"Allowed NAT destination IP addresses:\n\r");
++
++		if (pos < nat_dest_IPv4_size) {
++			/* make sure not to overwrite the buffer */
++			if (count < len + 18) {
++				pr_err("SVIP NAT: Only part of the text could be put into the buffer\n");
++				return len;
++			}
++			len += sprintf(buf + len, "%d.%d.%d.%d\n\r",
++				(ntohl(nat_dest_IPv4[pos]) >> 24) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 16) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 8) & 0xff,
++				(ntohl(nat_dest_IPv4[pos]) >> 0) & 0xff);
++		} else {
++			/* make sure not to overwrite the buffer */
++			if (count < len + 42) {
++				pr_err("SVIP NAT: Only part of the text could be put into the buffer\n");
++				return len;
++			}
++			len += sprintf(buf + len,
++				"%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n\r",
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[0]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[1]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[2]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[3]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[4]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[5]),
++				ntohs(nat_dest_IPv6[pos - nat_dest_IPv4_size].
++					ipv6_addr16[6]),
++				ntohs(nat_dest_IPv6[pos	 - nat_dest_IPv4_size].
++					ipv6_addr16[7]));
++		}
++		return len;
++	}
++	return 0;
++}
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++
++/******************************************************************************/
++/**
++ Function for reading /proc/net/svip_nat/portbase
++
++ \arguments
++ s
++
++ \return
++ none
++
++ \remarks:
++ ******************************************************************************/
++static void SVIP_NAT_proc_read_port_base(struct seq_file *s)
++{
++	seq_printf(s, "%d\n\r", nat_udp_port_base);
++}
++
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++/**
++ Converts MAC address from ASCII to hexadecimal representation
++ */
++static int SVIP_NAT_MAC_ASCII_2_hex(const char *MAC_str, unsigned char *MAC_hex)
++{
++	int i = 0, c = 0, b = 0, n = 0;
++
++	memset(MAC_hex, 0, ETH_ALEN);
++	while (MAC_str[i] != '\0') {
++		if (n >= 0) {
++			unsigned char to_hex = 0;
++
++			/* check for hex digit */
++			if (MAC_str[i] >= '0' && MAC_str[i] <= '9')
++				to_hex = 0x30;
++			else if (MAC_str[i] >= 'a' && MAC_str[i] <= 'f')
++				to_hex = 0x57;
++			else if (MAC_str[i] >= 'A' && MAC_str[i] <= 'F')
++				to_hex = 0x37;
++			else {
++				if (n != 0) {
++					pr_err("SVIP NAT: invalid MAC address format[%s]\n",
++						MAC_str);
++					return -1;
++				}
++				i++;
++				continue;
++			}
++			n ^= 1;
++			MAC_hex[b] |= ((MAC_str[i] - to_hex)&0xf) << (4*n);
++			if (n == 0) {
++				/* advance to next byte, check if complete */
++				if (++b >= ETH_ALEN)
++					return 0;
++				/* byte completed, next we expect a colon... */
++				c = 1;
++				/* and, do not check for hex digit */
++				n = -1;
++			}
++			i++;
++			continue;
++		}
++		if (c == 1) {
++			if (MAC_str[i] == ':') {
++				/* next we expect hex digit, again */
++				n = 0;
++			} else {
++				pr_err("SVIP NAT: invalid MAC address format[%s]\n",
++					MAC_str);
++				return -1;
++			}
++		}
++		i++;
++	}
++	return 0;
++}
++
++/**
++ Used to set the destination MAC address of a host where incoming
++ SVIP VoFW packets are to be addressed. In case the address is set
++ to 00:00:00:00:00:00 (the default case), the packets will written
++ out to eth0 with its original MAC addess.
++
++ \remark
++ usage: 'echo "00:03:19:00:15:D1" > cat /proc/net/svip_nat/snifferMAC'
++ */
++static ssize_t SVIP_NAT_proc_write_sniffer_MAC(struct file *file,
++	const char __user *buffer, size_t count, loff_t *data)
++{
++	/* at least strlen("xx:xx:xx:xx:xx:xx") characters, followed by '\0' */
++	if (count >= 18) {
++		int ret;
++
++		ret = SVIP_NAT_MAC_ASCII_2_hex(buffer, SVIP_NAT_sniffer_MAC);
++
++		if (ret != 0)
++			return 0;
++
++		if (!(SVIP_NAT_sniffer_MAC[0] == 0 &&
++			SVIP_NAT_sniffer_MAC[1] == 0 &&
++			SVIP_NAT_sniffer_MAC[2] == 0 &&
++			SVIP_NAT_sniffer_MAC[3] == 0 &&
++			SVIP_NAT_sniffer_MAC[4] == 0 &&
++			SVIP_NAT_sniffer_MAC[5] == 0))
++			SVIP_NAT_sniffer_MAC_set = 1;
++	}
++	return count;
++}
++
++/******************************************************************************/
++/**
++ Function for reading /proc/net/svip_nat/snifferMAC
++
++ \arguments
++ \param s
++
++ \return
++ none
++
++ \remarks:
++ ******************************************************************************/
++static void SVIP_NAT_proc_read_sniffer_MAC(struct seq_file *s)
++{
++	seq_printf(s, "%02x:%02x:%02x:%02x:%02x:%02x\n",
++		SVIP_NAT_sniffer_MAC[0], SVIP_NAT_sniffer_MAC[1],
++		SVIP_NAT_sniffer_MAC[2], SVIP_NAT_sniffer_MAC[3],
++		SVIP_NAT_sniffer_MAC[4], SVIP_NAT_sniffer_MAC[5]);
++}
++
++/**
++ Used to switch VoFW message sniffer on/off
++
++ \remark
++ usage: 'echo "1" > cat /proc/net/svip_nat/snifferOnOff'
++ */
++static ssize_t SVIP_NAT_proc_write_sniffer_on_off(struct file *file,
++	const char __user *buffer,
++	size_t count, loff_t *data)
++{
++	/* at least one digit expected, followed by '\0' */
++	if (count >= 2) {
++		int ret, sniffer;
++
++		ret = sscanf(buffer, "%d", &sniffer);
++
++		if (ret != 1)
++			return count;
++
++		if (sniffer > 0)
++			sniffer = 1;
++
++		SVIP_NAT_sniffer = sniffer;
++	}
++	return count;
++}
++
++/******************************************************************************/
++/**
++ Function for reading /proc/net/svip_nat/snifferOnOff
++
++ \arguments
++ \param s
++
++ \return
++ none
++
++ \remarks:
++*******************************************************************************/
++static void SVIP_NAT_proc_read_sniffer_on_off(struct seq_file *s)
++{
++	seq_printf(s, "%d\n", SVIP_NAT_sniffer);
++}
++#endif
++
++static void *SVIP_NAT_seq_start(struct seq_file *s, loff_t *pos)
++{
++	struct proc_file_entry *p = s->private;
++
++	if (*pos == 0 || *pos > p->max_pos)
++		down(sem_nat_tbl_access);
++
++	if (*pos > p->max_pos || *pos < 0)
++		return NULL;
++
++	/* set current position */
++	p->pos = *pos;
++
++	return p;
++}
++
++static void *SVIP_NAT_seq_next(struct seq_file *s, void *v, loff_t *pos)
++{
++	struct proc_file_entry *p = s->private;
++
++	(*pos)++;
++	p->pos = *pos;
++	if (*pos > p->max_pos || *pos < 0)
++		return NULL;
++
++	return p;
++}
++
++static void SVIP_NAT_seq_stop(struct seq_file *s, void *v)
++{
++	struct proc_file_entry *p = s->private;
++
++	if (p->pos > p->max_pos)
++		up(sem_nat_tbl_access);
++}
++
++static int SVIP_NAT_seq_show(struct seq_file *s, void *v)
++{
++	struct proc_file_entry *p = s->private;
++
++	return p->callback(s, p->pos);
++}
++
++static int SVIP_NAT_proc_open(struct inode *inode, struct file *file)
++{
++	struct seq_file *s;
++	struct proc_file_entry *p;
++	struct proc_entry *entry;
++	int ret;
++
++	ret = seq_open(file, &SVIP_NAT_seq_ops);
++	if (ret)
++		return ret;
++
++	s = file->private_data;
++	p = kmalloc(sizeof(*p), GFP_KERNEL);
++
++	if (!p) {
++		(void) seq_release(inode, file);
++		return -ENOMEM;
++	}
++
++	entry = PDE_DATA(inode);
++
++	p->callback = entry->callback;
++	if (entry->callback)
++		p->max_pos = entry->max_pos;
++	else
++		p->max_pos = 0;
++
++	s->private = p;
++
++	return 0;
++}
++
++static int SVIP_NAT_proc_release(struct inode *inode, struct file *file)
++{
++	struct seq_file *s;
++
++	s = file->private_data;
++	kfree(s->private);
++
++	return seq_release(inode, file);
++}
++
++static int SVIP_NAT_seq_single_show(struct seq_file *s, void *v)
++{
++	struct proc_entry *p = s->private;
++
++	p->single_callback(s);
++	return 0;
++}
++
++static int SVIP_NAT_proc_single_open(struct inode *inode, struct file *file)
++{
++	return single_open(file, SVIP_NAT_seq_single_show, PDE_DATA(inode));
++}
++
++static void SVIP_NAT_proc_entry_create(struct proc_dir_entry *parent_node,
++	struct proc_entry *proc_entry)
++{
++	mode_t mode = S_IFREG | S_IRUGO;
++
++	memset(&proc_entry->ops, 0, sizeof(const struct file_operations));
++	proc_entry->ops.owner = THIS_MODULE;
++
++	if (proc_entry->single_callback) {
++		proc_entry->ops.open = SVIP_NAT_proc_single_open;
++		proc_entry->ops.release = single_release;
++	} else {
++		proc_entry->ops.open = SVIP_NAT_proc_open;
++		proc_entry->ops.release = SVIP_NAT_proc_release;
++	}
++
++	proc_entry->ops.read = seq_read;
++	proc_entry->ops.write = proc_entry->write_function;
++	if (proc_entry->write_function != NULL)
++		mode |= S_IWUGO;
++	proc_entry->ops.llseek = seq_lseek;
++
++	proc_create_data(proc_entry->name, mode, parent_node, &proc_entry->ops,
++		proc_entry);
++}
++
++/******************************************************************************/
++/**
++ Creates proc read/write entries
++
++ \return
++ 0 on success, -1 on error
++ */
++/******************************************************************************/
++static int SVIP_NAT_proc_install(void)
++{
++	struct proc_dir_entry *proc_parent_dir, *proc_dir;
++	int i;
++
++	proc_parent_dir = init_net.proc_net;
++	proc_dir = proc_mkdir(SVIP_NAT_DEVICE_NAME, proc_parent_dir);
++	if (proc_dir == NULL) {
++		pr_err("SVIP NAT: cannot create "SVIP_NAT_DEVICE_NAME
++			" proc directory\n\r");
++		return -1;
++	}
++
++	for (i = 0; i < ARRAY_SIZE(proc_entries); i++)
++		SVIP_NAT_proc_entry_create(proc_dir, &proc_entries[i]);
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++	SVIP_NAT_sniffer = 0;
++	memset(SVIP_NAT_sniffer_MAC, 0, ETH_ALEN);
++	SVIP_NAT_sniffer_MAC_set = 0;
++#endif
++
++	return 0;
++}
++
++/******************************************************************************/
++/**
++ No actions done here, simply a check is performed if an open has already
++ been performed. Currently only a single open is allowed as it is a sufficient
++ to have hat a single process configuring the SVIP NAT at one time.
++
++ \arguments
++ inode   - pointer to disk file data
++ file    - pointer to device file data
++
++ \return
++ 0 on success, else -1
++ */
++/******************************************************************************/
++static int SVIP_NAT_device_open(struct inode *inode, struct file *file)
++{
++	unsigned long flags;
++	struct in_device *in_dev;
++	struct inet6_dev *in_dev_ip6;
++	struct in_ifaddr *ifa;
++	struct inet6_ifaddr *ifa_ip6;
++
++	local_save_flags(flags);
++	if (device_open) {
++		local_irq_restore(flags);
++		return 0;
++	}
++
++	/* find pointer to IP address of eth0 */
++	in_dev = in_dev_get(net_devs[SVIP_NET_DEV_ETH0_IDX]);
++	if (in_dev) {
++		for (ifa = in_dev->ifa_list; ifa != NULL; ifa = ifa->ifa_next) {
++			if (ifa->ifa_address != 0) {
++				paddr_eth0 = &ifa->ifa_address;
++				break;
++			}
++		}
++		in_dev_put(in_dev);
++	}
++	if (paddr_eth0 == NULL) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++	/* find pointer to IPv6 address of eth0 */
++	in_dev_ip6 = in6_dev_get(net_devs[SVIP_NET_DEV_ETH0_IDX]);
++	if (in_dev_ip6) {
++		read_lock_bh(&in_dev_ip6->lock);
++		list_for_each_entry(ifa_ip6, &in_dev_ip6->addr_list, if_list) {
++			if (!ipv6_addr_any(&ifa_ip6->addr)) {
++				paddr_eth0_ip6 = &ifa_ip6->addr;
++				break;
++			}
++		}
++		read_unlock_bh(&in_dev_ip6->lock);
++		in6_dev_put(in_dev_ip6);
++	}
++	if (ipv6_addr_any(paddr_eth0_ip6)) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++
++	/* find pointer to IP address of eth1 */
++	in_dev = in_dev_get(net_devs[SVIP_NET_DEV_ETH1_IDX]);
++	if (in_dev) {
++		for (ifa = in_dev->ifa_list; ifa != NULL; ifa = ifa->ifa_next) {
++			if (ifa->ifa_address != 0) {
++				paddr_eth1 = &ifa->ifa_address;
++				pmask_eth1 = &ifa->ifa_mask;
++				break;
++			}
++		}
++		in_dev_put(in_dev);
++	}
++	if (paddr_eth1 == NULL) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++	/* find pointer to IPv6 address of eth1 */
++	in_dev_ip6 = in6_dev_get(net_devs[SVIP_NET_DEV_ETH1_IDX]);
++	if (in_dev_ip6) {
++		read_lock_bh(&in_dev_ip6->lock);
++		list_for_each_entry(ifa_ip6, &in_dev_ip6->addr_list, if_list) {
++			if (!ipv6_addr_any(&ifa_ip6->addr)) {
++				paddr_eth1_ip6 = &ifa_ip6->addr;
++				prefix_len_eth1 = ifa_ip6->prefix_len;
++				break;
++			}
++		}
++		read_unlock_bh(&in_dev_ip6->lock);
++		in6_dev_put(in_dev_ip6);
++	}
++	if (ipv6_addr_any(paddr_eth1_ip6)) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++
++	/* find pointer to IP address of veth0 */
++	in_dev = in_dev_get(net_devs[SVIP_NET_DEV_VETH0_IDX]);
++	if (in_dev) {
++		for (ifa = in_dev->ifa_list; ifa != NULL; ifa = ifa->ifa_next) {
++			if (ifa->ifa_address != 0) {
++				paddr_veth0 = &ifa->ifa_address;
++				pmask_veth0 = &ifa->ifa_mask;
++				break;
++			}
++		}
++		in_dev_put(in_dev);
++	}
++	if (paddr_veth0 == NULL) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++	/* find pointer to IPv6 address of veth0 and set its prefix length */
++	in_dev_ip6 = in6_dev_get(net_devs[SVIP_NET_DEV_VETH0_IDX]);
++	if (in_dev_ip6) {
++		read_lock_bh(&in_dev_ip6->lock);
++		list_for_each_entry(ifa_ip6, &in_dev_ip6->addr_list, if_list) {
++			if (!ipv6_addr_any(&ifa_ip6->addr)) {
++				paddr_veth0_ip6 = &ifa_ip6->addr;
++				prefix_len_veth0 = ifa_ip6->prefix_len;
++				break;
++			}
++		}
++		read_unlock_bh(&in_dev_ip6->lock);
++		in6_dev_put(in_dev_ip6);
++	}
++	if (paddr_veth0_ip6 != NULL && ipv6_addr_any(paddr_veth0_ip6)) {
++		local_irq_restore(flags);
++		return -ENODATA;
++	}
++
++	device_open++;
++	local_irq_restore(flags);
++
++	return 0;
++}
++
++/******************************************************************************/
++/**
++ This function is called when a process closes the SVIP NAT device file
++
++ \arguments
++ inode   - pointer to disk file data
++ file    - pointer to device file data
++
++ \return
++ 0 on success, else -1
++
++ */
++/******************************************************************************/
++static int SVIP_NAT_device_release(struct inode *inode, struct file *file)
++{
++	return 0;
++}
++
++
++/******************************************************************************/
++/**
++ This function is called when a process closes the SVIP NAT device file
++
++ \arguments
++ inode          - pointer to disk file data
++ file           - pointer to device file data
++ ioctl_num      - ioctl number requested
++ ioctl_param    - pointer to data related to the ioctl number
++
++ \return
++ 0 on success, else -1
++
++ */
++/******************************************************************************/
++long SVIP_NAT_device_ioctl(struct file *file, unsigned int ioctl_num,
++	unsigned long ioctl_param)
++{
++	int ret = 0;
++	SVIP_NAT_IO_Rule_t *nat_rule, *NAT_rule_in;
++	SVIP_UDP_PORT_t port;
++	int NAT_idx;
++	int write = 0;
++	int read = 0;
++	unsigned char *data = 0;
++	int size;
++
++	if (_IOC_DIR(ioctl_num) & _IOC_WRITE)
++		write = 1;
++	if (_IOC_DIR(ioctl_num) & _IOC_READ)
++		read = 1;
++	size = _IOC_SIZE(ioctl_num);
++
++	if (size > 0) {
++		if (read || write) {
++			data = kmalloc(size, GFP_KERNEL);
++			if (write) {
++				if (copy_from_user((void *)data,
++					(void *)ioctl_param, size) != 0) {
++					pr_err("SVIP NAT: ioctl %x: copy_from_user() failed!\n",
++						ioctl_num);
++					ret = -1;
++					goto error;
++				}
++			}
++		}
++	}
++
++	switch (ioctl_num) {
++	case FIO_SVIP_NAT_RULE_ADD:
++
++		NAT_rule_in = (SVIP_NAT_IO_Rule_t *) data;
++
++		/* check if destination UDP port is within range */
++		port = ntohs(NAT_rule_in->locUDP);
++
++		if (!SVIP_PORT_INRANGE(port)) {
++			pr_err("SVIP NAT: Error, UDP port(%d) is out of range(%d..%d)\n",
++				port, SVIP_UDP_FROM, SVIP_UDP_TO);
++			ret = -1;
++			goto error;
++		}
++		NAT_idx = SVIP_PORT_INDEX(port);
++
++		down(sem_nat_tbl_access);
++		nat_rule = &nat_table[NAT_idx].nat_rule;
++
++		/* add rule to the NAT table */
++		nat_rule->typeIP = NAT_rule_in->typeIP;
++		if (NAT_rule_in->typeIP == SVIP_IPV4_ADDR_TYPE) {
++			nat_rule->remIP = NAT_rule_in->remIP;
++			nat_rule->locIP = NAT_rule_in->locIP;
++		} else {
++			nat_rule->remIP.v6 = NAT_rule_in->remIP.v6;
++			nat_rule->locIP.v6 = NAT_rule_in->locIP.v6;
++		}
++		memcpy((char *)nat_rule->remMAC, (char *)NAT_rule_in->remMAC,
++			ETH_ALEN);
++		memcpy((char *)nat_rule->locMAC, (char *)NAT_rule_in->locMAC,
++			ETH_ALEN);
++		nat_rule->locUDP = NAT_rule_in->locUDP;
++
++		memset(nat_table[NAT_idx].nat_stats, 0,
++			sizeof(struct SVIP_NAT_stats)*SVIP_NAT_STATS_TYPES);
++		up(sem_nat_tbl_access);
++		break;
++
++	case FIO_SVIP_NAT_RULE_REMOVE:
++
++		NAT_rule_in = (SVIP_NAT_IO_Rule_t *) data;
++
++		/* check if destination UDP port is within range */
++		port = ntohs(NAT_rule_in->locUDP);
++		if (!SVIP_PORT_INRANGE(port)) {
++			pr_err("SVIP NAT: Error, UDP port(%d) is out of range(%d..%d)\n",
++				port, SVIP_UDP_FROM, SVIP_UDP_TO);
++			ret = -1;
++			goto error;
++		}
++		NAT_idx = SVIP_PORT_INDEX(port);
++		down(sem_nat_tbl_access);
++		/* remove rule from the NAT table */
++		memset(&nat_table[NAT_idx], 0,
++			sizeof(struct SVIP_NAT_table_entry));
++		up(sem_nat_tbl_access);
++		break;
++
++	case FIO_SVIP_NAT_RULE_LIST:
++	{
++		int i, len;
++		char buf[256];
++
++		down(sem_nat_tbl_access);
++		for (i = 0; i < CONFIG_LTQ_SVIP_NAT_RULES_TOTAL; i++) {
++			len = SVIP_NAT_read_NAT(buf, sizeof(buf), i);
++			if (len > 0)
++				pr_notice("%s", buf);
++		}
++		up(sem_nat_tbl_access);
++		break;
++	}
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	case FIO_SVIP_NAT_DESTIP_CHECK_ADD:
++	{
++		int i;
++
++		down(sem_nat_tbl_access);
++		if (((SVIP_NAT_DESTIP_t *) data)->typeIP ==
++			SVIP_IPV4_ADDR_TYPE) {
++			for (i = 0; i < nat_dest_IPv4_size; i++)
++				if (nat_dest_IPv4[i]
++					== ((SVIP_NAT_DESTIP_t *) data)->IP.v4)
++					break;
++			if (i < CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE) {
++
++				nat_dest_IPv4[i] =
++					((SVIP_NAT_DESTIP_t *) data)->IP.v4;
++				nat_dest_IPv4_size =
++					i >= nat_dest_IPv4_size ? i + 1 :
++						nat_dest_IPv4_size;
++			} else {
++				up(sem_nat_tbl_access);
++				pr_err("SVIP NAT: Error, valid IPv4 address list full\n");
++				ret = -1;
++				goto error;
++			}
++		} else {
++			for (i = 0; i < nat_dest_IPv6_size; i++)
++				if (!ipv6_addr_cmp(
++					(struct in6_addr *) &nat_dest_IPv6[i],
++					(struct in6_addr *)
++					&((SVIP_NAT_DESTIP_t *) data)->IP.v6))
++					break;
++			if (i < CONFIG_LTQ_SVIP_NAT_DESTIP_LIST_SIZE) {
++				nat_dest_IPv6[i] =
++					((SVIP_NAT_DESTIP_t *) data)->IP.v6;
++				nat_dest_IPv6_size =
++					i >= nat_dest_IPv6_size ? i + 1 :
++						nat_dest_IPv6_size;
++			} else {
++				up(sem_nat_tbl_access);
++				pr_err("SVIP NAT: Error, valid IPv6 address list full\n");
++				ret = -1;
++				goto error;
++			}
++		}
++		up(sem_nat_tbl_access);
++		break;
++	}
++
++	case FIO_SVIP_NAT_DESTIP_CHECK_REMOVE:
++	{
++		int i;
++
++		down(sem_nat_tbl_access);
++		if (((SVIP_NAT_DESTIP_t *) data)->typeIP ==
++			SVIP_IPV4_ADDR_TYPE) {
++			for (i = 0; i < nat_dest_IPv4_size; i++)
++				if (nat_dest_IPv4[i]
++					== ((SVIP_NAT_DESTIP_t *) data)->IP.v4)
++					break;
++			if (i < nat_dest_IPv4_size) {
++				/* element found, remove it */
++				nat_dest_IPv4[i] = 0;
++				/* move last element to position of removed
++				 * element, decrease size by 1
++				 */
++				nat_dest_IPv4_size--;
++				nat_dest_IPv4[i] =
++					nat_dest_IPv4[nat_dest_IPv4_size];
++			}
++		} else {
++			for (i = 0; i < nat_dest_IPv6_size; i++)
++				if (!ipv6_addr_cmp(
++					(struct in6_addr *) &nat_dest_IPv6[i],
++					(struct in6_addr *)
++					&((SVIP_NAT_DESTIP_t *) data)->IP.v6))
++					break;
++			if (i < nat_dest_IPv6_size) {
++				/* element found, remove it */
++				ipv6_addr_set(
++					(struct in6_addr *) &nat_dest_IPv6[i],
++					0, 0, 0, 0);
++				/* move last element to position of removed
++				 * element, decrease size by 1
++				 */
++				nat_dest_IPv6_size--;
++				nat_dest_IPv6[i] =
++					nat_dest_IPv6[nat_dest_IPv6_size];
++			}
++		}
++		up(sem_nat_tbl_access);
++		break;
++	}
++	case FIO_SVIP_NAT_DESTIP_LIST_GET:
++	{
++		int len, i;
++		char buf[90];
++
++		down(sem_nat_tbl_access);
++		for (i = 0; i < nat_dest_IPv4_size + nat_dest_IPv6_size; i++) {
++			len = SVIP_NAT_read_dest_IP(buf, sizeof(buf), i);
++			if (len > 0)
++				pr_notice("%s", buf);
++		}
++		up(sem_nat_tbl_access);
++		break;
++	}
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++	case FIO_SVIP_NAT_UDP_PORT_BASE_SET:
++	{
++		int i;
++
++		down(sem_nat_tbl_access);
++		for (i = 0; i < CONFIG_LTQ_SVIP_NAT_RULES_TOTAL; i++) {
++			if (nat_table[i].nat_rule.locUDP != 0)
++				break;
++		}
++		if (i < CONFIG_LTQ_SVIP_NAT_RULES_TOTAL) {
++			up(sem_nat_tbl_access);
++			pr_err("SVIP NAT: Error. NAT table not empty. Changing NAT UDP base port number not possible\n");
++			ret = -1;
++			goto error;
++		}
++		nat_udp_port_base = *(SVIP_UDP_PORT_t *) data;
++		up(sem_nat_tbl_access);
++		break;
++	}
++	case FIO_SVIP_NAT_UDP_PORT_BASE_GET:
++	{
++		*(SVIP_UDP_PORT_t *) data = nat_udp_port_base;
++		break;
++	}
++	default:
++		pr_err("SVIP NAT: unsupported ioctl (%x) command for device %s\n",
++			ioctl_num, PATH_SVIP_NAT_DEVICE_NAME);
++		ret = -1;
++		goto error;
++	}
++
++	if (size > 0) {
++		if (read) {
++			if (copy_to_user((void *)ioctl_param, (void *)data,
++				size) != 0) {
++				pr_err("SVIP NAT: ioctl %x: copy_to_user() failed!\n",
++					ioctl_num);
++				ret = -1;
++				goto error;
++			}
++		}
++	}
++
++error: kfree(data);
++
++	return ret;
++}
++
++#if 0
++void dump_msg(unsigned char *data, unsigned int len)
++{
++	int i;
++
++	for (i = 0; i < len; i++) {
++		if (!i || !(i%16))
++			pr_notice("\n	 ");
++		else if (i && !(i%4))
++			printk(" ");
++		pr_notice("%02x", data[i]);
++	}
++	if (--i%16)
++		pr_notice("\n");
++}
++#endif
++
++/******************************************************************************/
++/**
++ Used to recalculate IP/UDP checksum using the original IP/UDP checksum
++ coming with the packet. The original source and destination IP addresses
++ are accounted for, and, the checksum is updated using the new source and
++ destination IP addresses.
++
++ \arguments
++ skb          - pointer to the receiving socket buffer
++ csum_old     - original checksum
++ saddr_old    - pointer to original source IP address
++ saddr_new    - pointer to new source IP address
++ daddr_old    - pointer to original destination IP address
++ daddr_new    - pointer to new destination IP address
++
++ \return
++ recalculated IP/UDP checksum
++ \remark
++ for IPv4
++ */
++/******************************************************************************/
++static inline u16 ip_udp_quick_csum(u16 csum_old, u16 *saddr_old,
++	u16 *saddr_new, u16 *daddr_old, u16 *daddr_new)
++{
++	u32 sum;
++
++	sum = csum_old;
++
++	/* convert back from one's complement */
++	sum = ~sum & 0xffff;
++
++	if (sum < saddr_old[0])
++		sum += 0xffff;
++	sum -= saddr_old[0];
++	if (sum < saddr_old[1])
++		sum += 0xffff;
++	sum -= saddr_old[1];
++	if (sum < daddr_old[0])
++		sum += 0xffff;
++	sum -= daddr_old[0];
++	if (sum < daddr_old[1])
++		sum += 0xffff;
++	sum -= daddr_old[1];
++
++	sum += saddr_new[0];
++	sum += saddr_new[1];
++	sum += daddr_new[0];
++	sum += daddr_new[1];
++
++	/* take only 16 bits out of the 32 bit sum and add up the carries */
++	while (sum >> 16)
++		sum = (sum & 0xffff) + ((sum >> 16) & 0xffff);
++
++	/* one's complement the result */
++	sum = ~sum;
++
++	return (u16) (sum & 0xffff);
++}
++
++/******************************************************************************/
++/**
++ Used to recalculate IP/UDP checksum using the original IP/UDP checksum
++ coming with the packet. The original source and destination IP addresses
++ are accounted for, and, the checksum is updated using the new source and
++ destination IP addresses.
++
++ \arguments
++ skb          - pointer to the receiving socket buffer
++ csum_old     - original checksum
++ saddr_old    - pointer to original source IP address
++ saddr_new    - pointer to new source IP address
++ daddr_old    - pointer to original destination IP address
++ daddr_new    - pointer to new destination IP address
++
++ \return
++ recalculated IP/UDP checksum
++ \remark
++ for IPv6
++ */
++/******************************************************************************/
++static inline u16 ip6_udp_quick_csum(u16 csum_old, u16 *saddr_old,
++	u16 *saddr_new, u16 *daddr_old, u16 *daddr_new)
++{
++	u32 sum;
++
++	sum = csum_old;
++
++	/* convert back from one's complement */
++	sum = ~sum & 0xffff;
++
++	if (sum < saddr_old[0])
++		sum += 0xffff;
++	sum -= saddr_old[0];
++	if (sum < saddr_old[1])
++		sum += 0xffff;
++	sum -= saddr_old[1];
++	if (sum < saddr_old[2])
++		sum += 0xffff;
++	sum -= saddr_old[2];
++	if (sum < saddr_old[3])
++		sum += 0xffff;
++	sum -= saddr_old[3];
++	if (sum < saddr_old[4])
++		sum += 0xffff;
++	sum -= saddr_old[4];
++	if (sum < saddr_old[5])
++		sum += 0xffff;
++	sum -= saddr_old[5];
++	if (sum < saddr_old[6])
++		sum += 0xffff;
++	sum -= saddr_old[6];
++	if (sum < saddr_old[7])
++		sum += 0xffff;
++	sum -= saddr_old[7];
++
++	if (sum < daddr_old[0])
++		sum += 0xffff;
++	sum -= daddr_old[0];
++	if (sum < daddr_old[1])
++		sum += 0xffff;
++	sum -= daddr_old[1];
++	if (sum < daddr_old[2])
++		sum += 0xffff;
++	sum -= daddr_old[2];
++	if (sum < daddr_old[3])
++		sum += 0xffff;
++	sum -= daddr_old[3];
++	if (sum < daddr_old[4])
++		sum += 0xffff;
++	sum -= daddr_old[4];
++	if (sum < daddr_old[5])
++		sum += 0xffff;
++	sum -= daddr_old[5];
++	if (sum < daddr_old[6])
++		sum += 0xffff;
++	sum -= daddr_old[6];
++	if (sum < daddr_old[7])
++		sum += 0xffff;
++	sum -= daddr_old[7];
++
++	sum += saddr_new[0];
++	sum += saddr_new[1];
++	sum += saddr_new[2];
++	sum += saddr_new[3];
++	sum += saddr_new[4];
++	sum += saddr_new[5];
++	sum += saddr_new[6];
++	sum += saddr_new[7];
++	sum += daddr_new[0];
++	sum += daddr_new[1];
++	sum += daddr_new[2];
++	sum += daddr_new[3];
++	sum += daddr_new[4];
++	sum += daddr_new[5];
++	sum += daddr_new[6];
++	sum += daddr_new[7];
++
++	/* take only 16 bits out of the 32 bit sum and add up the carries */
++	while (sum >> 16)
++		sum = (sum & 0xffff) + ((sum >> 16) & 0xffff);
++
++	/* one's complement the result */
++	sum = ~sum;
++
++	return (u16) (sum & 0xffff);
++}
++
++/******************************************************************************/
++/**
++ Returns a pointer to an ipv4 address assigned to device dev.
++
++ \arguments
++ dev         - pointer to network interface
++ ppifa_addr  - output parameter
++
++ \return
++ none
++ */
++/******************************************************************************/
++static void get_ifaddr(struct net_device *dev, unsigned int **ppifa_addr)
++{
++	struct in_device *in_dev;
++	struct in_ifaddr *ifa = NULL;
++
++	in_dev = in_dev_get(dev);
++	if (in_dev) {
++		ifa = in_dev->ifa_list;
++		if (ifa)
++			*ppifa_addr = &ifa->ifa_address;
++		in_dev_put(in_dev);
++	}
++	*ppifa_addr = NULL;
++}
++
++
++#ifndef VLAN_8021Q_UNUSED
++/******************************************************************************/
++/**
++ Returns a pointer to an ipv6 address assigned to device dev.
++
++ \arguments
++ dev         - pointer to network interface
++ ppifa_addr  - output parameter
++
++ \return
++ none
++ */
++/******************************************************************************/
++static void get_inet6_ifaddr(struct net_device *dev,
++	struct in6_addr **ppifa_addr)
++{
++	struct inet6_dev *in_dev;
++	struct inet6_ifaddr *ifa = NULL;
++
++	in_dev = in6_dev_get(dev);
++	if (in_dev) {
++		read_lock_bh(&in_dev->lock);
++		list_for_each_entry(ifa, &in_dev->addr_list, if_list) {
++			if (!ipv6_addr_any(&ifa->addr)) {
++				*ppifa_addr = &ifa->addr;
++				break;
++			}
++		}
++		read_unlock_bh(&in_dev->lock);
++		in6_dev_put(in_dev);
++		return;
++	}
++	*ppifa_addr = NULL;
++}
++#endif
++
++/******************************************************************************/
++/**
++ This function performs IP NAT for received packets satisfying the
++ following requirements:
++
++ - packet is destined to local IP host
++ - transport protocol type is UDP
++ - destination UDP port is within range
++ - IPv4 NAT rule is defined for this UDP port
++
++ \arguments
++ skb - pointer to the receiving socket buffer
++
++ \return
++ returns 1 on performed SVIP NAT, else returns 0
++
++ \remarks
++ When function returns 0, it indicates the caller to pass the
++ packet up the IP stack to make further decision about it
++ */
++/******************************************************************************/
++int do_SVIP_NAT(struct sk_buff *skb)
++{
++	struct net_device *real_dev;
++	struct iphdr *iph;
++	struct udphdr *udph;
++	SVIP_NAT_IO_Rule_t *nat_rule;
++	int NAT_idx, in_eth0, dir;
++#ifndef VLAN_8021Q_UNUSED
++	int vlan;
++	unsigned short vid;
++	__be16 vlan_proto = 0;
++#endif /* ! VLAN_8021Q_UNUSED */
++	SVIP_UDP_PORT_t port;
++	u32 org_src_IP, org_ds_IP, *src_IP, *dst_IP;
++	struct ethhdr *ethh;
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	int i = 0;
++#endif
++
++	/* do not consider if SVIP NAT device not open. */
++	if (!device_open)
++		return 0;
++
++	/* consider only UDP packets. */
++	iph = ip_hdr(skb);
++	if (iph->protocol != IPPROTO_UDP)
++		return 0;
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	for (i = 0; i < nat_dest_IPv4_size; i++)
++		if (iph->daddr == nat_dest_IPv4[i])
++			break;
++	if (i >= nat_dest_IPv4_size)
++		return 0;
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++
++	udph = (struct udphdr *) ((u_int32_t *) iph + iph->ihl);
++	/* consider only packets which UDP port numbers reside within
++	 * the predefined SVIP NAT UDP port range.
++	 */
++	if ((!SVIP_PORT_INRANGE(ntohs(udph->dest)))
++		&& (!SVIP_PORT_INRANGE(ntohs(udph->source))))
++		return 0;
++
++#ifndef VLAN_8021Q_UNUSED
++	/*
++	 * check if packet delivered over VLAN. VLAN packets will be routed over
++	 * the VLAN interfaces of the respective real Ethernet interface, if one
++	 * exists(VIDs must match). Else, the packet will be send out as
++	 * IEEE 802.3 Ethernet frame
++	 */
++	if (skb->dev->priv_flags & IFF_802_1Q_VLAN) {
++		vlan = 1;
++		vid = vlan_dev_vlan_id(skb->dev);
++		real_dev = vlan_dev_real_dev(skb->dev);
++		vlan_proto = skb->vlan_proto;
++	} else {
++		vlan = 0;
++		vid = 0;
++		real_dev = skb->dev;
++	}
++#endif /* ! VLAN_8021Q_UNUSED */
++
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++	/*
++	 * Debugging feature which can be enabled by writing,
++	 * 'echo 1 > /proc/net/svip_nat/snifferOnOff'.
++	 * It copies all packets received on veth0 and eth1 and, sends them out
++	 * over eth0. When a destination MAC address is specified through
++	 * /proc/net/svip_nat/snifferMAC, this MAC addess will substitute the
++	 * original MAC address of the packet.
++	 * It is recommended to specify a MAC address of some host where
++	 * Wireshark runs and sniffs for this traffic, else you may flood your
++	 * LAN with undeliverable traffic.
++	 * NOTE: In case of VLAN traffic the VLAN header information is lost.
++	 */
++	if (SVIP_NAT_sniffer) {
++		if (real_dev == net_devs[SVIP_NET_DEV_VETH0_IDX] ||
++			real_dev == net_devs[SVIP_NET_DEV_ETH1_IDX]) {
++			struct sk_buff *copied_skb;
++
++			/* gain the Ethernet header from the skb */
++			skb_push(skb, ETH_HLEN);
++
++			copied_skb = skb_copy(skb, GFP_ATOMIC);
++
++			if (SVIP_NAT_sniffer_MAC_set == 1) {
++				ethh = (struct ethhdr *)
++				skb_mac_header(copied_skb);
++				memcpy((char *)ethh->h_dest,
++					(char *)SVIP_NAT_sniffer_MAC,
++					ETH_ALEN);
++			}
++			copied_skb->dev = net_devs[SVIP_NET_DEV_ETH0_IDX];
++			dev_queue_xmit(copied_skb);
++
++			/* skip the ETH header again */
++			skb_pull(skb, ETH_HLEN);
++		}
++	}
++#endif
++
++	/* check if packet arrived on eth0 */
++	if (real_dev == net_devs[SVIP_NET_DEV_ETH0_IDX]) {
++		/*
++		 * Check if destination IP address equals the IP address
++		 * of interface eth0. This is the case of packets originating
++		 * from a remote peer that are to be
++		 * delivered to a channel residing on THIS voice linecard
++		 * system. This is typical SVIP NAT case, therefore this rule is
++		 * placed on top.
++		 */
++		if (iph->daddr == *paddr_eth0)
++		{
++			port = ntohs(udph->dest);
++			dir = SVIP_NAT_STATS_REM2LOC;
++		} else
++#ifndef VLAN_8021Q_UNUSED
++		/*
++		 * check if the packet has addressed any of the IP
++		 * addresses assigned to the VLAN interface attached to
++		 * eth0. This is not recommended approach because of
++		 * the CPU cost incurred. */
++		if (vlan)
++		{
++			struct in_device *in_dev;
++			struct in_ifaddr *ifa = NULL;
++			bool bAddrHit = false;
++			int i = 0;
++
++			if ((in_dev=in_dev_get(skb->dev)) == NULL)
++				return 0;
++
++			for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next)
++			{
++				if (iph->daddr == ifa->ifa_address)
++				{
++					bAddrHit = true;
++					break;
++				}
++				i++;
++			}
++
++			in_dev_put(in_dev);
++
++			if (bAddrHit) {
++				port = ntohs(udph->dest);
++				dir = SVIP_NAT_STATS_REM2LOC;
++			} else
++				return 0;
++		} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			return 0;
++		in_eth0 = 1;
++	} else if (real_dev == net_devs[SVIP_NET_DEV_ETH1_IDX]) {
++		/* check if packet arrived on eth1 */
++		/*
++		 * Packets originating from slave and destined to master or the
++		 * same slave
++		 */
++		if (iph->daddr == *paddr_eth1)
++		{
++			port = ntohs(udph->dest);
++			dir = SVIP_NAT_STATS_REM2LOC;
++		} else
++#ifndef VLAN_8021Q_UNUSED
++		/*
++		 * check if the packet has addressed any of the IP
++		 * addresses assigned to the VLAN interface attached to
++		 * eth0. This is not recommended approach because of
++		 * the CPU cost incurred. */
++		if (vlan)
++		{
++			struct in_device *in_dev;
++			struct in_ifaddr *ifa = NULL;
++			bool bAddrHit = false;
++			int i = 0;
++
++			if ((in_dev=in_dev_get(skb->dev)) == NULL)
++				return 0;
++
++			for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next)
++			{
++				if (iph->daddr == ifa->ifa_address)
++				{
++					bAddrHit = true;
++					break;
++				}
++				i++;
++			}
++
++			in_dev_put(in_dev);
++
++			if (bAddrHit) {
++				port = ntohs(udph->dest);
++				dir = SVIP_NAT_STATS_REM2LOC;
++			} else
++				return 0;
++		} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			return 0;
++		in_eth0 = 0;
++	} else if (real_dev == net_devs[SVIP_NET_DEV_VETH0_IDX]) {
++		/* check if packet arrived on veth0 */
++		port = ntohs(udph->source);
++		dir = SVIP_NAT_STATS_LOC2REM;
++	} else {
++		/* packet arrived neither on eth0, nor eth1, nor veth0 */
++		return 0;
++	}
++
++	/* calculate the respective index of the NAT table */
++	NAT_idx = SVIP_PORT_INDEX(port);
++	/* process the packet if a respective NAT rule exists */
++	nat_rule = &nat_table[NAT_idx].nat_rule;
++
++	if (nat_rule->typeIP != SVIP_IPV4_ADDR_TYPE) {
++		/* Rule check failed. IPv4 packet arrived, but IPv6 rule in
++		 * table.
++		 */
++		return 0;
++	}
++
++	ethh = (struct ethhdr *) skb_mac_header(skb);
++
++	/* copy packet's original source and destination IP addresses to use
++	 * later on to perform efficient checksum recalculation
++	 */
++	org_src_IP = iph->saddr;
++	org_ds_IP = iph->daddr;
++
++	if (dir == SVIP_NAT_STATS_REM2LOC) {
++		/* Process packet arrived on eth0 or eth1 */
++		u8 *dst_MAC;
++
++		if (iph->saddr != nat_rule->remIP.v4) {
++			/* Rule check failed. The packet is passed up the
++			 * layers, it will be dropped by UDP
++			 */
++			return 0;
++		}
++		dst_IP = &nat_rule->locIP.v4;
++		dst_MAC = nat_rule->locMAC;
++
++		/* determine weather the destination in on veth0 or on eth1 */
++		if ((*dst_IP & *pmask_veth0) == (*paddr_veth0 & *pmask_veth0)) {
++			/* veth0 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_VETH0_IDX],
++					vlan_proto, vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++				} else {
++					skb->dev = net_devs[
++					SVIP_NET_DEV_VETH0_IDX];
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_VETH0_IDX];
++				skb_push(skb, ETH_HLEN);
++			}
++			src_IP = paddr_veth0;
++		} else {
++			/* eth1 */
++			if (!in_eth0)
++			{
++				/* came in on eth1 */
++				skb_push(skb, ETH_HLEN);
++			} else {
++				/* came in on eth0 */
++#ifndef VLAN_8021Q_UNUSED
++				if (vlan) {
++					struct net_device *vlan_dev;
++
++					rcu_read_lock();
++					vlan_dev = vlan_find_dev(
++						net_devs[SVIP_NET_DEV_ETH1_IDX],
++						vlan_proto, vid);
++					rcu_read_unlock();
++					if (vlan_dev) {
++						skb_push(skb, ETH_HLEN);
++						skb->dev = vlan_dev;
++					} else {
++						skb->dev = net_devs[
++						SVIP_NET_DEV_ETH1_IDX];
++						skb_push(skb, ETH_HLEN);
++					}
++				} else
++#endif /* ! VLAN_8021Q_UNUSED */
++				{
++					skb->dev = net_devs[SVIP_NET_DEV_ETH1_IDX];
++					skb_push(skb, ETH_HLEN);
++				}
++			}
++			src_IP = paddr_eth1;
++		}
++		iph->saddr = *src_IP;
++		memcpy((char *)ethh->h_source, (char *)skb->dev->dev_addr,
++			ETH_ALEN);
++		iph->daddr = *dst_IP;
++		memcpy((char *)ethh->h_dest, (char *)dst_MAC, ETH_ALEN);
++	} else {
++		/* Process packet arrived on veth0 */
++		if (iph->saddr != nat_rule->locIP.v4) {
++			/* Rule check failed. The packet is passed up
++			 * the layers, it will be dropped by UDP
++			 */
++			return 0;
++		}
++
++		/* determine weather the destination in on veth0 or eth0
++		 * or eth1 */
++		if ((nat_rule->remIP.v4 & *pmask_veth0)
++			== (*paddr_veth0 & *pmask_veth0)) {
++			/* veth0 */
++			src_IP = paddr_veth0;
++			skb_push(skb, ETH_HLEN);
++		} else if ((nat_rule->remIP.v4 & *pmask_eth1)
++			== (*paddr_eth1 & *pmask_eth1)) {
++			/* eth1 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_ETH1_IDX],
++					vlan_proto, vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					unsigned int *pifa_addr;
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++
++					get_ifaddr(skb->dev, &pifa_addr);
++					if (pifa_addr)
++						src_IP = pifa_addr;
++					else
++						src_IP = paddr_eth1;
++				} else {
++					skb->dev =
++						net_devs[SVIP_NET_DEV_ETH1_IDX];
++					src_IP = paddr_eth1;
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_ETH1_IDX];
++				src_IP = paddr_eth1;
++				skb_push(skb, ETH_HLEN);
++			}
++		} else {
++			/* eth0 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_ETH0_IDX],
++					vlan_proto, vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					unsigned int *pifa_addr;
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++
++					get_ifaddr(skb->dev, &pifa_addr);
++					if (pifa_addr)
++						src_IP = pifa_addr;
++					else
++						src_IP = paddr_eth0;
++				} else {
++					skb->dev =
++						net_devs[SVIP_NET_DEV_ETH0_IDX];
++					src_IP = paddr_eth0;
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_ETH0_IDX];
++				src_IP = paddr_eth0;
++				skb_push(skb, ETH_HLEN);
++			}
++		}
++		iph->saddr = *src_IP;
++		memcpy((char *)ethh->h_source, (char *)skb->dev->dev_addr,
++			ETH_ALEN);
++		iph->daddr = nat_rule->remIP.v4;
++		memcpy((char *)ethh->h_dest, (char *)nat_rule->remMAC,
++			ETH_ALEN);
++	}
++	nat_table[NAT_idx].nat_stats[dir].in_packets++;
++
++	iph->check = ip_udp_quick_csum(iph->check, (u16 *) &org_src_IP,
++		(u16 *) &iph->saddr, (u16 *) &org_ds_IP, (u16 *) &iph->daddr);
++	if (udph->check != 0) {
++		udph->check = ip_udp_quick_csum(udph->check,
++			(u16 *) &org_src_IP,
++			(u16 *) &iph->saddr, (u16 *) &org_ds_IP,
++			(u16 *) &iph->daddr);
++	}
++
++	/* write the packet out, directly to the network device */
++	if (dev_queue_xmit(skb) < 0)
++		nat_table[NAT_idx].nat_stats[dir].out_errors++;
++	else
++		nat_table[NAT_idx].nat_stats[dir].out_packets++;
++
++	return 1;
++}
++
++/******************************************************************************/
++/**
++ This function performs IPv6 NAT for received packets satisfying the
++ following requirements:
++
++ - packet is destined to local IP host
++ - transport protocol type is UDP
++ - destination UDP port is within range
++ - IPv6 NAT rule is defined for this UDP port
++
++ \arguments
++ skb - pointer to the receiving socket buffer
++
++ \return
++ returns 1 on performed SVIP NAT, else returns 0
++
++ \remarks
++ When function returns 0, it indicates the caller to pass the
++ packet up the IP stack to make further decision about it
++ */
++/******************************************************************************/
++int ipv6_do_SVIP_NAT(struct sk_buff *skb)
++{
++	struct net_device *real_dev;
++	struct ipv6hdr *iph;
++	struct udphdr *udph;
++	SVIP_NAT_IO_Rule_t *nat_rule;
++	int NAT_idx, in_eth0, dir;
++#ifndef VLAN_8021Q_UNUSED
++	int vlan;
++	unsigned short vid;
++	__be16 vlan_proto = 0;
++#endif /* ! VLAN_8021Q_UNUSED */
++	SVIP_UDP_PORT_t port;
++	struct in6_addr org_src_IP, org_ds_IP, *src_IP, *dst_IP;
++	struct ethhdr *ethh;
++	unsigned int ptr = 0;
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	int i = 0;
++#endif
++
++	/* do not consider if SVIP NAT device not open. */
++	if (!device_open)
++		return 0;
++
++	iph = ipv6_hdr(skb);
++	/* consider only UDP packets. */
++	if (ipv6_find_hdr(skb, &ptr, NEXTHDR_UDP, NULL, NULL) < 0)
++		return 0;
++
++#ifdef CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK
++	for (i = 0; i < nat_dest_IPv6_size; i++)
++		if (ipv6_addr_equal((struct in6_addr *) &iph->daddr,
++			(struct in6_addr *) &nat_dest_IPv6[i]))
++			break;
++	if (i >= nat_dest_IPv6_size)
++		return 0;
++#endif /* CONFIG_LTQ_SVIP_NAT_DESTIP_CHECK */
++
++	udph = (struct udphdr *) (skb->data + ptr);
++	/* consider only packets which UDP port numbers reside within
++	 * the predefined SVIP NAT UDP port range.
++	 */
++	if ((!SVIP_PORT_INRANGE(ntohs(udph->dest)))
++		&& (!SVIP_PORT_INRANGE(ntohs(udph->source))))
++		return 0;
++
++#ifndef VLAN_8021Q_UNUSED
++	/*
++	 * check if packet delivered over VLAN. VLAN packets will be routed over
++	 * the VLAN interfaces of the respective real Ethernet interface, if one
++	 * exists(VIDs must match). Else, the packet will be send out as IEEE
++	 * 802.3 Ethernet frame
++	 */
++	if (skb->dev->priv_flags & IFF_802_1Q_VLAN) {
++		vlan = 1;
++		vid = vlan_dev_vlan_id(skb->dev);
++		real_dev = vlan_dev_real_dev(skb->dev);
++		vlan_proto = skb->vlan_proto;
++	} else {
++		vlan = 0;
++		vid = 0;
++		real_dev = skb->dev;
++	}
++#endif /* ! VLAN_8021Q_UNUSED */
++
++#ifdef CONFIG_SVIP_FW_PKT_SNIFFER
++	/*
++	 * Debugging feature which can be enabled by writing,
++	 * 'echo 1 > /proc/net/svip_nat/snifferOnOff'.
++	 * It copies all packets received on veth0 and eth1 and, sends them out
++	 * over eth0. When a destination MAC address is specified through
++	 * /proc/net/svip_nat/snifferMAC, this MAC addess will substitute the
++	 * original MAC address of the packet.
++	 * It is recommended to specify a MAC address of some host where
++	 * Wireshark runs and sniffs for this traffic, else you may flood your
++	 * LAN with undeliverable traffic.
++	 * NOTE: In case of VLAN traffic the VLAN header information is lost.
++	 */
++	if (SVIP_NAT_sniffer) {
++		if (real_dev == net_devs[SVIP_NET_DEV_VETH0_IDX] ||
++			real_dev == net_devs[SVIP_NET_DEV_ETH1_IDX]) {
++			struct sk_buff *copied_skb;
++
++			/* gain the Ethernet header from the skb */
++			skb_push(skb, ETH_HLEN);
++
++			copied_skb = skb_copy(skb, GFP_ATOMIC);
++
++			if (SVIP_NAT_sniffer_MAC_set == 1) {
++				ethh = (struct ethhdr *)
++				skb_mac_header(copied_skb);
++				memcpy((char *)ethh->h_dest,
++					(char *)SVIP_NAT_sniffer_MAC,
++					ETH_ALEN);
++			}
++			copied_skb->dev = net_devs[SVIP_NET_DEV_ETH0_IDX];
++			dev_queue_xmit(copied_skb);
++
++			/* skip the ETH header again */
++			skb_pull(skb, ETH_HLEN);
++		}
++	}
++#endif
++
++	/* check if packet arrived on eth0 */
++	if (real_dev == net_devs[SVIP_NET_DEV_ETH0_IDX]) {
++		/*
++		 * Check if destination IP address equals IP address
++		 * of interface eth0. This is the case of packets originating
++		 * from a remote peer that are to be delivered to a channel
++		 * residing on THIS voice line card system. This is typical SVIP
++		 * NAT case, therefore this rule is placed on top.
++		 */
++		if (!ipv6_addr_cmp(&iph->daddr, paddr_eth0_ip6)) {
++			port = ntohs(udph->dest);
++			dir = SVIP_NAT_STATS_REM2LOC;
++		} else
++			return 0;
++		in_eth0 = 1;
++	/* check if packet arrived on eth1 */
++	} else if (real_dev == net_devs[SVIP_NET_DEV_ETH1_IDX]) {
++		if (!ipv6_addr_cmp(&iph->daddr, paddr_eth1_ip6)) {
++			port = ntohs(udph->dest);
++			dir = SVIP_NAT_STATS_REM2LOC;
++		} else
++			return 0;
++		in_eth0 = 0;
++	/* check if packet arrived on veth0 */
++	} else if (real_dev == net_devs[SVIP_NET_DEV_VETH0_IDX]) {
++		port = ntohs(udph->source);
++		dir = SVIP_NAT_STATS_LOC2REM;
++	} else
++		/* packet arrived neither on eth0, nor on eth1, nor on veth0 */
++		return 0;
++
++	/* calculate the respective index of the NAT table */
++	NAT_idx = SVIP_PORT_INDEX(port);
++	/* process the packet if a respective NAT rule exists */
++	nat_rule = &nat_table[NAT_idx].nat_rule;
++
++	if (nat_rule->typeIP != SVIP_IPV6_ADDR_TYPE)
++		/* Rule check failed. IPv6 packet arrived, but IPv4 rule in
++		 * table.
++		 */
++		return 0;
++
++	ethh = (struct ethhdr *) skb_mac_header(skb);
++
++	/* copy packet's original source and destination IP addresses to use
++	 * later on to perform efficient checksum recalculation
++	 */
++	org_src_IP = iph->saddr;
++	org_ds_IP = iph->daddr;
++
++	if (dir == SVIP_NAT_STATS_REM2LOC) {
++		/* Process packet arrived on eth0 or eth1 */
++		u8 *dst_MAC;
++
++		if (!ipv6_addr_equal((struct in6_addr *) &iph->saddr,
++			(struct in6_addr *) &nat_rule->remIP.v6)) {
++			/* Rule check failed. The packet is passed up the
++			 * layers, it will be dropped by UDP
++			 */
++			return 0;
++		}
++
++		dst_IP = (struct in6_addr *) &nat_rule->locIP.v6;
++		dst_MAC = nat_rule->locMAC;
++
++		/* determine weather the destination in on veth0 or on eth1 */
++		if (paddr_veth0_ip6 != NULL
++			&& ipv6_prefix_equal(dst_IP, paddr_veth0_ip6,
++			prefix_len_veth0)) {
++			/* veth0 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_VETH0_IDX],
++					vlan_proto,
++					vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++				} else {
++					skb->dev = net_devs[
++					SVIP_NET_DEV_VETH0_IDX];
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_VETH0_IDX];
++				skb_push(skb, ETH_HLEN);
++			}
++			src_IP = paddr_veth0_ip6;
++		} else {
++			/* eth1 */
++			if (!in_eth0)
++			{
++				/* came in on eth1 */
++				skb_push(skb, ETH_HLEN);
++			} else {
++				/* came in on eth0 */
++#ifndef VLAN_8021Q_UNUSED
++				if (vlan) {
++					struct net_device *vlan_dev;
++
++					rcu_read_lock();
++					vlan_dev = vlan_find_dev(
++						net_devs[SVIP_NET_DEV_ETH1_IDX],
++						vlan_proto, vid);
++					rcu_read_unlock();
++					if (vlan_dev) {
++						skb_push(skb, ETH_HLEN);
++						skb->dev = vlan_dev;
++					} else {
++						skb->dev = net_devs[
++						SVIP_NET_DEV_ETH1_IDX];
++						skb_push(skb, ETH_HLEN);
++					}
++				} else
++#endif /* ! VLAN_8021Q_UNUSED */
++				{
++					skb->dev = net_devs[SVIP_NET_DEV_ETH1_IDX];
++					skb_push(skb, ETH_HLEN);
++				}
++			}
++			src_IP = paddr_eth1_ip6;
++		}
++		iph->saddr = *src_IP;
++		memcpy((char *)ethh->h_source, (char *)skb->dev->dev_addr,
++			ETH_ALEN);
++		iph->daddr = *dst_IP;
++		memcpy((char *)ethh->h_dest, (char *)dst_MAC, ETH_ALEN);
++	} else {
++		/* Process packet arrived on veth0 */
++		if (ipv6_addr_cmp((struct in6_addr *) &iph->saddr,
++			(struct in6_addr *) &nat_rule->locIP.v6)) {
++			/* Rule check failed. The packet is passed up the
++			 * layers, it will be dropped by UDP
++			 */
++			return 0;
++		}
++
++		/* determine weather the destination in on veth0 or eth0
++		 * or eth1 */
++		if (ipv6_prefix_equal((struct in6_addr *) &nat_rule->remIP.v6,
++			paddr_veth0_ip6, prefix_len_veth0)) {
++			/* veth0 */
++			src_IP = paddr_veth0_ip6;
++			skb_push(skb, ETH_HLEN);
++		} else if (ipv6_prefix_equal((struct in6_addr *) &nat_rule->remIP.v6,
++			paddr_eth1_ip6, prefix_len_eth1)) {
++			/* eth1 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_ETH1_IDX],
++					vlan_proto, vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					struct in6_addr *pifa_addr;
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++
++					get_inet6_ifaddr(skb->dev, &pifa_addr);
++					if (pifa_addr)
++						src_IP = pifa_addr;
++					else
++						src_IP = paddr_eth1_ip6;
++				} else {
++					skb->dev = net_devs[
++					SVIP_NET_DEV_ETH1_IDX];
++					src_IP = paddr_eth1_ip6;
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_ETH1_IDX];
++				src_IP = paddr_eth1_ip6;
++				skb_push(skb, ETH_HLEN);
++			}
++		} else {
++			/* eth0 */
++#ifndef VLAN_8021Q_UNUSED
++			if (vlan) {
++				struct net_device *vlan_dev;
++
++				rcu_read_lock();
++				vlan_dev = vlan_find_dev(
++					net_devs[SVIP_NET_DEV_ETH0_IDX],
++					vlan_proto, vid);
++				rcu_read_unlock();
++				if (vlan_dev) {
++					struct in6_addr *pifa_addr;
++					skb_push(skb, ETH_HLEN);
++					skb->dev = vlan_dev;
++
++					get_inet6_ifaddr(skb->dev, &pifa_addr);
++					if (pifa_addr)
++						src_IP = pifa_addr;
++					else
++						src_IP = paddr_eth0_ip6;
++				} else {
++					skb->dev = net_devs[
++					SVIP_NET_DEV_ETH0_IDX];
++					src_IP = paddr_eth0_ip6;
++					skb_push(skb, ETH_HLEN);
++				}
++			} else
++#endif /* ! VLAN_8021Q_UNUSED */
++			{
++				skb->dev = net_devs[SVIP_NET_DEV_ETH0_IDX];
++				src_IP = paddr_eth0_ip6;
++				skb_push(skb, ETH_HLEN);
++			}
++		}
++		iph->saddr = *src_IP;
++		memcpy((char *)ethh->h_source, (char *)skb->dev->dev_addr,
++			ETH_ALEN);
++		memcpy(&iph->daddr, &nat_rule->remIP.v6,
++			sizeof(struct in6_addr));
++		memcpy((char *)ethh->h_dest, (char *)nat_rule->remMAC,
++			ETH_ALEN);
++	}
++	nat_table[NAT_idx].nat_stats[dir].in_packets++;
++
++	if (udph->check != 0) {
++		udph->check = ip6_udp_quick_csum(udph->check,
++			(u16 *) &org_src_IP,
++			(u16 *) &iph->saddr, (u16 *) &org_ds_IP,
++			(u16 *) &iph->daddr);
++	}
++
++	/* write the packet out, directly to the network device */
++	if (dev_queue_xmit(skb) < 0)
++		nat_table[NAT_idx].nat_stats[dir].out_errors++;
++	else
++		nat_table[NAT_idx].nat_stats[dir].out_packets++;
++
++	return 1;
++}
++
++/******************************************************************************/
++/**
++ Function executed upon unloading of the SVIP NAT module. It unregisters the
++ SVIP NAT configuration device and frees the memory used for the NAT table.
++
++ \remarks:
++ Currently the SVIP NAT module is statically linked into the Linux kernel
++ therefore this routine cannot be executed.
++ ******************************************************************************/
++static int __init init(void)
++{
++	int ret = 0;
++	struct net_device *dev;
++
++	if (misc_register(&SVIP_NAT_miscdev) != 0) {
++		pr_err("%s: cannot register SVIP NAT device node.\n",
++			SVIP_NAT_miscdev.name);
++		return -EIO;
++	}
++
++	/* allocation of memory for NAT table */
++	nat_table = kmalloc(sizeof(struct SVIP_NAT_table_entry) *
++		CONFIG_LTQ_SVIP_NAT_RULES_TOTAL,
++		GFP_ATOMIC);
++	if (nat_table == NULL) {
++		pr_err("SVIP NAT: Error(%d), allocating memory for NAT table\n",
++			ret);
++		return -1;
++	}
++
++	/* clear the NAT table */
++	memset((void *)nat_table, 0,
++		sizeof(struct SVIP_NAT_table_entry) *
++		CONFIG_LTQ_SVIP_NAT_RULES_TOTAL);
++
++	sem_nat_tbl_access = kmalloc(sizeof(struct semaphore), GFP_KERNEL);
++	if (sem_nat_tbl_access)
++		sema_init(sem_nat_tbl_access, 1);
++
++	SVIP_NAT_proc_install();
++
++	/* find pointers to 'struct net_device' of eth0 and veth0,
++	 * respectevely
++	 */
++	read_lock(&dev_base_lock);
++	for_each_netdev(&init_net, dev) {
++		if (!strcmp(dev->name, SVIP_NET_DEV_ETH0_NAME))
++			net_devs[SVIP_NET_DEV_ETH0_IDX] = dev;
++		if (!strcmp(dev->name, SVIP_NET_DEV_ETH1_NAME))
++			net_devs[SVIP_NET_DEV_ETH1_IDX] = dev;
++		else if (!strcmp(dev->name, SVIP_NET_DEV_VETH0_NAME))
++			net_devs[SVIP_NET_DEV_VETH0_IDX] = dev;
++	}
++	read_unlock(&dev_base_lock);
++
++	if (net_devs[SVIP_NET_DEV_ETH0_IDX] == NULL
++		|| net_devs[SVIP_NET_DEV_ETH1_IDX] == NULL
++		|| net_devs[SVIP_NET_DEV_VETH0_IDX] == NULL) {
++		pr_err("SVIP NAT: Error, unable to locate eth0 or eth1 or veth0 interfaces\n");
++		return -1;
++	}
++
++	pr_info("%s, (c) 2014, Lantiq Deutschland GmbH\n",
++		&SVIP_NAT_INFO_STR[4]);
++
++	return ret;
++}
++
++/******************************************************************************/
++/**
++ Function executed upon unloading of the SVIP NAT module. It unregisters the
++ SVIP NAT configuration device and frees the memory used for the NAT table.
++
++ \remarks:
++ Currently the SVIP NAT module is statically linked into the Linux kernel
++ therefore this routine cannot be executed.
++ ******************************************************************************/
++static void __exit fini(void)
++{
++	/* unregister SVIP NAT configuration device */
++	misc_deregister(&SVIP_NAT_miscdev);
++
++	/* release memory of SVIP NAT table */
++	if (nat_table != NULL)
++		kfree(nat_table);
++}
++
++module_init(init);
++module_exit(fini);
+diff --git a/net/ipv6/ip6_input.c b/net/ipv6/ip6_input.c
+--- a/net/ipv6/ip6_input.c
++++ b/net/ipv6/ip6_input.c
+@@ -29,6 +29,9 @@
+ #include <linux/icmpv6.h>
+ #include <linux/mroute6.h>
+ #include <linux/slab.h>
++#ifdef CONFIG_LTQ_SVIP_NAT
++#include <linux/svip_nat.h>
++#endif
+ 
+ #include <linux/netfilter.h>
+ #include <linux/netfilter_ipv6.h>
+@@ -179,6 +182,13 @@ int ipv6_rcv(struct sk_buff *skb, struct
+ 	/* Must drop socket now because of tproxy. */
+ 	skb_orphan(skb);
+ 
++#ifdef CONFIG_LTQ_SVIP_NAT
++	if (ipv6_do_SVIP_NAT(skb)) {
++		/* SVIP NAT performed, receiving successful */
++		return NET_RX_SUCCESS;
++	}
++#endif
++
+ 	return NF_HOOK(NFPROTO_IPV6, NF_INET_PRE_ROUTING, skb, dev, NULL,
+ 		       ip6_rcv_finish);
+ err:
diff --git a/target/linux/lantiq/svip_be/config-default b/target/linux/lantiq/svip_be/config-default
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/svip_be/config-default
@@ -0,0 +1,39 @@
+CONFIG_ADM6996_PHY=y
+CONFIG_AR8216_PHY=y
+# CONFIG_CPU_LITTLE_ENDIAN is not set
+CONFIG_IMAGE_CMDLINE_HACK=y
+CONFIG_INPUT=y
+CONFIG_INPUT_EVDEV=y
+CONFIG_INPUT_POLLDEV=y
+CONFIG_IPV6=y
+CONFIG_IP_PNP=y
+# CONFIG_IP_PNP_BOOTP is not set
+# CONFIG_IP_PNP_DHCP is not set
+# CONFIG_IP_PNP_RARP is not set
+# CONFIG_ISDN is not set
+CONFIG_LANTIQ_SVIP_ETH_DRV=y
+# CONFIG_LANTIQ_SVIP_VIRTUAL_ETH is not set
+# CONFIG_LTQ_SVIP_NAT is not set
+CONFIG_LOCKD=y
+CONFIG_MTD_NAND=y
+CONFIG_MTD_NAND_ECC=y
+CONFIG_MTD_NAND_PLATFORM=y
+CONFIG_MTD_NAND_SVIP=y
+# CONFIG_MTD_ROOTFS_SPLIT is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_SPLIT_FIRMWARE_NAME="linux"
+CONFIG_NFS_FS=y
+CONFIG_NFS_V2=y
+CONFIG_PINCTRL_SVIP=y
+CONFIG_ROOT_NFS=y
+CONFIG_RTL8306_PHY=y
+CONFIG_SOC_SVIP=y
+# CONFIG_SOC_TYPE_XWAY is not set
+# CONFIG_SOC_XWAY is not set
+# CONFIG_SQUASHFS is not set
+CONFIG_SUNRPC=y
+# CONFIG_USB_ARCH_HAS_EHCI is not set
+# CONFIG_USB_ARCH_HAS_HCD is not set
+# CONFIG_USB_ARCH_HAS_OHCI is not set
+# CONFIG_USB_ARCH_HAS_XHCI is not set
+CONFIG_USB_SUPPORT=y
diff --git a/target/linux/lantiq/svip_be/target.mk b/target/linux/lantiq/svip_be/target.mk
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/svip_be/target.mk
@@ -0,0 +1,11 @@
+ARCH:=mips
+SUBTARGET:=svip_be
+BOARDNAME:=SVIP Big Endian
+FEATURES:=squashfs jffs2
+DEVICE_TYPE:=other
+
+DEFAULT_PACKAGES+= uboot-svip hostapd-mini
+
+define Target/Description
+	Lantiq SVIP Big Endian
+endef
diff --git a/target/linux/lantiq/svip_le/config-default b/target/linux/lantiq/svip_le/config-default
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/svip_le/config-default
@@ -0,0 +1,41 @@
+CONFIG_ADM6996_PHY=y
+CONFIG_AR8216_PHY=y
+# CONFIG_CPU_BIG_ENDIAN is not set
+CONFIG_CPU_LITTLE_ENDIAN=y
+CONFIG_IMAGE_CMDLINE_HACK=y
+CONFIG_INPUT=y
+CONFIG_INPUT_EVDEV=y
+CONFIG_INPUT_POLLDEV=y
+CONFIG_IP_PNP=y
+# CONFIG_IP_PNP_BOOTP is not set
+# CONFIG_IP_PNP_DHCP is not set
+# CONFIG_IP_PNP_RARP is not set
+# CONFIG_ISDN is not set
+# CONFIG_PCIE_LANTIQ is not set
+# CONFIG_PCI_LANTIQ is not set
+# CONFIG_PCI_LANTIQ_NONE is not set
+CONFIG_LANTIQ_SVIP_ETH_DRV=y
+CONFIG_LOCKD=y
+CONFIG_MTD_NAND=y
+CONFIG_MTD_NAND_ECC=y
+CONFIG_MTD_NAND_PLATFORM=y
+CONFIG_MTD_NAND_SVIP=y
+# CONFIG_MTD_ROOTFS_SPLIT is not set
+# CONFIG_MTD_SM_COMMON is not set
+CONFIG_MTD_SPLIT_FIRMWARE_NAME="linux"
+CONFIG_NFS_FS=y
+CONFIG_NFS_V2=y
+CONFIG_PINCTRL_SVIP=y
+CONFIG_ROOT_NFS=y
+CONFIG_RTL8306_PHY=y
+CONFIG_SOC_SVIP=y
+# CONFIG_SOC_TYPE_XWAY is not set
+# CONFIG_SOC_XWAY is not set
+CONFIG_SYS_SUPPORTS_LITTLE_ENDIAN=y
+# CONFIG_SQUASHFS is not set
+CONFIG_SUNRPC=y
+# CONFIG_USB_ARCH_HAS_EHCI is not set
+# CONFIG_USB_ARCH_HAS_HCD is not set
+# CONFIG_USB_ARCH_HAS_OHCI is not set
+# CONFIG_USB_ARCH_HAS_XHCI is not set
+CONFIG_USB_SUPPORT=y
diff --git a/target/linux/lantiq/svip_le/target.mk b/target/linux/lantiq/svip_le/target.mk
new file mode 100644
--- /dev/null
+++ b/target/linux/lantiq/svip_le/target.mk
@@ -0,0 +1,11 @@
+ARCH:=mipsel
+SUBTARGET:=svip_le
+BOARDNAME:=SVIP Little Endian
+FEATURES:=squashfs jffs2
+DEVICE_TYPE:=other
+
+DEFAULT_PACKAGES+= uboot-svip hostapd-mini
+
+define Target/Description
+	Lantiq SVIP Little Endian
+endef
