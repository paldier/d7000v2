VPE Management Block (VMB) Implementation. Implemented APIs for VPE/CPU and TC lauching/stopping.

diff --git a/arch/mips/Kconfig b/arch/mips/Kconfig
--- a/arch/mips/Kconfig
+++ b/arch/mips/Kconfig
@@ -2088,6 +2088,20 @@ config MIPS_CMP
 	help
 	  Enable Coherency Manager processor (CMP) support.
 
+config LTQ_VMB
+	bool "Lantiq VPE Management Block (VMB)"
+	depends on MIPS_CMP && (SOC_GRX500 || MIPS_MALTA)
+	default n
+	help
+	 Lantiq VPE Management Block support to launch Secondary FW/Linux
+
+config LTQ_DYN_CPU_ALLOC
+	bool "Dynamic CPU allocation in vmb_cpu_alloc"
+	depends on LTQ_VMB
+	default n
+	help
+	 Option to select CPU dynamically using vmb_cpu_alloc. This overrides the mapping on CPU to FW/Linux.	
+
 config SB1_PASS_1_WORKAROUNDS
 	bool
 	depends on CPU_SB1_PASS_1
diff --git a/arch/mips/include/asm/ltq_vmb.h b/arch/mips/include/asm/ltq_vmb.h
new file mode 100644
--- /dev/null
+++ b/arch/mips/include/asm/ltq_vmb.h
@@ -0,0 +1,209 @@
+/******************************************************************************
+
+                         Copyright (c) 2012, 2014, 2015
+                            Lantiq Deutschland GmbH
+
+  For licensing information, see the file 'LICENSE' in the root folder of
+  this software module.
+
+******************************************************************************/
+
+#ifndef _LTQ_VMB_H
+#define _LTQ_VMB_H
+
+#include <asm/gic.h>
+#include <asm/gcmpregs.h>
+
+//const uint8_t vmb_cores = (GCMPGCB(GC) & GCMP_GCB_GC_NUMCORES_MSK) + 1;
+//const uint8_t vmb_vpes = (GCMPCLCB(CFG) & GCMP_CCB_CFG_NUMVPE_MSK) + 1;
+//#define MAX_CORE vmb_cores
+//#define MAX_CPUS vmb_vpes
+
+#define MAX_CORE 2 
+#define MAX_VPE 2 
+#define MAX_TC	6	/* Used internally for Database ONLY */
+#define MAX_TCS	4
+#define MAX_CPU (MAX_CORE * MAX_VPE)
+
+#define NLINUX_CPUS (vmb_vpes - NR_CPUS)
+
+#define vmb_msg_size (sizeof(VMB_fw_msg_t) + sizeof(FW_vmb_msg_t))
+#define CPU_LAUNCH 0xa0000000
+
+/* REVERT BACK TO 10 */
+#define QUEUE_TIMEOUT (100 * HZ) 
+
+#define get_cpu_id(core, vpe)   ((core * 2) + vpe)
+#define which_core(cpu)         (cpu / 2)
+#define vpe_in_core(cpu)        (cpu % 2)
+
+
+#define VMB_CPU_START	0x00000001
+#define VMB_CPU_STOP	0x00000002
+#define VMB_TC_START	0x00000004
+#define VMB_TC_STOP	0x00000008
+#define VMB_TC_PAUSE	0x00000010
+#define VMB_TC_RESUME	0x00000020
+
+#define FW_VMB_ACK	0x00000001
+#define FW_VMB_NACK	0x00000002
+#define FW_RESET	0x00000004
+#define IBL_IN_WAIT	0x00000008
+#define FW_VMB_PRIV_INFO	0x00000010
+
+
+#define IBL_ACTIVE	0x1
+#define IBL_INACTIVE	0x2
+
+#define CORE_ACTIVE	0x1
+#define CORE_INACTIVE	0x2
+
+#define CPU_ACTIVE	0x1
+#define CPU_INACTIVE	0x2
+#define CPU_BOOTUP_DT	0x10
+
+#define TC_ACTIVE	0x1
+#define TC_INACTIVE	0x2
+
+#define MAX_YIELD_INTF	16
+#define	YR_ACTIVE	IBL_ACTIVE
+#define YR_INACTIVE	IBL_INACTIVE
+
+#define MAX_ITC_SEMID	16
+#define	ITC_ACTIVE	IBL_ACTIVE
+#define ITC_INACTIVE	IBL_INACTIVE
+
+/* ERROR codes */
+#define VMB_SUCCESS		1
+#define VMB_ERROR		2
+#define VMB_EBUSY		3
+#define VMB_EAVAIL		4
+#define VMB_ERESET		5
+#define VMB_ETIMEOUT		6
+#define VMB_ENACK		7
+#define VMB_ENOPRM		8
+
+
+typedef struct {
+	uint32_t	start_addr;
+	uint32_t	sp;
+	uint32_t	gp;
+	uint32_t	a0;
+	uint32_t	eva;
+	uint32_t	mt_group;
+	uint32_t	yield_res;
+	uint32_t	priv_info;
+} CPU_launch_t; 
+
+typedef struct {
+	uint32_t	tc_num;
+	uint32_t	mt_group;
+	uint32_t	start_addr;
+	uint32_t	sp;
+	uint32_t	gp;
+	uint32_t	a0;
+	uint32_t	state;
+	uint32_t	priv_info;
+} TC_launch_t;
+
+typedef struct {
+	uint32_t	msg_id;
+	CPU_launch_t	cpu_launch;
+	TC_launch_t	tc_launch[MAX_TCS];
+	uint32_t	tc_num;
+} VMB_fw_msg_t;
+
+typedef struct {
+	uint32_t	status;
+	uint32_t	priv_info;
+} FW_vmb_msg_t;
+
+/* Structure for Event Handling used during handshake between VMB and FW/Linux */
+
+typedef struct { 
+	wait_queue_head_t	vmb_wq;   /* event object */
+	uint8_t		wakeup_vpe;  	 /* wakeup condition flag */
+} VMB_evt_t;
+
+typedef struct	{
+	uint8_t		vpe_id; /* Indicates which VPE this TC is attached */
+	uint8_t		tc_status; 
+	uint8_t         tcmt_grp;
+} VMB_tc_t;
+
+typedef struct {
+	int8_t cpu_id;
+	int8_t yr_status;	
+} VMB_yr_t;
+
+typedef struct {
+	int8_t cpu_id;
+	int8_t tc_id;
+	int8_t itc_status;
+} VMB_itc_t;
+
+typedef struct {
+	char name[16];	/* Name of the FW or LinuxOS */
+	int8_t		bl_status;
+	int8_t		cpu_status;
+	int8_t		core_id;
+	int8_t		cpu_id; /* Keep track of CPU numbers like CORE0/VPE0 - CPU0 , 
+				CORE0/VPE1 - CPU1, CORE1/VPE0 - CPU2, CORE1/VPE1 - CPU3. This will save lot of internal processing */
+	spinlock_t	vpe_lock;
+	void	(*vmb_callback_fn)(uint32_t status);
+	VMB_evt_t 		v_wq;
+	FW_vmb_msg_t		fw_vmb;
+	uint8_t		vpemt_grp;
+} VMB_vpe_t;
+
+typedef struct {
+	uint8_t		active; /* May be used for power management or hotplug to check the status of the core */
+	VMB_vpe_t	vpe_t[MAX_VPE];	
+	VMB_tc_t	tc_t[MAX_TC];	/* TCs can be unevenly distrubited to VPEs so should be under core structure and not in vpe structure */ 
+	VMB_yr_t	yr_t[MAX_YIELD_INTF];
+} VMB_core_t;
+
+/* IRQ Numbers and handler definations */
+	#define	 VMB_CPU_IPI1		20
+	#define	 VMB_CPU_IPI2		21
+	#define	 VMB_CPU_IPI3		85
+
+/* 
+  IPI numbers generated from FW (MPE/Voice) --> VMB for handling of requests w.r.t initial  boot-up handshake OR reset/NMI scenarios
+  The fw_vmb_msg_t->status field acts as the differentiator to recognise if the IPI is from FW  due FW_VMB_ACK or NACK or FW_RESET (Async reset)
+  IPI numbers generated from InterAptiv-BL --> VMB to indicate that BL is ready. 
+  The fw_vmb_msg_t->status field acts as the differentiator to recognise if the IPI is from FW (FW_VMB_ACK or NACK) OR from InterAptiv-BL (IBL_IN_WAIT) 
+*/
+ 
+	#define FW_VMB_IPI1		(87 + MIPS_GIC_IRQ_BASE) /* FW/IBL on CPU1 -> VMB */
+	#define FW_VMB_IPI2		(88 + MIPS_GIC_IRQ_BASE) /* FW/IBL on CPU2 -> VMB */
+	#define FW_VMB_IPI3		(110 + MIPS_GIC_IRQ_BASE) /* FW/IBL on CPU3 -> VMB */
+
+/* Function Definations as per Spec*/
+/* return type should be int[16/8]_t due to negative error return types */
+/* if int8_t used then retrun type cannot cross +- 253 */
+
+typedef void (*vmb_callback_func)(uint32_t status);
+
+int8_t vmb_cpu_alloc(int8_t cpu, char* fw_name);
+int8_t vmb_cpu_start(int8_t cpu, CPU_launch_t cpu_launch, TC_launch_t tc_launch[], uint8_t num_tcs, uint8_t num_yr);
+int8_t vmb_cpu_free(int8_t cpu);
+int8_t vmb_cpu_stop(int8_t cpu);
+int8_t vmb_cpu_force_stop(int8_t cpu);
+void vmb_register_callback (uint8_t cpu_num, vmb_callback_func func); 
+
+int8_t vmb_tc_stop(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_tc_start(uint8_t cpu, TC_launch_t tc_launch[], uint8_t num_tcs);
+int8_t vmb_tc_alloc (uint8_t cpu);
+int8_t vmb_tc_free(int8_t cpu, int8_t tc_num);
+int8_t vmb_tc_pause(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_tc_resume(uint8_t cpu, uint8_t tc_num);
+int8_t vmb_get_vpeid (uint8_t cpu, uint8_t tc_num);
+void *VMB_get_msg_addr (int cpu, int direction);
+int32_t vmb_yr_get(uint8_t cpu, uint16_t num_yr);
+void vmb_yr_free (uint8_t cpu, int16_t yr);
+int32_t vmb_itc_sem_get(uint8_t cpu, uint8_t tc_num);
+void vmb_itc_sem_free(int8_t semID);
+int32_t fw_vmb_get_irq(uint8_t cpu);
+
+#endif
diff --git a/arch/mips/kernel/smp-cmp.c b/arch/mips/kernel/smp-cmp.c
--- a/arch/mips/kernel/smp-cmp.c
+++ b/arch/mips/kernel/smp-cmp.c
@@ -40,6 +40,9 @@
 #include <asm/gic.h>
 #include <asm/gcmpregs.h>
 #include <asm/bootinfo.h>
+#ifdef CONFIG_LTQ_VMB
+#include <asm/ltq_vmb.h>
+#endif
 
 static void ipi_call_function(unsigned int cpu)
 {
@@ -247,6 +250,10 @@ static void cmp_boot_secondary(int cpu, 
 	unsigned long sp = __KSTK_TOS(idle);
 	unsigned long pc = (unsigned long)&smp_bootstrap;
 	unsigned long a0 = 0;
+#ifdef CONFIG_LTQ_VMB
+	int ret;
+	CPU_launch_t cpu_launch;
+#endif
 
 	pr_debug("SMPCMP: CPU%d: %s cpu %d\n", smp_processor_id(),
 		__func__, cpu);
@@ -257,7 +264,40 @@ static void cmp_boot_secondary(int cpu, 
 			   (unsigned long)(gp + sizeof(struct thread_info)));
 #endif
 
+#ifdef CONFIG_LTQ_VMB
+	ret = vmb_cpu_alloc(cpu, "LINUX");
+	if (ret == -VMB_EBUSY) {
+		printk ("VPE %d is Busy !!! \n", cpu);
+		ret = vmb_cpu_alloc(MAX_CPU, "LINUX");
+			printk ("[%s]:[%d] CPU ret = %d \n", __FUNCTION__, __LINE__, ret);
+			if (ret == -VMB_EBUSY) {
+				printk ("ALL the CPUs are Busy !!!! \n");
+				return;
+			}
+	}
+
+	memset(&cpu_launch, 0, sizeof(CPU_launch_t));
+
+#ifdef CONFIG_EVA
+	cpu_launch.start_addr = CKSEG1ADDR(pc);
+#else	
+	cpu_launch.start_addr = pc;
+#endif
+	cpu_launch.sp = sp;
+	cpu_launch.gp = (unsigned long)gp;
+	cpu_launch.a0 =  a0;
+
+	/* printk ("[%s]:[%d] vmb_cpu_start from smp-cmp.c sizeof(CPU_launch_t) = %x CPU = %d PC = %x, SP =%lx , gp = %lx, a0 = %lx !!!! \n ", __FUNCTION__, __LINE__, sizeof(CPU_launch_t), cpu, cpu_launch.start_addr , sp , (unsigned long)gp, a0 ); */
+
+	ret = vmb_cpu_start(ret, cpu_launch, 0, 0, 0);
+	if (ret == -VMB_ETIMEOUT || ret == -VMB_ENACK) {
+		printk ("[%s]:[%d] FW %s could not be launched on CPU %d. Please use another CPU \n", __FUNCTION__, __LINE__, "LINUX", cpu );
+		printk ("The CPU has been force reset. Please use alloc and then start .. \n");
+		return;
+	}
+#else	
 	amon_cpu_start(cpu, pc, sp, (unsigned long)gp, a0);
+#endif
 }
 
 /*
diff --git a/arch/mips/lantiq/Makefile b/arch/mips/lantiq/Makefile
--- a/arch/mips/lantiq/Makefile
+++ b/arch/mips/lantiq/Makefile
@@ -11,3 +11,4 @@ obj-$(CONFIG_EARLY_PRINTK) += early_prin
 obj-$(CONFIG_SOC_TYPE_XWAY) += xway/
 obj-$(CONFIG_SOC_FALCON) += falcon/
 obj-$(CONFIG_MIPS_CMP)+=lantiq-amon.o
+obj-$(CONFIG_LTQ_VMB)+=lantiq-vmb.o
diff --git a/arch/mips/lantiq/lantiq-vmb.c b/arch/mips/lantiq/lantiq-vmb.c
new file mode 100644
--- /dev/null
+++ b/arch/mips/lantiq/lantiq-vmb.c
@@ -0,0 +1,1489 @@
+/******************************************************************************
+
+                         Copyright (c) 2012, 2014, 2015
+                            Lantiq Deutschland GmbH
+
+  For licensing information, see the file 'LICENSE' in the root folder of
+  this software module.
+
+******************************************************************************/
+
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/interrupt.h>
+#include <asm/cacheflush.h>
+#include <asm/smp-ops.h>
+#include <asm/traps.h>
+#include <asm/fw/fw.h>
+#include <asm/gcmpregs.h>
+#include <linux/of_platform.h>
+#include <linux/of_fdt.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sched.h>
+#include <linux/bootmem.h>
+#include <asm/ltq_vmb.h>
+
+#undef LINUX_SHM_DDR
+
+/* Number of cores*/
+VMB_core_t core_t[MAX_CORE];
+static DEFINE_SPINLOCK(vmb_lock);
+
+/* ITC SemIDs, Commom to all cores */
+VMB_itc_t	itc_t[MAX_ITC_SEMID];
+
+static struct proc_dir_entry *vmb_proc, *vmb_proc_dir ;
+extern void amon_cpu_start(int, unsigned long, unsigned long,
+                    unsigned long, unsigned long);
+
+void *VMB_get_msg_addr (int cpu, int direction);
+void vmb_itc_sem_free_pcpu (int8_t cpu);
+
+static irqreturn_t fw_vmb_ipi1_hdlr(int irq, void *ptr) {
+	int cpu, cid =0 , vid=0;
+	VMB_vpe_t *vpet_t;
+	FW_vmb_msg_t *fw_t ;
+
+	printk ("[%s]:[%d] CPU = %d irq = %d \n",__FUNCTION__, __LINE__, smp_processor_id(), (irq - MIPS_GIC_IRQ_BASE));
+	
+	/* This handler is from FW_VMB_IPI1 so called from CPU1 */
+	cpu = 1;
+	cid = which_core(cpu);
+	vid = vpe_in_core(cpu);
+
+	printk ("[%s]:[%d] cid = %d , vid = %d\n",__FUNCTION__, __LINE__, cid, vid);
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/* May lead to deadlock in case of FW_RESET as spinlock for that VPE is taken and then wait_event_called thus 
+	   commenting the locks if this doesnt work then need to use  IBL_IN_ handler to wakeup this MPE handler*/
+
+
+	/* W.r.t FW copy the DDR section to Internal DB */
+        memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+        smp_rmb();
+
+	if(vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+		/* Be cautious : TEST Throughly, VPE will go for a reset so it will be in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res1_ext;	
+
+	} else if(vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		vpet_t->bl_status = IBL_ACTIVE;
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+	}
+
+       /* Clear DDR section */
+       //  memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+
+	if (vpet_t->fw_vmb.status == (uint32_t)FW_VMB_ACK)
+		printk ("[%s]:[%d] \n",__FUNCTION__, __LINE__);
+
+	printk ("[%s]:[%d] \n",__FUNCTION__, __LINE__);
+	printk ("  vmb_wq = %p \n",  &vpet_t->v_wq.vmb_wq);
+
+        /* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+        wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res1_ext:
+	return IRQ_HANDLED;	
+}
+
+static irqreturn_t fw_vmb_ipi2_hdlr(int irq, void *ptr) {
+	int cpu, cid =0 , vid=0;
+	VMB_vpe_t *vpet_t;
+	FW_vmb_msg_t *fw_t ;
+
+        /* This handler is from FW_VMB_IPI2 so called from CPU2 */
+        cpu = 2;
+        cid = which_core(cpu);
+        vid = vpe_in_core(cpu);
+
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/* May lead to deadlock in case of FW_RESET as spinlock for that VPE is taken and then wait_event_called thus 
+	   commenting the locks if this doesnt work then need to use  IBL_IN_ handler to wakeup this MPE handler*/
+
+
+	/* W.r.t FW copy the DDR section to Internal DB */
+        memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+        smp_rmb();
+
+	if(vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+	
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+		/* Be cautious : TEST Throughly, VPE will go for a reset so it will be in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res2_ext;	
+
+	} else if(vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		vpet_t->bl_status = IBL_ACTIVE;
+
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+	}
+
+       /* Clear DDR section */
+       //  memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+
+        /* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+        wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res2_ext:
+	return IRQ_HANDLED;	
+}
+
+static irqreturn_t fw_vmb_ipi3_hdlr(int irq, void *ptr) {
+	int cpu, cid =0 , vid=0;
+	VMB_vpe_t *vpet_t;
+	FW_vmb_msg_t *fw_t ;
+
+	printk ("[%s]:[%d] CPU = %d irq = %d \n",__FUNCTION__, __LINE__, smp_processor_id(), (irq - MIPS_GIC_IRQ_BASE));
+	
+	/* This handler is from FW_VMB_IPI3 so called from CPU3 */
+	cpu = 3;
+	cid = which_core(cpu);
+	vid = vpe_in_core(cpu);
+
+	printk ("[%s]:[%d] cid = %d , vid = %d\n",__FUNCTION__, __LINE__, cid, vid);
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/* May lead to deadlock in case of FW_RESET as spinlock for that VPE is taken and then wait_event_called thus 
+	   commenting the locks if this doesnt work then need to use  IBL_IN_ handler to wakeup this MPE handler*/
+
+
+	/* W.r.t FW copy the DDR section to Internal DB */
+        memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+        smp_rmb();
+
+	if(vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+		/* Be cautious : TEST Throughly, VPE will go for a reset so it will be in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res3_ext;	
+
+	} else if(vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+
+		vpet_t->bl_status = IBL_ACTIVE;
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+	}
+
+       /* Clear DDR section */
+       //  memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+
+	if (vpet_t->fw_vmb.status == (uint32_t)FW_VMB_ACK)
+		printk ("[%s]:[%d] \n",__FUNCTION__, __LINE__);
+
+	printk ("[%s]:[%d] \n",__FUNCTION__, __LINE__);
+	printk ("  vmb_wq = %p \n",  &vpet_t->v_wq.vmb_wq);
+
+        /* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+        wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res3_ext:
+	return IRQ_HANDLED;	
+}
+
+/* Update a dummy vmb_msg_t->status in DB as this is for Linux SMP which doesn't update DDR in Linux --> VMB direction*/
+static int linux_cpu_ipi_update(unsigned long cpu, unsigned long action) {
+	unsigned long flags;
+	int c_id = which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet_t = &core_t[c_id].vpe_t[v_id];
+
+#ifdef LINUX_SHM_DDR
+	FW_vmb_msg_t *fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);	
+	
+	/* W.r.t FW copy the DDR section to Internal DB */
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+	smp_rmb();
+
+	/* Clear DDR section */
+	memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+	
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#else
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);	
+
+	vpet_t->fw_vmb.status= (uint32_t)FW_VMB_ACK;
+	vpet_t->fw_vmb.priv_info = 0;
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#endif
+	return 0;
+}
+
+static int __cpuinit vmb_cpu_active(struct notifier_block *nfb,
+                                      unsigned long action, void *hcpu)
+{
+        switch (action) {
+        case CPU_STARTING:
+                printk ("[%s]:[%d] action = %ld , cpu = %ld\n", __FUNCTION__, __LINE__, action, (long)hcpu);
+		/* Come here on LINUX secondary is up . Get the DDR and update DB on the cpu_status, boot_flag and ACK and NACK and 
+		   wakeup the waitqueue in vmb_cpu_start() */
+		linux_cpu_ipi_update((unsigned long)hcpu, action);
+                return NOTIFY_OK;
+        default:
+                return NOTIFY_DONE;
+        }
+
+	return 0;
+}
+
+#if 1
+/* API to get the memory in DDR for strutures VMB_fw_msg_t/FW_vmb_msg_t */
+void *VMB_get_msg_addr (int cpu, int direction) {
+#ifdef CONFIG_EVA
+        void *msg_t =  (void *)(CPU_LAUNCH);
+#else
+        void *msg_t =  (void *)CKSEG0ADDR(CPU_LAUNCH);
+#endif
+
+        /* VMB --> FW : VMB_fw_msg_t structure */
+        if (direction == 0)
+                msg_t = msg_t + (vmb_msg_size * cpu) + sizeof(FW_vmb_msg_t);
+        else
+                msg_t = msg_t + (vmb_msg_size * cpu);
+
+        return (msg_t);
+}
+#else
+/* API to get the memory in DDR for strutures VMB_fw_msg_t/FW_vmb_msg_t */
+void *VMB_get_msg_addr (int cpu, int direction) {
+#ifdef CONFIG_EVA
+	void *msg_t =  (void *)(CPU_LAUNCH);
+#else
+	void *msg_t =  (void *)CKSEG0ADDR(CPU_LAUNCH);
+#endif
+
+	/* VMB --> FW : VMB_fw_msg_t structure */
+	if (direction == 0)
+		msg_t = msg_t + (vmb_msg_size * cpu) ;
+	else
+		msg_t = msg_t + (vmb_msg_size * cpu) + sizeof(VMB_fw_msg_t);
+
+	return (msg_t);
+}
+#endif
+
+/* Lock --> Access and Copy DDR --> check for status and update Internal DB */
+static void vmb_check_IBL_fw_msg(void) {
+	int i,j;
+	VMB_core_t *ct = core_t;
+	FW_vmb_msg_t *fw_msg_t;
+
+       for (i=0; i < MAX_CORE; i++) {
+		VMB_vpe_t *vpet = ct[i].vpe_t;
+		int cpu;
+           for (j=0; j < MAX_VPE; j++) {
+			if ((i == 0) && (j == 0))
+				continue;
+
+                	spin_lock(&vpet[j].vpe_lock);
+			cpu = get_cpu_id(i, j);
+                	printk ("[%s]:[%d] vpet[j].bl_status = %d , cpu = %d\n", __FUNCTION__, __LINE__, vpet[j].bl_status, cpu);
+			fw_msg_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+			memcpy(&vpet[j].fw_vmb, fw_msg_t, sizeof(FW_vmb_msg_t));
+			if (vpet[j].fw_vmb.status == IBL_IN_WAIT)
+				vpet[j].bl_status = IBL_ACTIVE;
+			else
+				vpet[j].bl_status = IBL_INACTIVE;
+                	spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+}
+
+static int update_DB_from_DT(VMB_vpe_t *vt) {
+	struct device_node *np;
+	char str1[16], *name;
+	int ret;
+
+	memset(str1, '\0', sizeof(str1));
+	sprintf(str1,"%s%d","/cpus/cpu@",vt->cpu_id);
+
+	np = of_find_node_by_path(str1);
+	if (!np)
+                return -ENODEV;
+
+	ret = of_property_read_string_index(np, "default-OS", 0, (const char **)&name);
+      	if (ret < 0 && ret != -EINVAL) {
+		printk ("ERROR : Property could be read from DT \n");
+                return ret;
+        }
+
+	strncpy(vt->name, name, sizeof(vt->name));
+
+	printk("[%s]:[%d], cpuid = %d name = %s \n", __FUNCTION__, __LINE__, vt->cpu_id, vt->name);
+
+	vt->cpu_status |= CPU_BOOTUP_DT;
+
+	return 0;
+}
+
+static void initialise_vmb_DB(void) {
+	VMB_core_t *coret = core_t;
+	int i,j;
+
+	coret[0].active |= CORE_ACTIVE;
+
+	for (i=0; i < MAX_CORE; i++) {
+		VMB_tc_t *tct = coret[i].tc_t;
+		VMB_vpe_t *vpet = coret[i].vpe_t;
+		VMB_yr_t *yrit = coret[i].yr_t;
+
+		for (j=0; j < MAX_VPE; j++) {
+			vpet[j].vpemt_grp = 0;
+			vpet[j].core_id = (((i * 2) + j)/2);
+			spin_lock_init(&vpet[j].vpe_lock);
+			init_waitqueue_head(&vpet[j].v_wq.vmb_wq);
+			vpet[j].v_wq.wakeup_vpe = 0;
+			vpet[j].cpu_id = ((i * 2) + j);
+			if ((i == 0) && (j == 0)) { 
+				/* Core0/VPE0 always active*/
+				vpet[j].bl_status = IBL_ACTIVE;
+				vpet[j].cpu_status = CPU_ACTIVE;
+			} else {
+				vpet[j].bl_status = IBL_INACTIVE;
+				vpet[j].cpu_status = CPU_INACTIVE;
+			}
+			tct[j].vpe_id = j;
+			/* Get the information from DT for the FW names  also update the bootflag for cpu */
+			update_DB_from_DT(&vpet[j]);
+		}
+
+		for (j=0; j < MAX_TC; j++) {
+			if (j >= 2) {
+				tct[j].vpe_id = MAX_VPE;
+				tct[j].tc_status = TC_INACTIVE;
+			} else {
+				tct[j].vpe_id = j;
+				tct[j].tc_status = TC_ACTIVE;
+			}
+			tct[j].tcmt_grp = 0;	
+		}
+
+		for (j=0; j < MAX_YIELD_INTF; j++) {
+			yrit[j].yr_status = YR_INACTIVE;
+			yrit[j].cpu_id = -1;
+		}
+
+		for (j=0; j < MAX_ITC_SEMID; j++) {
+			itc_t[j].itc_status = ITC_INACTIVE;
+			itc_t[j].cpu_id = -1;
+			itc_t[j].tc_id = -1;
+		}
+	}
+}
+
+
+static void initialise_vmb_IRQhdlr(void) {
+	int err = 0;
+
+	err = request_irq(FW_VMB_IPI1, fw_vmb_ipi1_hdlr, IRQF_DISABLED, "fw_vmb_ipi1", NULL);
+	if (err)
+		printk ("request_irq for IRQ FW_VMB_IPI1 = %d failed !!! \n", FW_VMB_IPI1);
+
+	err = request_irq(FW_VMB_IPI2, fw_vmb_ipi2_hdlr, IRQF_DISABLED, "fw_vmb_ipi2", NULL);
+	if (err)
+		printk ("request_irq for IRQ FW_VMB_IPI2 = %d failed !!! \n", FW_VMB_IPI2);
+
+	err = request_irq(FW_VMB_IPI3, fw_vmb_ipi3_hdlr, IRQF_DISABLED, "fw_vmb_ipi3", NULL);
+	if (err)
+		printk ("request_irq for IRQ FW_VMB_IPI3 = %d failed !!! \n", FW_VMB_IPI3);
+
+	
+	enable_irq(VMB_CPU_IPI1 + MIPS_GIC_IRQ_BASE);
+	enable_irq(VMB_CPU_IPI2 + MIPS_GIC_IRQ_BASE);
+	enable_irq(VMB_CPU_IPI3 + MIPS_GIC_IRQ_BASE);
+
+}
+
+/* Dump the tree */
+static int dump_vmb_tree(struct seq_file *s, void *v) {
+	VMB_core_t *coret = core_t;
+	int i,j,k,cpu_id ;
+
+        for (i=0; i < MAX_CORE; i++){
+                VMB_tc_t *tct = coret[i].tc_t;
+                VMB_vpe_t *vpet = coret[i].vpe_t;
+		VMB_yr_t *yr = coret[i].yr_t;
+
+		for (j=0; j < MAX_VPE; j++) {
+		spin_lock(&vpet[j].vpe_lock);
+			cpu_id = get_cpu_id (i, j);
+			seq_printf (s, "CORE ID %d \n", i);
+			seq_printf (s, "\t - VPE ID %d \n", j);
+			seq_printf (s, "\t \t - OS Name = %s \n", vpet[j].name);
+			seq_printf (s, "\t \t - InterAptiv-BL status = %s \n", (vpet[j].bl_status == IBL_ACTIVE ? "ACTIVE" : "INACTIVE"));
+			seq_printf (s, "\t \t - CPU status = %s \n", ((vpet[j].cpu_status & CPU_ACTIVE) == CPU_ACTIVE ? "ACTIVE" : "INACTIVE"));
+			seq_printf (s, "\t \t - CPU ID = %d \n", vpet[j].cpu_id);
+			seq_printf (s, "\t \t - Allocated TC(s) =  ");
+			for (k=0; k < MAX_TC; k++) {
+				if (tct[k].tc_status == TC_ACTIVE && cpu_id == get_cpu_id(i, tct[k].vpe_id))
+					seq_printf (s, "TC%d, ", k );
+			}
+			seq_printf (s, "\t \t \n");
+			seq_printf (s, "\t \t - Allocated Yield Resource(s) =  ");
+			for (k=0; k < MAX_YIELD_INTF; k++) {
+				if (yr[k].yr_status == YR_ACTIVE && cpu_id == yr[k].cpu_id)
+					seq_printf (s, "%d, ", k );
+			}
+			seq_printf (s, "\t \t \n");
+			seq_printf (s, "\t \t - Allocated ITC Resource(s) =  ");
+			for (k=0; k < MAX_ITC_SEMID; k++) {
+				if (itc_t[k].itc_status == ITC_ACTIVE && cpu_id == itc_t[k].cpu_id)
+					seq_printf (s, "%d, ", k );
+			}
+			seq_printf (s, "\t \t \n");
+			seq_printf (s, "\t \t - FW_VMB_MSG Structure \n");
+			seq_printf (s, "\t \t \t - status = 0X%x\n", vpet[j].fw_vmb.status);
+			seq_printf (s, "\t \t \t - priv_info = 0X%x\n", vpet[j].fw_vmb.priv_info);
+				
+		spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+
+	return 0;
+}
+
+static int vmb_proc_open(struct inode *inode, struct file *file)
+{
+ return single_open(file, dump_vmb_tree , NULL);
+}
+
+static const struct file_operations vmb_proc_fops = {
+ .open           = vmb_proc_open,
+ .read           = seq_read,
+ .llseek         = seq_lseek,
+ .release        = single_release,
+};
+
+/* Initialise VMB structures and IRQ */
+static int32_t vmb_init(void) 
+{
+	memset(core_t, 0 , sizeof(VMB_core_t));
+
+	printk ("size of core = %d, MAXCORE = %d , MAXCPU = %d, MAXTCS = %d MAX_VPE = %d\n", sizeof(core_t), MAX_CORE, MAX_CPU, MAX_TC, MAX_VPE);
+
+	/* CPU notifier for Linux SMP bootup indication notify (CPU_STARTING) from start_secondary */
+	cpu_notifier(vmb_cpu_active, CPU_PRI_SCHED_ACTIVE);
+
+	/* Initialise a New core before vmbDB init as it will get time for dumping IBL_INWAIT in DDR 
+           which can be picked by vmb_check_IBL() */
+
+
+	/* Basic initialisation of the DB */
+	initialise_vmb_DB();
+
+	/* register IRQ handlers for VMB_FW_IRQx */
+	initialise_vmb_IRQhdlr();
+	
+	/* Initialise the /proc/vm/debug to dump the Internal DB */
+	vmb_proc_dir = proc_mkdir("vmb", NULL);
+	vmb_proc = proc_create("status", 0644, vmb_proc_dir, &vmb_proc_fops);
+
+	/* API to update th IBL status if the IBL_IN_WAIT IPI is missed during initialisation. 
+	   Mainly a fail-safe Linux running VPE updates as there is no IPI mentioned to handle IBL_INWAIT for Linux */
+	vmb_check_IBL_fw_msg();
+
+	spin_lock_init(&vmb_lock);
+
+	return 0;
+}
+
+early_initcall(vmb_init);
+
+/* 
+   This function is needed to trigger GIC IPI Interrupt .
+   Needed as IPI IRQ number are not consecutive . Currently we use 20,21 and 85 for CPU1,2 adnd 3 respectviely 
+*/
+
+static void gic_trigger(int8_t cpu) {
+	switch(cpu) {
+		case 1:
+			printk ("[%s]:[%d] VMB_CPU_IPI1 - %d CPU = %d\n", __FUNCTION__, __LINE__, VMB_CPU_IPI1, cpu);
+			gic_send_ipi(VMB_CPU_IPI1);
+			break;
+		case 2:
+			printk ("[%s]:[%d] VMB_CPU_IPI2 - %d CPU = %d\n", __FUNCTION__, __LINE__, VMB_CPU_IPI2, cpu);
+			gic_send_ipi(VMB_CPU_IPI2);
+			break;
+		case 3:
+			printk ("[%s]:[%d] VMB_CPU_IPI3 - %d CPU = %d\n", __FUNCTION__, __LINE__, VMB_CPU_IPI3, cpu);
+			gic_send_ipi(VMB_CPU_IPI3);
+			break;
+		default:
+			break;
+	}
+
+	return;
+}
+
+/* Allocate Free CPU from the pool */
+int8_t vmb_cpu_alloc(int8_t cpu, char* fw_name) {
+	int ret = -VMB_ERROR;
+
+/*CHECK : May be for alloc we need to have a global lock to avoid a scenario where 2 FWs call at the same time race condi 
+and getting same cpu number */
+
+	if (cpu == MAX_CPU) {
+        	VMB_core_t *coret = core_t;
+        	int i,j;
+
+        	for (i=0; i < MAX_CORE; i++) {
+                	VMB_vpe_t *vpet = coret[i].vpe_t;
+                	for (j=0; j < MAX_VPE; j++) {
+				if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+					if (strncmp(vpet[j].name, fw_name, sizeof(vpet[j].name)) != 0) {
+						ret=-VMB_EAVAIL;
+						continue;
+					}
+				}
+
+				if ((vpet[j].bl_status == IBL_ACTIVE) && ((vpet[j].cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+					ret = get_cpu_id(i, j);
+					vpet[j].cpu_status &= ~CPU_INACTIVE;
+					vpet[j].cpu_status |= CPU_ACTIVE;
+					printk ("[%s]:[%d] CPU vpet[j].cpu_status = %x \n", __FUNCTION__, __LINE__, vpet[j].cpu_status);
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+					if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+						vpet[j].cpu_status &= ~CPU_BOOTUP_DT;
+					else
+						strncpy(vpet[j].name, fw_name, sizeof(vpet[j].name));
+#endif
+					goto fin_alloc;
+				} else 
+					ret = -VMB_EBUSY;
+			}
+		}
+	} else {
+        	int c_id= which_core(cpu);
+	        int v_id = vpe_in_core(cpu);
+        	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+		
+		if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+			if (strncmp(vpet->name, fw_name, sizeof(vpet->name)) != 0) {
+				printk("WARNING : As per DT Bootup Scenario %s shouldn't run on CPU %d \n", fw_name, cpu);
+				printk(" Please retry with MAX_CPUS !!! \n");
+				ret=-VMB_EAVAIL;
+				goto fin_alloc;
+			}
+		}
+		
+		if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+			ret = cpu;
+			vpet->cpu_status &= ~CPU_INACTIVE;
+			vpet->cpu_status |= CPU_ACTIVE;
+			printk ("[%s]:[%d] CPU vpet.cpu_status = %x \n", __FUNCTION__, __LINE__, vpet->cpu_status);
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+			if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+				vpet->cpu_status &= ~CPU_BOOTUP_DT;
+			else
+				strncpy(vpet->name, fw_name, sizeof(vpet->name));
+#endif
+		}
+		else 
+			ret = -VMB_EBUSY;
+	}
+
+fin_alloc: 
+	return ret;
+}
+
+void vmb_register_callback (uint8_t cpu, vmb_callback_func func) {
+	int c_id= which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->vmb_callback_fn = func;
+}
+
+/* Mark the VPE as free (INACTIVE) and Free all the allocated Yield resources to CPU/VPE */
+int8_t vmb_cpu_free(int8_t cpu) {
+	int c_id= which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	if (vpet->bl_status != IBL_ACTIVE)
+		printk("WARNING : IBL is not active for CPU = %d !!! \n", cpu);
+
+	vpet->cpu_status &= ~CPU_ACTIVE;
+	vpet->cpu_status |= CPU_INACTIVE;
+
+	/* Free Yield resorces allocated to this CPU */
+	vmb_yr_free(cpu, -1);
+
+	/* Free ITC SemIDs allocated to this CPU */
+	vmb_itc_sem_free_pcpu(cpu);
+
+	return VMB_SUCCESS;
+}
+
+int8_t vmb_cpu_force_stop(int8_t cpu) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->bl_status &= ~IBL_ACTIVE;
+	vpet->bl_status |= IBL_INACTIVE;
+
+	/* Generate a NMI for that VPE*/
+	
+	/* TODO ????????????*/
+
+	/* Wait for IBL_IN_WAIT or timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("Consider reseting the CPU using vmb_cpu_force_stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+
+		ret = -VMB_ETIMEOUT;	
+		goto fin_fstop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)IBL_IN_WAIT) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU using vmb_cpu_force_stop \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_fstop:
+	return ret;
+}
+
+int8_t vmb_cpu_stop(int8_t cpu) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+		vmb_fw_msg_t->msg_id = VMB_CPU_STOP;
+	} else {
+		ret = -VMB_ERROR;
+		spin_unlock(&vpet->vpe_lock);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		goto fin_stop;
+	}
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+#if 1
+	gic_trigger(cpu);
+        //gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+        gic_send_ipi(VMB_BLW_IPI3);
+#endif
+        mips_ihb();
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;	
+		goto fin_stop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if ((vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) || (vpet->fw_vmb.status == (uint32_t)IBL_IN_WAIT)) {
+		ret = VMB_SUCCESS;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin_stop:
+	return ret;
+}
+
+int8_t vmb_cpu_start(int8_t cpu, CPU_launch_t cpu_launch, TC_launch_t tc_launch[], uint8_t num_tcs, uint8_t num_yr) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+	int32_t yieldres_t = 0;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)&cpu_launch == NULL)
+		return -VMB_ENOPRM;
+
+	if (num_tcs > MAX_TCS) {
+		printk ("[%s]:[%d] num_tcs (%d) greater than MAX_TCS (%d) so reseting num_tcs to MAX_TCS \n", __FUNCTION__, __LINE__, num_tcs, MAX_TCS);
+		num_tcs = MAX_TCS;
+	}
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+/* Request num_yr should be a non-zero value */
+	if (num_yr > 0) {
+		yieldres_t = vmb_yr_get(cpu, num_yr);
+		if (yieldres_t == -VMB_EBUSY) {
+			spin_unlock(&vpet->vpe_lock);
+			printk (KERN_ERR "[%s]:[%d] vmb_cpu_start failed as %d Yield resources not free !!!  \n ", __FUNCTION__, __LINE__, num_yr);
+			ret = -VMB_EBUSY;
+			goto fin;
+		}
+	}
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_CPU_START;
+	memcpy(&vmb_fw_msg_t->cpu_launch, &cpu_launch, sizeof(vmb_fw_msg_t->cpu_launch));
+	if ((void *)tc_launch != NULL) {
+		for (i=0; i < (num_tcs); i++) {
+			memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i], sizeof(vmb_fw_msg_t->tc_launch[i]));
+			vmb_fw_msg_t->tc_launch[i].mt_group = vpet->vpemt_grp;
+		}
+	}
+
+	/* update the allocated yield resource bitmap */
+	vmb_fw_msg_t->cpu_launch.yield_res = (uint32_t)yieldres_t;
+
+	/* update tc_num to indicate number of TCs only in cpu and tc start */
+	vmb_fw_msg_t->tc_num = num_tcs;
+
+	/* Set the vpe mt_grp */
+	vmb_fw_msg_t->cpu_launch.mt_group = vpet->vpemt_grp;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+#if 1
+	gic_trigger(cpu);
+	//gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+	gic_send_ipi(VMB_BLW_IPI3);
+#endif
+	mips_ihb();
+	
+	/* TODO ???? */
+
+#ifdef CONFIG_MIPS_MALTA
+	/* Linux bootup in MALTA*/
+	amon_cpu_start(cpu, cpu_launch.start_addr, cpu_launch.sp, cpu_launch.gp, cpu_launch.a0);
+#endif
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		vpet->v_wq.wakeup_vpe = 0;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;	
+		goto fin;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+	
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin:
+	return ret;
+}
+
+/********************************** VMB TC APIs **************************/
+
+int8_t vmb_tc_alloc (uint8_t cpu) {
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_vpe_t *vpet;
+	
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		VMB_tc_t *tct = coret[c_id].tc_t;
+		for (i=2; i < MAX_TC; i++) {
+			if ((tct[i].tc_status & TC_INACTIVE) == TC_INACTIVE) {
+				tct[i].tc_status &= ~TC_INACTIVE;
+				tct[i].tc_status |= TC_ACTIVE;
+				ret = i;
+				tct[i].vpe_id = v_id;
+				printk ("[%s]:[%d] CPU tct[i].tc_status = %x \n", __FUNCTION__, __LINE__, tct[i].tc_status);
+				goto fin_talloc;
+			} else 
+				ret = -VMB_EBUSY;
+		}
+	}
+fin_talloc:
+		return ret;
+}
+
+
+int8_t vmb_tc_free(int8_t cpu, int8_t tc_num) {
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_vpe_t *vpet;
+	
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE))
+		printk ("VPE %d is not active !!! still freeing the TC %d \n", cpu, tc_num);
+
+	if (tc_num == -1) {
+		VMB_tc_t *tct = coret[c_id].tc_t;
+		for (i=2; i < MAX_TC; i++) {
+			ret = get_cpu_id (c_id, tct[i].vpe_id);
+			if (ret == cpu) {
+				tct[i].tc_status &= ~TC_ACTIVE;
+				tct[i].tc_status |= TC_INACTIVE;
+			}
+		}
+		ret = VMB_SUCCESS;
+	} else {
+		VMB_tc_t *tct = &coret[c_id].tc_t[tc_num];
+		ret = get_cpu_id (c_id, tct->vpe_id);
+
+		if (ret == cpu) {
+			tct->tc_status &= ~TC_ACTIVE;
+			tct->tc_status |= TC_INACTIVE;
+			ret = VMB_SUCCESS;
+		} else {
+			printk (" TC %d is not attached to CPU %d . Please use vmb_get_vpeid(cpu, tc_num) to get CPU to which active TC is attached \n", tc_num, cpu);
+			ret = -VMB_EAVAIL;
+		}
+	}
+	return ret;
+}
+
+int8_t vmb_get_vpeid (uint8_t cpu, uint8_t tc_num) {
+	int ret = -VMB_ERROR;
+	int c_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_tc_t *tct;
+	
+	c_id = which_core(cpu);
+	tct = &coret[c_id].tc_t[tc_num];
+	ret = get_cpu_id (c_id, tct->vpe_id);
+
+	return ret;
+}
+
+int8_t vmb_tc_start(uint8_t cpu, TC_launch_t tc_launch[], uint8_t num_tcs ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)tc_launch == NULL)
+		return -VMB_ENOPRM;
+
+	if (num_tcs > MAX_TCS) {
+		printk ("[%s]:[%d] num_tcs (%d) greater than MAX_TCS (%d) so reseting num_tcs to MAX_TCS \n", __FUNCTION__, __LINE__, num_tcs, MAX_TCS);
+		num_tcs = MAX_TCS;
+	}
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_START;
+
+	for (i=0; i < (num_tcs); i++)
+		memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i], sizeof(vmb_fw_msg_t->tc_launch[i]));
+
+	/* update tc_num to indicate number of TCs only in cpu and tc start */
+	vmb_fw_msg_t->tc_num = num_tcs;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+#if 1
+	gic_trigger(cpu);
+        //gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+        gic_send_ipi(VMB_BLW_IPI3);
+#endif
+        mips_ihb();
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		vmb_tc_free(cpu, -1);
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcstart;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_tc_free(cpu, -1);
+	}
+
+fin_tcstart:
+	return ret;
+}
+
+
+int8_t vmb_tc_stop(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_STOP;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+#if 1
+	gic_trigger(cpu);
+        //gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+        gic_send_ipi(VMB_BLW_IPI3);
+#endif
+        mips_ihb(); 
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcstop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcstop:
+	vmb_tc_free(cpu, tc_num);
+	return ret;
+}
+
+
+int8_t vmb_tc_pause(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_PAUSE;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+#if 1
+	gic_trigger(cpu);
+        //gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+        gic_send_ipi(VMB_BLW_IPI3);
+#endif
+        mips_ihb(); 
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU could not be Paused !!!.... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcpause;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcpause:
+	return ret;
+}
+
+
+int8_t vmb_tc_resume(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_RESUME;
+	vmb_fw_msg_t->tc_num = tc_num;
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+#if 1
+	gic_trigger(cpu);
+        //gic_send_ipi(VMB_BLW_IPI0 + cpu);
+#else
+        gic_send_ipi(VMB_BLW_IPI3);
+#endif
+        mips_ihb(); 
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU could not be Resumed !!!.... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcres;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk ("CPU could not be Resumed !!!.... \n");
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcres:
+	return ret;
+}
+
+int amon_cpu_avail (int cpu) {
+	return 1;
+}
+
+/* Need this function to check if a CPU needs to run Linux or not. 
+Linux traverses the NR_CPU sequentially to bring the CPU up but will have
+issues when we need to run Linux on Core0/VPE0 (CPU 0) and Core1/VPE0 (CPU 2).
+We cannot set NR_CPU as 2 as again it means CPU0 and CPU1 and is referenced as mentioned
+in whole kernel . So we cannot set bit 2 as CPU3 instead set NR_CPUS=4 and selectively start
+the Linux based on DT */
+
+int is_linux_OS_vmb (int cpu) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret = 0;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if (!strncmp(vpet->name, "LINUX" , sizeof(vpet->name)))
+		ret = 1;
+
+	return ret;
+}
+
+int32_t vmb_yr_get(uint8_t cpu, uint16_t num_yr) {
+        int32_t ret = -VMB_ERROR;
+        int i, c_id, n_yr;
+        VMB_core_t *coret = core_t;
+	VMB_yr_t *yri;
+	int32_t yield_bits = 0x0;
+
+        c_id = which_core(cpu);
+	yri = coret[c_id].yr_t;
+	n_yr = 0;
+
+	/* 
+	  Make sure requested # of Yield resources are availablei before proceeding to allocate.
+	  if not return -EBUSY.
+	*/
+        for (i=0; i < MAX_YIELD_INTF; i++)
+                if ((yri[i].yr_status & YR_INACTIVE) == YR_INACTIVE)
+                        n_yr++;
+
+	if (n_yr < num_yr) {
+		ret = -VMB_EBUSY;
+		goto yralloc_failed; 	 
+	}
+
+	n_yr = 0;
+	for (i=0; i < MAX_YIELD_INTF; i++) {
+		if ((yri[i].yr_status & YR_INACTIVE) == YR_INACTIVE) {
+			if (n_yr == num_yr)
+				break;
+
+			yri[i].yr_status &= ~YR_INACTIVE;
+			yri[i].yr_status |= YR_ACTIVE;
+			yield_bits |= 0x1 << i;
+			yri[i].cpu_id = cpu; /* Add the CPU to which it belongs */
+			n_yr++;
+			ret = yield_bits;
+			printk ("[%s]:[%d] CPU = %d, yri[i].yr_status = %x, yield_bits = %x n_yr = %d yri[i].cpu_id = %d \n", __FUNCTION__, __LINE__, cpu, yri[i].yr_status, yield_bits, n_yr, yri[i].cpu_id);
+		}
+	}
+
+yralloc_failed: 
+	return ret;
+}
+
+void vmb_yr_free (uint8_t cpu, int16_t yr) {
+        int i, c_id;
+        VMB_core_t *coret = core_t;
+	VMB_yr_t *yri;
+	int16_t n_yr;
+
+        c_id = which_core(cpu);
+	yri = coret[c_id].yr_t;
+	i = 0;
+
+	if (yr == -1) {
+		for (i=0; i < MAX_YIELD_INTF; i++) {
+			if (yri[i].cpu_id == cpu) {
+				yri[i].yr_status &= ~YR_ACTIVE;
+				yri[i].yr_status |= YR_INACTIVE;
+				yri[i].cpu_id = -1;	
+			}
+		}
+	} else {
+		do 
+		{
+			n_yr = (yr >> i) & 0x1;
+			if (n_yr) {
+				if (yri[i].cpu_id == cpu) {
+					yri[i].yr_status &= ~YR_ACTIVE;
+					yri[i].yr_status |= YR_INACTIVE;
+					yri[i].cpu_id = -1;
+				} else {
+					printk (KERN_ERR "[%s]:[%d] Illegal freeing of Yield ID %d as this is not allocated to CPU%d\n ", __FUNCTION__, __LINE__, i, cpu);
+				}
+			}
+			i++;
+		} while (i < MAX_YIELD_INTF);
+	}
+
+	return;
+}
+
+int32_t vmb_itc_sem_get(uint8_t cpu, uint8_t tc_num) {
+        int32_t ret = -VMB_EBUSY;
+        int i;
+
+	for (i=0; i < MAX_ITC_SEMID; i++) {
+		if((itc_t[i].itc_status & ITC_INACTIVE) == ITC_INACTIVE) {
+			ret = i;
+                        itc_t[i].itc_status &= ~ITC_INACTIVE;
+                        itc_t[i].itc_status |= ITC_ACTIVE;
+                        itc_t[i].cpu_id = cpu; /* Add the CPU to which it belongs */
+			itc_t[i].tc_id = tc_num;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+void vmb_itc_sem_free_pcpu (int8_t cpu) {
+	int i;
+	
+	for (i=0; i < MAX_ITC_SEMID; i++) {
+		if((itc_t[i].cpu_id) == cpu) {
+			vmb_itc_sem_free(i);
+		}
+	}
+
+	return;
+}
+
+void vmb_itc_sem_free (int8_t semID) {
+	
+	itc_t[semID].itc_status &= ~ITC_ACTIVE;
+	itc_t[semID].itc_status |= ITC_INACTIVE;
+	itc_t[semID].cpu_id = -1;
+	itc_t[semID].tc_id = -1;
+	
+	return;
+}
+
+int32_t fw_vmb_get_irq(uint8_t cpu) {
+	int32_t ret = -VMB_EAVAIL;
+
+	switch (cpu){
+		case 1 :
+			ret = (FW_VMB_IPI1 - MIPS_GIC_IRQ_BASE);
+			break;
+		case 2 :
+			ret = (FW_VMB_IPI2 - MIPS_GIC_IRQ_BASE);
+			break;
+		case 3 :
+			ret = (FW_VMB_IPI3 - MIPS_GIC_IRQ_BASE);
+			break;
+		default :
+			printk ("[%s]:[%d] Invalid CPU number !!!\n", __FUNCTION__, __LINE__);
+	}
+
+	printk ("[%s]:[%d] CPU = %d  IRQ = %d !!!\n", __FUNCTION__, __LINE__, cpu , ret);
+
+	return ret;
+}
+
+
+EXPORT_SYMBOL(vmb_cpu_alloc);
+EXPORT_SYMBOL(vmb_cpu_start);
+EXPORT_SYMBOL(vmb_cpu_free);
+EXPORT_SYMBOL(vmb_cpu_stop);
+EXPORT_SYMBOL(vmb_register_callback);
+
+EXPORT_SYMBOL(vmb_tc_stop);
+EXPORT_SYMBOL(vmb_tc_start);
+EXPORT_SYMBOL(vmb_tc_alloc);
+EXPORT_SYMBOL(vmb_tc_free);
+EXPORT_SYMBOL(vmb_tc_pause);
+EXPORT_SYMBOL(vmb_tc_resume);
+EXPORT_SYMBOL(vmb_get_vpeid);
+EXPORT_SYMBOL(VMB_get_msg_addr);
+EXPORT_SYMBOL(vmb_yr_get);
+EXPORT_SYMBOL(vmb_yr_free);
+EXPORT_SYMBOL(vmb_itc_sem_get);
+EXPORT_SYMBOL(vmb_itc_sem_free);
+EXPORT_SYMBOL(fw_vmb_get_irq);
diff --git a/arch/mips/mti-malta/Makefile b/arch/mips/mti-malta/Makefile
--- a/arch/mips/mti-malta/Makefile
+++ b/arch/mips/mti-malta/Makefile
@@ -12,6 +12,7 @@ obj-y				:= malta-amon.o malta-display.o
 obj-$(CONFIG_EARLY_PRINTK)	+= malta-console.o
 obj-$(CONFIG_PCI)		+= malta-pci.o
 obj-$(CONFIG_DTC)		+= malta-dt-setup.o
+obj-$(CONFIG_LTQ_VMB)		+=lantiq-vmb.o
 
 # FIXME FIXME FIXME
 obj-$(CONFIG_MIPS_MT_SMTC)	+= malta-smtc.o
diff --git a/arch/mips/mti-malta/dts/Malta.dtsi b/arch/mips/mti-malta/dts/Malta.dtsi
--- a/arch/mips/mti-malta/dts/Malta.dtsi
+++ b/arch/mips/mti-malta/dts/Malta.dtsi
@@ -6,15 +6,19 @@
 	cpus {
 		cpu@0 {
 			compatible = "mips,InterAptiv";
+			default-OS = "LINUX";
 		};
 		cpu@1 {
 			compatible = "mips,InterAptiv";
+			default-OS = "LINUX";
 		};
 		cpu@2 {
 			compatible = "mips,InterAptiv";
+			default-OS = "LINUX";
 		};
 		cpu@3 {
 			compatible = "mips,InterAptiv";
+			default-OS = "LINUX";
 		};
 	};
 };
diff --git a/arch/mips/mti-malta/lantiq-vmb.c b/arch/mips/mti-malta/lantiq-vmb.c
new file mode 100644
--- /dev/null
+++ b/arch/mips/mti-malta/lantiq-vmb.c
@@ -0,0 +1,1082 @@
+#include <linux/init.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/interrupt.h>
+#include <asm/cacheflush.h>
+#include <asm/smp-ops.h>
+#include <asm/traps.h>
+#include <asm/fw/fw.h>
+#include <asm/gcmpregs.h>
+#include <linux/of_platform.h>
+#include <linux/of_fdt.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sched.h>
+#include <linux/bootmem.h>
+#include <asm/ltq_vmb.h>
+
+#undef LINUX_SHM_DDR
+
+/* Number of cores*/
+VMB_core_t core_t[MAX_CORE];
+static DEFINE_SPINLOCK(vmb_lock);
+
+
+static struct proc_dir_entry *vmb_proc, *vmb_proc_dir ;
+extern void amon_cpu_start(int, unsigned long, unsigned long,
+                    unsigned long, unsigned long);
+
+void *VMB_get_msg_addr (int cpu, int direction);
+
+static irqreturn_t mpe_vmb_ipi1_hdlr(int irq, void *ptr) {
+        int i,j;
+        VMB_core_t *ct = core_t;
+	int cpu = -1, cid =0 , vid=0;
+	VMB_vpe_t *vpet_t;
+	FW_vmb_msg_t *fw_t ;
+
+	/* CPU number can also be found using ptr and passing the cpu to it */
+	/* Get the CPU number from the name as we know this handler is coupled with MPE */
+
+	spin_lock(&vmb_lock);
+	for (i=0; i < MAX_CORE; i++) {
+                VMB_vpe_t *vpet = ct[i].vpe_t;
+           for (j=0; j < MAX_VPE; j++) {
+		if (!strncmp(vpet[j].name, "MPEFW",strlen("MPEFW"))) {
+			cpu = get_cpu_id(i, j);
+			cid = i;
+			vid = j;
+			goto loop_out;
+		}
+	   }
+	}
+	spin_unlock(&vmb_lock);
+
+	if (cpu == -1) {
+		printk ("Interrupt Handler for IRQ %d is not for FW_NAME = %s .. \n", irq, "VOICEFW");
+		return 0;
+	}
+
+loop_out:
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/* May lead to deadlock in case of FW_RESET as spinlock for that VPE is taken and then wait_event_called thus 
+	   commenting the locks if this doesnt work then need to use  IBL_IN_ handler to wakeup this MPE handler*/
+
+	spin_lock(&vpet_t->vpe_lock);
+
+	/* W.r.t FW copy the DDR section to Internal DB */
+        memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+        smp_rmb();
+
+	if(vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+		/* Be cautious : TEST Throughly, VPE will go for a reset so it will be in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res_ext;	
+
+	} else if(vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vpet_t->bl_status = IBL_ACTIVE;
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+	}
+
+       /* Clear DDR section */
+       //  memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+
+        /* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+        wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res_ext:
+        /* unlock */
+        spin_unlock(&vpet_t->vpe_lock);
+
+	return 0;	
+}
+
+static irqreturn_t vce_vmb_ipi1_hdlr(int irq, void *ptr) {
+        int i,j;
+        VMB_core_t *ct = core_t;
+	int cpu = -1, cid =0 , vid=0;
+	VMB_vpe_t *vpet_t;
+	FW_vmb_msg_t *fw_t ;
+
+	/* CPU number can also be found using ptr and passing the cpu to it */
+	/* Get the CPU number from the name as we know this handler is coupled with MPE */
+
+	spin_lock(&vmb_lock);
+	for (i=0; i < MAX_CORE; i++) {
+                VMB_vpe_t *vpet = ct[i].vpe_t;
+           for (j=0; j < MAX_VPE; j++) {
+		if (!strncmp(vpet[j].name, "VOICEFW",strlen("VOICEFW"))) {
+			cpu = get_cpu_id(i, j);
+			cid = i;
+			vid = j;
+			goto loop_out;
+		}
+	   }
+	}
+	spin_unlock(&vmb_lock);
+
+	if (cpu == -1) {
+		printk ("Interrupt Handler for IRQ %d is not for FW_NAME = %s .. \n", irq, "VOICEFW");
+		return 0;
+	}
+
+loop_out:
+	vpet_t = &core_t[cid].vpe_t[vid];
+	fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+
+	/* May lead to deadlock in case of FW_RESET as spinlock for that VPE is taken and then wait_event_called thus 
+	   commenting the locks if this doesnt work then need to use  IBL_IN_ handler to wakeup this MPE handler*/
+
+	spin_lock(&vpet_t->vpe_lock);
+
+	/* W.r.t FW copy the DDR section to Internal DB */
+        memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+        smp_rmb();
+
+	if(vpet_t->fw_vmb.status == FW_RESET) {
+		vmb_cpu_free(cpu);
+		
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+		if (vpet_t->vmb_callback_fn && vpet_t->vmb_callback_fn != NULL)
+			vpet_t->vmb_callback_fn(vpet_t->fw_vmb.status);
+
+		/* Be cautious : TEST Throughly, VPE will go for a reset so it will be in deactivated STATE*/
+		vpet_t->bl_status = IBL_INACTIVE;
+
+		goto res_ext;	
+
+	} else if(vpet_t->fw_vmb.status == IBL_IN_WAIT) {
+		vmb_cpu_free(cpu);
+		vpet_t->bl_status = IBL_ACTIVE;
+
+		printk("[%s]:[%d], cpu = %d \n", __FUNCTION__, __LINE__, cpu);
+	}
+
+       /* Clear DDR section */
+       //  memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+
+        /* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+        wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+res_ext:
+        /* unlock */
+        spin_unlock(&vpet_t->vpe_lock);
+
+	return 0;	
+}
+
+
+static irqreturn_t bl_vmb_ipi1_hdlr(int irq, void *ptr) {
+
+	/* Orginating CPU unknown !!! */
+
+	return 0;	
+}
+
+/* Update a dummy vmb_msg_t->status in DB as this is for Linux SMP which doesn't update DDR in Linux --> VMB direction*/
+static int linux_cpu_ipi_update(unsigned long cpu, unsigned long action) {
+	unsigned long flags;
+	int c_id = which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet_t = &core_t[c_id].vpe_t[v_id];
+
+#ifdef LINUX_SHM_DDR
+	FW_vmb_msg_t *fw_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);	
+	
+	/* W.r.t FW copy the DDR section to Internal DB */
+	memcpy(&vpet_t->fw_vmb, fw_t, sizeof(FW_vmb_msg_t));
+	smp_rmb();
+
+	/* Clear DDR section */
+	memset(fw_t, 0, sizeof(FW_vmb_msg_t));
+	
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#else
+	spin_lock_irqsave(&vpet_t->vpe_lock, flags);	
+
+	vpet_t->fw_vmb.status= (uint32_t)FW_VMB_ACK;
+	vpet_t->fw_vmb.priv_info = 0;
+
+	/* Set the wakeup_vpe and wakeup the waitqueue */
+	vpet_t->v_wq.wakeup_vpe = 1;
+	wake_up_interruptible(&vpet_t->v_wq.vmb_wq);
+
+	/* unlock */
+	spin_unlock_irqrestore(&vpet_t->vpe_lock, flags);
+#endif
+	return 0;
+}
+
+static int __cpuinit vmb_cpu_active(struct notifier_block *nfb,
+                                      unsigned long action, void *hcpu)
+{
+        switch (action) {
+        case CPU_STARTING:
+                printk ("[%s]:[%d] action = %ld , cpu = %ld\n", __FUNCTION__, __LINE__, action, (long)hcpu);
+		/* Come here on LINUX secondary is up . Get the DDR and update DB on the cpu_status, boot_flag and ACK and NACK and 
+		   wakeup the waitqueue in vmb_cpu_start() */
+		linux_cpu_ipi_update((unsigned long)hcpu, action);
+                return NOTIFY_OK;
+        default:
+                return NOTIFY_DONE;
+        }
+
+	return 0;
+}
+
+/* API to get the memory in DDR for strutures VMB_fw_msg_t/FW_vmb_msg_t */
+void *VMB_get_msg_addr (int cpu, int direction) {
+#ifdef CONFIG_EVA
+	void *msg_t =  (void *)(CPU_LAUNCH);
+#else
+	void *msg_t =  (void *)CKSEG0ADDR(CPU_LAUNCH);
+#endif
+
+	/* VMB --> FW : VMB_fw_msg_t structure */
+	if (direction == 0)
+		msg_t = msg_t + (vmb_msg_size * cpu) ;
+	else
+		msg_t = msg_t + (vmb_msg_size * cpu) + sizeof(VMB_fw_msg_t);
+
+	return (msg_t);
+}
+
+/* Lock --> Access and Copy DDR --> check for status and update Internal DB */
+static void vmb_check_IBL_fw_msg(void) {
+	int i,j;
+	VMB_core_t *ct = core_t;
+	FW_vmb_msg_t *fw_msg_t;
+
+       for (i=0; i < MAX_CORE; i++) {
+		VMB_vpe_t *vpet = ct[i].vpe_t;
+		int cpu;
+           for (j=0; j < MAX_VPE; j++) {
+			if ((i == 0) && (j == 0))
+				continue;
+
+                	spin_lock(&vpet[j].vpe_lock);
+			cpu = get_cpu_id(i, j);
+                	printk ("[%s]:[%d] vpet[j].bl_status = %d , cpu = %d\n", __FUNCTION__, __LINE__, vpet[j].bl_status, cpu);
+			fw_msg_t = (FW_vmb_msg_t *)VMB_get_msg_addr(cpu, 1);
+			memcpy(&vpet[j].fw_vmb, fw_msg_t, sizeof(FW_vmb_msg_t));
+			if (vpet[j].fw_vmb.status == IBL_IN_WAIT)
+				vpet[j].bl_status = IBL_ACTIVE;
+			else
+				vpet[j].bl_status = IBL_INACTIVE;
+                	spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+}
+
+static int update_DB_from_DT(VMB_vpe_t *vt) {
+	struct device_node *np;
+	char str1[16], *name;
+	int ret;
+
+	memset(str1, '\0', sizeof(str1));
+	sprintf(str1,"%s%d","/cpus/cpu@",vt->cpu_id);
+
+	printk("[%s]:[%d], str1 = %s , cpuid = %d\n", __FUNCTION__, __LINE__, str1, vt->cpu_id);
+	np = of_find_node_by_path(str1);
+	if (!np)
+                return -ENODEV;
+
+	ret = of_property_read_string_index(np, "default-OS", 0, (const char **)&name);
+      	if (ret < 0 && ret != -EINVAL) {
+		printk ("ERROR : Property could be read from DT \n");
+                return ret;
+        }
+
+	strncpy(vt->name, name, sizeof(vt->name));
+	printk("[%s]:[%d], name = %s \n", __FUNCTION__, __LINE__, vt->name);
+	vt->cpu_status |= CPU_BOOTUP_DT;
+
+	return 0;
+}
+
+static void initialise_vmb_DB(void) {
+	VMB_core_t *coret = core_t;
+	int i,j;
+
+	coret[0].active |= CORE_ACTIVE;
+
+	for (i=0; i < MAX_CORE; i++) {
+		VMB_tc_t *tct = coret[i].tc_t;
+		VMB_vpe_t *vpet = coret[i].vpe_t;
+		for (j=0; j < MAX_VPE; j++) {
+			vpet[j].core_id = (((i * 2) + j)/2);
+			spin_lock_init(&vpet[j].vpe_lock);
+			printk ("MAX_VPE j = %d \n", j);
+			printk (" j = %d , vmb_wq = %p \n", j, &vpet[j].v_wq.vmb_wq);
+			init_waitqueue_head(&vpet[j].v_wq.vmb_wq);
+			vpet[j].v_wq.wakeup_vpe = 0;
+			vpet[j].cpu_id = ((i * 2) + j);
+			if ((i == 0) && (j == 0)) { 
+				/* Core0/VPE0 always active*/
+				vpet[j].bl_status = IBL_ACTIVE;
+				vpet[j].cpu_status = CPU_ACTIVE;
+			} else {
+				vpet[j].bl_status = IBL_INACTIVE;
+				vpet[j].cpu_status = CPU_INACTIVE;
+			}
+			tct[j].vpe_id = j;
+			/* Get the information from DT for the FW names  also update the bootflag for cpu */
+			update_DB_from_DT(&vpet[j]);
+		}
+
+		for (j=0; j < MAX_TC; j++) {
+			if (j >= 2) {
+				tct[j].vpe_id = MAX_VPE;
+				tct[j].tc_status = TC_INACTIVE;
+			} else {
+				tct[j].vpe_id = j;
+				tct[j].tc_status = TC_ACTIVE;
+			}	
+		}
+	}
+}
+
+
+static void initialise_vmb_IRQhdlr(void) {
+	int err = 0;
+
+	err = request_irq(MPEFW_VMB_IPI1, mpe_vmb_ipi1_hdlr, IRQF_DISABLED, "mpe_vmb_ipi1", NULL);
+	if (err)
+		printk ("request_irq for IRQ MPEFW_VMB_IPI1 = %d failed !!! \n", MPEFW_VMB_IPI1);
+
+	err = request_irq(VCEFW_VMB_IPI1, vce_vmb_ipi1_hdlr, IRQF_DISABLED, "vce_vmb_ipi1", NULL);
+	if (err)
+		printk ("request_irq for IRQ VCEFW_VMB_IPI1 = %d failed !!! \n", VCEFW_VMB_IPI1);
+
+	err = request_irq(BLW_VMB_IPI1, bl_vmb_ipi1_hdlr, IRQF_DISABLED, "bl_vmb_ipi1", NULL);
+	if (err)
+		printk ("request_irq for IRQ BLW_VMB_IPI1 = %d failed !!! \n", BLW_VMB_IPI1);
+}
+
+/* Dump the tree */
+static int dump_vmb_tree(struct seq_file *s, void *v) {
+	VMB_core_t *coret = core_t;
+	int i,j;
+
+        for (i=0; i < MAX_CORE; i++){
+                //VMB_tc_t *tct = coret[i].tc_t;
+                VMB_vpe_t *vpet = coret[i].vpe_t;
+
+		for (j=0; j < MAX_VPE; j++) {
+		spin_lock(&vpet[j].vpe_lock);
+			printk ("CORE ID %d \n", i);
+			printk ("\t - VPE ID %d \n", j);
+			printk ("\t \t - OS Name = %s \n", vpet[j].name);
+			printk ("\t \t - InterAptiv-BL status = %s \n", (vpet[j].bl_status == IBL_ACTIVE ? "ACTIVE" : "INACTIVE"));
+			printk ("\t \t - CPU status = %s \n", ((vpet[j].cpu_status & CPU_ACTIVE) == CPU_ACTIVE ? "ACTIVE" : "INACTIVE")); 
+			printk ("\t \t - Core ID = %d \n", vpet[j].core_id);
+			printk ("\t \t - CPU ID = %d \n", vpet[j].cpu_id);
+			printk ("\t \t - FW_VMB_MSG Structure \n");
+			printk ("\t \t \t - status = 0X%x\n", vpet[j].fw_vmb.status);
+			printk ("\t \t \t - priv_info = 0X%x\n", vpet[j].fw_vmb.priv_info);
+				
+		spin_unlock(&vpet[j].vpe_lock);
+		}
+	}
+
+	return 0;
+}
+
+static int vmb_proc_open(struct inode *inode, struct file *file)
+{
+ return single_open(file, dump_vmb_tree , NULL);
+}
+
+static const struct file_operations vmb_proc_fops = {
+ .open           = vmb_proc_open,
+ .read           = seq_read,
+ .llseek         = seq_lseek,
+ .release        = single_release,
+};
+
+/* Initialise VMB structures and IRQ */
+int8_t vmb_init(void) 
+{
+	memset(core_t, 0 , sizeof(VMB_core_t));
+
+	printk ("size of core = %d, MAXCORE = %d , MAXCPU = %d, MAXTCS = %d MAX_VPE = %d\n", sizeof(core_t), MAX_CORE, MAX_CPU, MAX_TC, MAX_VPE);
+
+	/* CPU notifier for Linux SMP bootup indication notify (CPU_STARTING) from start_secondary */
+	cpu_notifier(vmb_cpu_active, CPU_PRI_SCHED_ACTIVE);
+
+	/* Initialise a New core before vmbDB init as it will get time for dumping IBL_INWAIT in DDR 
+           which can be picked by vmb_check_IBL() */
+
+
+	/* Basic initialisation of the DB */
+	initialise_vmb_DB();
+
+	/* register IRQ handlers for VMB_FW_IRQx */
+	initialise_vmb_IRQhdlr();
+	
+	/* Initialise the /proc/vm/debug to dump the Internal DB */
+	vmb_proc_dir = proc_mkdir("vmb", NULL);
+	vmb_proc = proc_create("status", 0644, vmb_proc_dir, &vmb_proc_fops);
+
+	/* API to update th IBL status if the IBL_IN_WAIT IPI is missed during initialisation. 
+	   Mainly a fail-safe Linux running VPE updates as there is no IPI mentioned to handle IBL_INWAIT for Linux */
+	vmb_check_IBL_fw_msg();
+
+	spin_lock_init(&vmb_lock);
+
+	return 0;
+}
+
+/* Allocate Free CPU from the pool */
+int8_t vmb_cpu_alloc(int8_t cpu, char* fw_name) {
+	int ret = -VMB_ERROR;
+
+/*CHECK : May be for alloc we need to have a global lock to avoid a scenario where 2 FWs call at the same time race condi 
+and getting same cpu number */
+
+	if (cpu == MAX_CPU) {
+        	VMB_core_t *coret = core_t;
+        	int i,j;
+
+        	for (i=0; i < MAX_CORE; i++) {
+                	VMB_vpe_t *vpet = coret[i].vpe_t;
+                	for (j=0; j < MAX_VPE; j++) {
+				if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+					if (strncmp(vpet[j].name, fw_name, sizeof(vpet[j].name)) != 0) {
+						ret=-VMB_EAVAIL;
+						continue;
+					}
+				}
+
+				if ((vpet[j].bl_status == IBL_ACTIVE) && ((vpet[j].cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+					ret = get_cpu_id(i, j);
+					vpet[j].cpu_status &= ~CPU_INACTIVE;
+					vpet[j].cpu_status |= CPU_ACTIVE;
+					printk ("[%s]:[%d] CPU vpet[j].cpu_status = %x \n", __FUNCTION__, __LINE__, vpet[j].cpu_status);
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+					if ((vpet[j].cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+						vpet[j].cpu_status &= ~CPU_BOOTUP_DT;
+					else
+						strncpy(vpet[j].name, fw_name, sizeof(vpet[j].name));
+#endif
+					goto fin_alloc;
+				} else 
+					ret = -VMB_EBUSY;
+			}
+		}
+	} else {
+        	int c_id= which_core(cpu);
+	        int v_id = vpe_in_core(cpu);
+        	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+		
+		if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT) {
+			if (strncmp(vpet->name, fw_name, sizeof(vpet->name)) != 0) {
+				printk("WARNING : As per DT Bootup Scenario %s shouldn't run on CPU %d \n", fw_name, cpu);
+				printk(" Please retry with MAX_CPUS !!! \n");
+				ret=-VMB_EAVAIL;
+				goto fin_alloc;
+			}
+		}
+		
+		if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_INACTIVE) == CPU_INACTIVE)) {
+			ret = cpu;
+			vpet->cpu_status &= ~CPU_INACTIVE;
+			vpet->cpu_status |= CPU_ACTIVE;
+			printk ("[%s]:[%d] CPU vpet.cpu_status = %x \n", __FUNCTION__, __LINE__, vpet->cpu_status);
+#ifdef CONFIG_LTQ_DYN_CPU_ALLOC
+			if ((vpet->cpu_status & CPU_BOOTUP_DT) == CPU_BOOTUP_DT)
+				vpet->cpu_status &= ~CPU_BOOTUP_DT;
+			else
+				strncpy(vpet->name, fw_name, sizeof(vpet->name));
+#endif
+		}
+		else 
+			ret = -VMB_EBUSY;
+	}
+
+fin_alloc: 
+	return ret;
+}
+
+void vmb_register_callback (uint8_t cpu, vmb_callback_func func) {
+	int c_id= which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->vmb_callback_fn = func;
+}
+
+/* Mark the VPE as free (INACTIVE) */
+int8_t vmb_cpu_free(int8_t cpu) {
+	int c_id= which_core(cpu);
+	int v_id = vpe_in_core(cpu);
+	VMB_vpe_t *vpet = &core_t[c_id].vpe_t[v_id];
+
+	if (vpet->bl_status != IBL_ACTIVE)
+		printk("WARNING : IBL is not active for CPU = %d !!! \n", cpu);
+
+	vpet->cpu_status &= ~CPU_ACTIVE;
+	vpet->cpu_status |= CPU_INACTIVE;
+
+	return VMB_SUCCESS;
+}
+
+int8_t vmb_cpu_force_stop(int8_t cpu) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	vpet->bl_status &= ~IBL_ACTIVE;
+	vpet->bl_status |= IBL_INACTIVE;
+
+	/* Generate a NMI for that VPE*/
+	
+	/* TODO ????????????*/
+
+	/* Wait for IBL_IN_WAIT or timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("Consider reseting the CPU using vmb_cpu_force_stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+
+		ret = -VMB_ETIMEOUT;	
+		goto fin_fstop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)IBL_IN_WAIT) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU using vmb_cpu_force_stop \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_fstop:
+	return ret;
+}
+
+int8_t vmb_cpu_stop(int8_t cpu) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+		memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+		vmb_fw_msg_t->msg_id = VMB_CPU_STOP;
+	} else {
+		ret = -VMB_ERROR;
+		spin_unlock(&vpet->vpe_lock);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		goto fin_stop;
+	}
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;	
+		goto fin_stop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin_stop:
+	return ret;
+}
+
+int8_t vmb_cpu_start(int8_t cpu, char* fw_name, CPU_launch_t cpu_launch, TC_launch_t tc_launch[], uint8_t num_tcs ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)&cpu_launch == NULL)
+		return -VMB_ENOPRM;
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	strncpy(vpet->name, fw_name, sizeof(vpet->name));
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_CPU_START;
+	memcpy(&vmb_fw_msg_t->cpu_launch, &cpu_launch, sizeof(vmb_fw_msg_t->cpu_launch));
+	if ((void *)tc_launch != NULL) {
+		for (i=0; i < (sizeof(tc_launch)/sizeof(tc_launch[0])); i++)
+			memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i], sizeof(vmb_fw_msg_t->tc_launch[i]));
+	}
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+#ifdef CONFIG_MIPS_MALTA
+	/* Linux bootup in MALTA*/
+	amon_cpu_start(cpu, cpu_launch.start_addr, cpu_launch.sp, cpu_launch.gp, cpu_launch.a0);
+#endif
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+		ret = -VMB_ETIMEOUT;	
+		goto fin;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_cpu_free(cpu);
+		vmb_tc_free(cpu, -1);
+		vmb_cpu_force_stop(cpu);
+	}
+
+fin:
+	return ret;
+}
+
+/********************************** VMB TC APIs **************************/
+
+int8_t vmb_tc_alloc (uint8_t cpu) {
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_vpe_t *vpet;
+	
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE)) {
+		VMB_tc_t *tct = coret[c_id].tc_t;
+		for (i=2; i < MAX_TC; i++) {
+			if ((tct[i].tc_status & TC_INACTIVE) == TC_INACTIVE) {
+				tct[i].tc_status &= ~TC_INACTIVE;
+				tct[i].tc_status |= TC_ACTIVE;
+				ret = i;
+				tct[i].vpe_id = v_id;
+				printk ("[%s]:[%d] CPU tct[i].tc_status = %x \n", __FUNCTION__, __LINE__, tct[i].tc_status);
+				goto fin_talloc;
+			} else 
+				ret = -VMB_EBUSY;
+		}
+	}
+fin_talloc:
+		return ret;
+}
+
+
+int8_t vmb_tc_free(int8_t cpu, int8_t tc_num) {
+	int ret = -VMB_ERROR;
+	int i, c_id, v_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_vpe_t *vpet;
+	
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+	if ((vpet->bl_status == IBL_ACTIVE) && ((vpet->cpu_status & CPU_ACTIVE) == CPU_ACTIVE))
+		printk ("VPE %d is not active !!! still freeing the TC %d \n", cpu, tc_num);
+
+	if (tc_num == -1) {
+		VMB_tc_t *tct = coret[c_id].tc_t;
+		for (i=2; i < MAX_TC; i++) {
+			ret = get_cpu_id (c_id, tct[i].vpe_id);
+			if (ret == cpu) {
+				tct[i].tc_status &= ~TC_ACTIVE;
+				tct[i].tc_status |= TC_INACTIVE;
+			}
+		}
+		ret = VMB_SUCCESS;
+	} else {
+		VMB_tc_t *tct = &coret[c_id].tc_t[tc_num];
+		ret = get_cpu_id (c_id, tct->vpe_id);
+
+		if (ret == cpu) {
+			tct->tc_status &= ~TC_ACTIVE;
+			tct->tc_status |= TC_INACTIVE;
+			ret = VMB_SUCCESS;
+		} else {
+			printk (" TC %d is not attached to CPU %d . Please use vmb_get_vpeid(cpu, tc_num) to get CPU to which active TC is attached \n", tc_num, cpu);
+			ret = -VMB_EAVAIL;
+		}
+	}
+	return ret;
+}
+
+int8_t vmb_get_vpeid (uint8_t cpu, uint8_t tc_num) {
+	int ret = -VMB_ERROR;
+	int c_id;
+ 	VMB_core_t *coret = core_t;
+	VMB_tc_t *tct;
+	
+	c_id = which_core(cpu);
+	tct = &coret[c_id].tc_t[tc_num];
+	ret = get_cpu_id (c_id, tct->vpe_id);
+
+	return ret;
+}
+
+int8_t vmb_tc_start(uint8_t cpu, TC_launch_t tc_launch[], uint8_t num_tcs ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1, i;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+/* After getting the cpu then we may use Per-VPE locks */
+
+	if ((void *)tc_launch == NULL || ((sizeof(tc_launch)/sizeof(tc_launch[0])) != num_tcs))
+		return -VMB_ENOPRM;
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_START;
+
+	for (i=0; i < (sizeof(tc_launch)/sizeof(tc_launch[0])); i++)
+		memcpy(&vmb_fw_msg_t->tc_launch[i], &tc_launch[i], sizeof(vmb_fw_msg_t->tc_launch[i]));
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		vmb_tc_free(cpu, -1);
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcstart;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+		vmb_tc_free(cpu, -1);
+	}
+
+fin_tcstart:
+	return ret;
+}
+
+
+int8_t vmb_tc_stop(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+
+	printk ("[%s]:[%d]  vmb_fw_msg_t = %p cpu = %d vmb_msg_size = %x sizeof(VMB_fw_msg_t) = %d sizeof(FW_vmb_msg_t) = %d !!!! \n ", __FUNCTION__, __LINE__, vmb_fw_msg_t, cpu, vmb_msg_size, sizeof(VMB_fw_msg_t), sizeof(FW_vmb_msg_t));
+
+	vmb_fw_msg_t->msg_id = VMB_TC_STOP;
+	memcpy(&vmb_fw_msg_t->tc_num, &tc_num, sizeof(vmb_fw_msg_t->tc_num));
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU not responding so going for a force cpu stop .... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcstop;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcstop:
+	vmb_tc_free(cpu, tc_num);
+	return ret;
+}
+
+
+int8_t vmb_tc_pause(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_PAUSE;
+	memcpy(&vmb_fw_msg_t->tc_num, &tc_num, sizeof(vmb_fw_msg_t->tc_num));
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU could not be Paused !!!.... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcpause;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcpause:
+	return ret;
+}
+
+
+int8_t vmb_tc_resume(uint8_t cpu, uint8_t tc_num ) {
+	int c_id, v_id;
+	VMB_vpe_t *vpet;
+	int ret, ret1;
+	VMB_fw_msg_t *vmb_fw_msg_t;
+
+/* CHECK :  GLOBAL LOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+	c_id = which_core(cpu);
+	v_id = vpe_in_core(cpu);
+	vpet = &core_t[c_id].vpe_t[v_id];
+
+/* CHECK : GLOBAL UNLOCK needed as "cpu" can for a simutaneous vmb_cpu_start from 2 FWs */
+
+
+/* Per-VPE Lock and update the vmb_fw_msg_t struct and update DDR */
+
+	spin_lock(&vpet->vpe_lock);
+
+	vmb_fw_msg_t = (VMB_fw_msg_t *)VMB_get_msg_addr(cpu, 0);
+	memset (vmb_fw_msg_t, 0 , sizeof(VMB_fw_msg_t));
+	vmb_fw_msg_t->msg_id = VMB_TC_RESUME;
+	memcpy(&vmb_fw_msg_t->tc_num, &tc_num, sizeof(vmb_fw_msg_t->tc_num));
+
+	spin_unlock(&vpet->vpe_lock);
+
+	/* Generate an IPI */
+
+	/* TODO ???? */
+
+	printk ("[%s]:[%d] WAITING FOR RESPONSE vpet->v_wq.wakeup_vpe = %d !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe);
+
+	/* Wait for timeout */
+	ret1 = wait_event_interruptible_timeout(vpet->v_wq.vmb_wq, 
+					(vpet->v_wq.wakeup_vpe == 1), QUEUE_TIMEOUT);
+	if (ret1 <= 0 ) {
+		printk ("[%s]:[%d] wait_event timeout occured for CPU = %d starting FW = %s \n", __FUNCTION__, __LINE__, cpu, vpet->name);
+		printk ("CPU could not be Resumed !!!.... \n");
+		vpet->v_wq.wakeup_vpe = 0;
+		ret = -VMB_ETIMEOUT;	
+		goto fin_tcres;	
+	}
+
+	printk ("[%s]:[%d] OUTSIDE WAIT wakeup_vpe = %d vpet->fw_vmb.status = %d ACK = %d cpu = %d ret1 = %d vpet = %p, smp_processor_id() = %d , vpet->fw_vmb =%p !!!! \n ", __FUNCTION__, __LINE__, vpet->v_wq.wakeup_vpe, vpet->fw_vmb.status, FW_VMB_ACK , cpu, ret1 , vpet, smp_processor_id(), &vpet->fw_vmb);
+
+	vpet->v_wq.wakeup_vpe = 0;
+
+	if (vpet->fw_vmb.status == (uint32_t)FW_VMB_ACK) {
+		ret = VMB_SUCCESS;
+		printk (KERN_ERR "[%s]:[%d]  ret = %d \n ", __FUNCTION__, __LINE__, ret);
+	} else {
+		ret = -VMB_ENACK;
+		printk ("CPU could not be Resumed !!!.... \n");
+		printk (KERN_ERR "[%s]:[%d] -ENACK recieved from FW .. Consider reseting the CPU \n ", __FUNCTION__, __LINE__);
+	}
+
+fin_tcres:
+	return ret;
+}
diff --git a/kernel/smp.c b/kernel/smp.c
--- a/kernel/smp.c
+++ b/kernel/smp.c
@@ -535,6 +535,10 @@ void __init setup_nr_cpu_ids(void)
 	nr_cpu_ids = find_last_bit(cpumask_bits(cpu_possible_mask),NR_CPUS) + 1;
 }
 
+#ifdef CONFIG_LTQ_VMB
+extern int is_linux_OS_vmb(int cpu);
+#endif
+
 /* Called by boot processor to activate the rest. */
 void __init smp_init(void)
 {
@@ -544,6 +548,12 @@ void __init smp_init(void)
 
 	/* FIXME: This should be done in userspace --RR */
 	for_each_present_cpu(cpu) {
+#ifdef CONFIG_LTQ_VMB
+		if (!is_linux_OS_vmb(cpu)) {
+			printk ("[%s]:[%d] CPU%d shouldn't run LINUX as per DT (model) config \n", __FUNCTION__, __LINE__, cpu);
+			continue;
+		}
+#endif
 		if (num_online_cpus() >= setup_max_cpus)
 			break;
 		if (!cpu_online(cpu))
