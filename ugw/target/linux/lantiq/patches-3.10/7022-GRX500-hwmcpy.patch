# HG changeset patch
# Parent 0b447063c03950aa33aac01d103d232417b2f5be

diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -358,4 +358,40 @@ config DMATEST
 	  Simple DMA test client. Say N unless you're debugging a
 	  DMA Device driver.
 
+config LTQ_HWMCPY
+	bool "Lantiq Hardware Memcopy Engine"
+	default n
+	depends on LANTIQ && SOC_GRX500
+	---help---
+	  Lantiq Hardware Memory Copy Engine.
+
+choice
+	prompt "LTQ UMT Version Select"
+    default LTQ_UMT_EXPAND_MODE if SOC_GRX500_A21
+	default LTQ_UMT_LEGACY_MODE
+
+config LTQ_UMT_LEGACY_MODE
+	bool "Lantiq UMT in A11 legacy mode"
+	depends on LTQ_HWMCPY
+	---help---
+	  Lantiq UMT Hardware support A11(legacy) mode
+	  and A21(expand) mode. Enable this option, UMT
+	  HW will be working on legacy mode.
+	  In legacy mode, UMT hardware only support one
+	  UMT port.
+
+config LTQ_UMT_EXPAND_MODE
+	bool "Lantiq UMT in A21 expand mode"
+	depends on LTQ_HWMCPY && SOC_GRX500_A21
+	---help---
+	  In expand mode, UMT HW is able to support max
+	  four UMT port. CBM port and DMA channels that
+	  associated with the UMT port can be configurable.
+	  Expand mode use dedicated OCP master to send out
+	  the UMT message instead of using DMA as in the
+	  legacy mode.
+
+endchoice
+
+
 endif
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -38,3 +38,6 @@ obj-$(CONFIG_DMA_SA11X0) += sa11x0-dma.o
 obj-$(CONFIG_MMP_TDMA) += mmp_tdma.o
 obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
+obj-$(CONFIG_LTQ_UMT_LEGACY_MODE) += ltq_umt_legacy.o
+obj-$(CONFIG_LTQ_UMT_EXPAND_MODE) += ltq_umt_expand.o
+obj-$(CONFIG_LTQ_HWMCPY) += ltq_hwmcpy.o
diff --git a/drivers/dma/ltq_hwmcpy.c b/drivers/dma/ltq_hwmcpy.c
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy.c
@@ -0,0 +1,1389 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ *
+ *  HISTORY
+ *  $Date		$Author	$Version               $Comment
+ *  01/02/2015	Zhu YiXin 1.0.0		 hwmemcpy driver
+ *  21/04/2015	Zhu YiXin 1.0.1   Add GCR workaround for UMT(A11 only)
+ *  15/07/2015	Zhu YiXin 1.1.0	optimized hwmcpy driver
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <lantiq_dmax.h>
+#include <asm/mipsmtregs.h>
+#include <asm/gic.h>
+#include <asm/ltq_vmb.h>
+
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+
+#include <net/datapath_proc_api.h>
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy_addr.h"
+#include "ltq_hwmcpy.h"
+
+#define MCPY_MIN_LEN		64
+#define YLD_PIN_ALLOC		1
+#define MCPY_DRV_VERSION	"1.1.0"
+
+u32 g_mcpy_dbg = MCPY_ERR | MCPY_INFO;
+void __iomem *g_mcpy_addr_base;
+static u32 g_mcpy_min_len = MCPY_MIN_LEN;
+
+static struct mcpy_ctrl ltq_mcpy_ctrl;
+static const char *g_trunksz[MCPY_TKSZ_MAX]
+	= {"512B", "1KB", "2KB", "4KB", "8KB", "16KB", "32KB", "64KB"};
+static const char * const hwmcpy_name[MCPY_PORTS_NUM] = {
+	"HWMCPY PORT 0",
+	"HWMCPY PORT 1",
+	"HWMCPY PORT 2",
+	"HWMCPY PORT 3",
+	"HWMCPY PORT 4",
+	"HWMCPY PORT 5",
+	"HWMCPY PORT 6",
+	"HWMCPY PORT 7",
+};
+static const struct mcpy_cfg mcpy_def_cfg[MCPY_PORTS_NUM] = {
+	/* Prio,  VPE id,  Enable */
+	{1,	0,	MCPY_PORT_ENABLED},
+	{1,	1,	MCPY_PORT_ENABLED},
+	{1,	2,	MCPY_PORT_ENABLED},
+	{1,	3,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+	{0,	-1,	MCPY_PORT_ENABLED},
+};
+
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value)	#name,
+char *g_mcpy_dbg_list[] = {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+
+/* #define HWMCPY_PROFILE_CYCLE_CNT */
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+extern void *CycleCounter_Create(char *);
+extern void CycleCounter_Start(void *);
+extern void CycleCounter_End(void *);
+static DEFINE_PER_CPU(void *, hwmcpy_cycles);
+static DEFINE_PER_CPU(void *, hw_cycles);
+static DEFINE_PER_CPU(void *, sw_cycles);
+
+static void mcpy_profile_cycle_init(void);
+#endif
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+static void mcpy_profile_cycle_init(void)
+{
+    int i;
+    char name[32];
+
+    for_each_online_cpu(i) {
+        sprintf(name, "hwmcpy%02d", i);
+        per_cpu(hwmcpy_cycles, i) = CycleCounter_Create(name);
+	sprintf(name, "hw%02d", i);
+        per_cpu(hw_cycles, i) = CycleCounter_Create(name);
+	sprintf(name, "sw%02d", i);
+        per_cpu(sw_cycles, i) = CycleCounter_Create(name);
+    }
+}
+#else
+#define mcpy_profile_cycle_init()
+#endif
+
+struct device *mcpy_get_dev(void)
+{
+	return ltq_mcpy_ctrl.dev;
+}
+EXPORT_SYMBOL(mcpy_get_dev);
+
+static const char *mcpy_get_name_by_pid(int pid)
+{
+	return hwmcpy_name[pid];
+}
+
+static void mcpy_sw_reset(void)
+{
+	int timeout = 10000;
+	ltq_mcpy_w32_mask(0x10, 0x10 , MCPY_GCTRL);
+	/* HW auto clean the reset bit */
+	while ((ltq_mcpy_r32(MCPY_GCTRL) & 0x10) == 1 && timeout >= 0)
+		timeout--;	/*wait reset to finish */
+
+	if (timeout < 0 && (ltq_mcpy_r32(MCPY_GCTRL) & 0x10) == 1) {
+		mcpy_dbg(MCPY_ERR, "MCPY soft reset fail!!\n");
+		panic("MCPY soft reset fail!\n");
+	}
+}
+
+static inline void mcpy_set_port_irq_intvl(u32 pid, u32 intvl)
+{
+	ltq_mcpy_w32(intvl, PORT_TO_CNT(pid));
+}
+
+static inline void mcpy_port_irq_enable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT((pid << 1) + 1), BIT((pid << 1) + 1), MCPY_INT_EN);
+}
+
+static inline void mcpy_port_irq_disable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT((pid << 1) + 1), 0, MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_enable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT(pid << 1), BIT(pid << 1), MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_disable(u32 pid)
+{
+	ltq_mcpy_w32_mask(BIT(pid << 1), 0, MCPY_INT_EN);
+}
+
+static inline void mcpy_port_yield_ack(u32 pid)
+{
+	ltq_mcpy_w32(BIT(pid << 1), MCPY_INT_STAT);
+}
+
+static inline void mcpy_port_irq_ack(u32 pid)
+{
+	ltq_mcpy_w32(0x3 << (pid << 1), MCPY_INT_STAT);
+}
+
+static inline void mcpy_irq_ack_mcpy_error(void)
+{
+	u32 err = ltq_mcpy_r32(MCPY_INT_STAT);
+
+	mcpy_dbg(MCPY_ERR, "MCPY_INT_STAT: 0x%x, CMD Err: %s, Len Err: %s\n",
+		err, (err & MCPY_CMD_ERR) ? "Yes" : "No",
+		(err & MCPY_LEN_ERR) ? "Yes" : "No");
+	ltq_mcpy_w32((MCPY_CMD_ERR | MCPY_LEN_ERR), MCPY_INT_STAT);
+}
+
+static inline int mcpy_inquire_irq_status(u32 pid)
+{
+	return ltq_mcpy_r32(MCPY_INT_STAT) & (BIT(pid*2) | BIT(pid*2+1));
+}
+
+static void mcpy_intr_setup(struct mcpy_port *pport)
+{
+	if (pport->irq_mode == MCPY_YLD_MODE) { /* Enable Yield */
+		mcpy_port_irq_disable(pport->pid);
+		mcpy_port_yield_enable(pport->pid);
+	} else {
+/* PRIO low -> Enable Interrupt, Note: Enable interrupt must Enable Yield */
+		mcpy_port_yield_enable(pport->pid);
+		mcpy_port_irq_enable(pport->pid);
+	}
+}
+static int mcpy_dma_init(struct mcpy_port *pport)
+{
+	u32 dma_rxch, dma_txch;
+
+	pport->chan.rch = MCPY_DMA_RX_CID + pport->pid * 2;
+	pport->chan.tch = MCPY_DMA_TX_CID + pport->pid * 2;
+	pport->chan.rch_dnum = 2;
+	pport->chan.tch_dnum = 2;
+	pport->chan.rch_dbase =
+		pport->ctrl->phybase + MCPY_DBASE
+		+ pport->pid * MCPY_DBASE_OFFSET;
+	pport->chan.tch_dbase
+		= pport->chan.rch_dbase + pport->chan.rch_dnum * DMA_DESC_SIZE;
+	pport->chan.onoff     = DMA_CH_ON;
+	sprintf(pport->chan.rch_name, "MCPY Port%d RXCH", pport->pid);
+	sprintf(pport->chan.tch_name, "MCPY Port%d TXCH", pport->pid);
+
+	dma_rxch = _DMA_C(pport->ctrl->dma_ctrl_id,
+			pport->ctrl->dma_port_id,
+			pport->chan.rch);
+	mcpy_dbg(MCPY_INIT, "dma_ch: 0x%x, rch_name:%s\n",
+			dma_rxch, pport->chan.rch_name);
+	if (ltq_request_dma(dma_rxch, pport->chan.rch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_rxch);
+		goto __MCPY_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_rxch,
+			pport->chan.rch_dbase, pport->chan.rch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_rxch);
+		goto __MCPY_PORT_FAIL;
+	}
+
+	dma_txch = _DMA_C(pport->ctrl->dma_ctrl_id,
+			pport->ctrl->dma_port_id,
+			pport->chan.tch);
+	mcpy_dbg(MCPY_INIT, "dma_ch: 0x%x, rch_name:%s\n",
+		    dma_txch, pport->chan.tch_name);
+	if (ltq_request_dma(dma_txch, pport->chan.tch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_txch);
+		goto __MCPY_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_txch,
+		    pport->chan.tch_dbase, pport->chan.tch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_txch);
+		goto __MCPY_PORT_FAIL;
+	}
+
+	if (pport->chan.onoff == DMA_CH_ON) {
+		ltq_dma_chan_on(dma_rxch);
+		ltq_dma_chan_on(dma_txch);
+	} else {
+		ltq_dma_chan_off(dma_rxch);
+		ltq_dma_chan_off(dma_txch);
+	}
+
+	return 0;
+
+__MCPY_PORT_FAIL: /* Disable Flag in MCPY means malfunction */
+	return -ENODEV;
+}
+
+static inline int mcpy_done(u32 pid)
+{
+	u32 resp;
+
+	resp = MCPY_GET_RESPONSE(pid);
+	if (unlikely(!(resp & MRES_DONE))) {
+		mcpy_dbg(MCPY_ERR,
+			"port: %d done bit not set after get interrupt\n",
+			pid);
+		return -1;
+	}
+
+	if (unlikely((resp & MRES_ERROR) != 0)) {
+		mcpy_dbg(MCPY_ERR, "MCPY[%d]: MCPY Error, resp: 0x%x!!!\n",
+			pid, resp);
+		return -1;
+	}
+	return 0;
+}
+
+/* This function should not be called */
+static irqreturn_t mcpy_yld_dummy_handler(int irq, void *dev_id)
+{
+	struct mcpy_port *pport = (struct mcpy_port *)dev_id;
+
+	mcpy_dbg(MCPY_ERR, "MCPY PID[%d]: Yield Dummy handler called!!!\n", pport->pid);
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t mcpy_irq_handler(int irq, void *dev_id)
+{
+	struct mcpy_port *pport;
+
+	pport = (struct mcpy_port *)dev_id;
+
+	/* mcpy_port_irq_disable(pport->pid); */
+	if (unlikely(mcpy_done(pport->pid) < 0))
+		mcpy_irq_ack_mcpy_error();
+
+	/*Clear status and Enable the interrupt */
+	mcpy_port_irq_ack(pport->pid);
+
+	/* Wake up the waiting process */
+	pport->cmd.status = MCPY_CMD_RELEASED;
+	barrier();
+	wake_up_interruptible(&pport->cmd.cmdq);
+
+	/* mcpy_port_irq_enable(pport->pid); */
+
+	return IRQ_HANDLED;
+}
+
+static inline void set_yield_mask(void *mask)
+{
+	u32 yqmask = read_c0_yqmask();
+
+	yqmask |= (*(u32 *)mask);
+	write_c0_yqmask(yqmask);
+}
+
+static inline void setup_percpu_yqmask(u32 mask, int cpu)
+{
+	preempt_disable();
+	if (cpu != smp_processor_id())
+		smp_call_function_single(cpu,
+			(smp_call_func_t)set_yield_mask,
+			(void *)&mask, 1);
+	else
+		set_yield_mask(&mask);
+	preempt_enable();
+}
+
+static inline void mcpy_yield(u32 yield_pin)
+{
+	mips_mt_yield(1 << yield_pin);
+}
+
+static inline void mcpy_yield_handler(struct mcpy_port *pport)
+{
+	if (unlikely(mcpy_done(pport->pid) < 0))
+		mcpy_irq_ack_mcpy_error();
+
+	mcpy_port_yield_ack(pport->pid);
+	/* gic_intr_ack(pport->yield_no); */
+}
+
+static void mcpy_intr_init(void)
+{
+	ltq_mcpy_w32(0x3FFFF, MCPY_INTERNAL_INT_EN);
+	ltq_mcpy_w32(0x0,     MCPY_INTERNAL_INT_MASK);
+	ltq_mcpy_w32(0x0,     MCPY_INT_EN);
+	ltq_mcpy_w32(0x0,     MCPY_INT_MASK);
+}
+
+inline void mcpy_eva_cfg(void)
+{
+	 u32 cfg_reg;
+
+	 cfg_reg = read_c0_segctl0();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(0));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(1));
+
+	 cfg_reg = read_c0_segctl1();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(2));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(3));
+
+	 cfg_reg = read_c0_segctl2();
+	 ltq_mcpy_w32(cfg_reg & 0xFFFF, MCPY_MIPS_CFG(4));
+	 ltq_mcpy_w32((cfg_reg >> 16) & 0xFFFF, MCPY_MIPS_CFG(5));
+}
+
+static void mcpy_cmd_init(struct mcpy_cmd *pcmd, struct mcpy_port *pport)
+{
+	pcmd->port = pport;
+	mutex_init(&pcmd->mtx_lock);
+	init_waitqueue_head(&pcmd->cmdq);
+	pcmd->status = MCPY_CMD_RELEASED;
+}
+
+#if defined(YLD_PIN_ALLOC) && YLD_PIN_ALLOC
+static int mcpy_get_yield_pin(unsigned int vpe_id)
+{
+	unsigned int yld_pin_bitmap;
+	int i;
+
+	yld_pin_bitmap = vmb_yr_get(vpe_id, 1);
+	if (!(yld_pin_bitmap & 0xFFFF))
+		return -1;
+	for (i = 0; i < 16; i++) {
+		if (yld_pin_bitmap & (1 << i))
+			return i;
+	}
+
+	return -1;
+}
+#else
+static int mcpy_get_yield_pin(unsigned int vpe_id)
+{
+	static int yld_pin = 0xF;
+	return yld_pin--;
+}
+
+#endif
+
+static int mcpy_port_init(struct mcpy_port *pport, u32 pid)
+{
+	int yld_pin;
+	pport->pid = pid;
+	pport->name = mcpy_get_name_by_pid(pid);
+
+	if (pport->status & MCPY_PORT_DISABLED)
+		goto mcpy_port_err;
+
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		if (!cpu_online(pport->vpe_id)) {
+			mcpy_dbg(MCPY_INIT, "HWMCPY vpe: %d not online!\n",
+				pport->vpe_id);
+			goto mcpy_port_err;
+		}
+		yld_pin = mcpy_get_yield_pin(pport->vpe_id);
+		if (yld_pin == -1) { /* No Yield resource */
+			mcpy_dbg(MCPY_INIT, "HWMCPY Allocate Yield resource Failure !\n");
+			goto mcpy_port_err;
+		}
+		pport->yld_pin = yld_pin;
+	}
+
+	if (mcpy_dma_init(pport) < 0) {
+		mcpy_dbg(MCPY_ERR, "HWMCPY DMA init failure!\n");
+		goto mcpy_port_err;
+	}
+
+	mcpy_intr_setup(pport);
+	mcpy_set_port_irq_intvl(pid, pport->irq_intvl);
+	spin_lock_init(&pport->port_lock);
+	mcpy_cmd_init(&pport->cmd, pport);
+
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		mcpy_dbg(MCPY_INIT, "Register Yield: irq:%d\n",
+				pport->yld_no);
+		if (devm_request_irq(pport->ctrl->dev, pport->yld_no,
+			mcpy_yld_dummy_handler,
+			IRQF_DISABLED, pport->name, (void *)pport)) {
+			mcpy_dbg(MCPY_ERR, "%s: Request yield irq: %d fail\n",
+				pport->name, pport->irq_no);
+			goto mcpy_port_err;
+		}
+		setup_percpu_yqmask((1 << pport->yld_pin), pport->vpe_id);
+	}
+	if (pport->irq_no != 0) {
+		mcpy_dbg(MCPY_INIT,
+			"Register interrupt: irq:%d\n", pport->irq_no);
+		if (devm_request_irq(pport->ctrl->dev, pport->irq_no,
+			mcpy_irq_handler,
+			IRQF_DISABLED, pport->name, (void *)pport)) {
+			mcpy_dbg(MCPY_ERR, "%s: Request irq: %d fail\n",
+				pport->name, pport->irq_no);
+			goto mcpy_port_err;
+		}
+	}
+	if (pport->irq_mode == MCPY_YLD_MODE) {
+		disable_irq(pport->yld_no);
+	/* NOTE: gic_yield_setup API must be called after register the irq
+	handler as it will clear the percpu mask in gic while request_irq
+	will set the per cpu mask via set irq affinity function.
+	For Yield interrupt, it must be cleared so that gic will not handle
+	Yield interrupt as it will set RMASK in gic*/
+		if (gic_yield_setup(pport->vpe_id,
+			pport->yld_pin, pport->yld_no)) {
+			mcpy_dbg(MCPY_INIT, "Yield Setup Fail: vpe:%d, pin:%d, yld_no:%d\n",
+				pport->vpe_id, pport->yld_pin, pport->yld_no);
+			goto mcpy_port_err;
+		}
+	}
+
+	return 0;
+
+mcpy_port_err:	
+	pport->status = MCPY_PORT_DISABLED;
+	return -ENODEV;
+}
+
+static int mcpy_port_cfg_init(struct platform_device *pdev,
+			struct mcpy_port *port, int pid)
+{
+	struct device_node *np = pdev->dev.of_node;
+	char res_name[32];
+	u32 cfg_res[5];
+	struct mcpy_port_ctrl *port_ctrl = port->pctrl;
+
+	sprintf(res_name, "lantiq,mcpy-ch%d", pid);
+	if (of_property_read_u32_array(np, res_name,
+			cfg_res, ARRAY_SIZE(cfg_res)) < 0) {
+		port->prio = mcpy_def_cfg[pid].prio;
+		port->vpe_id = mcpy_def_cfg[pid].vpe_id;
+		port->status = mcpy_def_cfg[pid].en;
+		port->trunk_size = MCPY_DEF_TRKSZ;
+		port->irq_intvl = MCPY_DEF_IRQ_INTVAL;
+		
+	} else {
+		port->prio = cfg_res[0] == MCPY_PRIO_LOW ? 
+				MCPY_PRIO_LOW : MCPY_PRIO_HIGH;
+		port->vpe_id = cfg_res[1] < num_possible_cpus() ?
+				(int)cfg_res[1] : -1;
+		port->trunk_size = cfg_res[2] >= MCPY_TKSZ_MAX ?
+				MCPY_DEF_TRKSZ : cfg_res[2];
+		port->irq_intvl = cfg_res[3] == 0?
+				MCPY_DEF_IRQ_INTVAL : cfg_res[3];
+		port->status = cfg_res[4] == 1 ?
+				MCPY_PORT_ENABLED : MCPY_PORT_DISABLED;
+	}
+
+	port->pid = pid;
+	sprintf(res_name, "yld%d", pid);
+	port->yld_no = platform_get_irq_byname(pdev, res_name);
+	if (port->yld_no <= 0) {
+		mcpy_dbg(MCPY_ERR,
+			"Cannot get mcpy ch %d yield irq!!\n", pid);
+		return -ENODEV;
+	}
+	sprintf(res_name, "irq%d", pid);
+	port->irq_no = platform_get_irq_byname(pdev, res_name);
+	if (port->irq_no <= 0) {
+		mcpy_dbg(MCPY_ERR,
+			"Cannot get mcpy ch %d irq !!\n", pid);
+		return -ENODEV;
+	}
+	port->irq_mode = port->prio;
+
+	if (port->status & MCPY_PORT_DISABLED)
+		return -ENODEV;
+	if (port->vpe_id >= 0 && port->vpe_id < num_possible_cpus()) {
+		if (port->prio)
+			port_ctrl->hi_per_vpe[port->vpe_id] = pid;
+		else
+			port_ctrl->lo_per_vpe[port->vpe_id] = pid;
+		port->status |= MCPY_PORT_RESERVED;
+	} else {
+		if (port->prio == MCPY_PRIO_LOW)
+			port_ctrl->prio_lo_map |= BIT(pid);
+	}
+
+	return 0;
+}
+
+static void mcpy_init(struct platform_device *pdev, struct mcpy_ctrl *pctrl)
+{
+	int i;
+	struct mcpy_port *pport;
+	struct mcpy_port_ctrl *port_ctrl;
+	u32 port_map = 0;
+
+	pctrl->dma_ctrl_id = DMA3;
+	pctrl->dma_port_id = DMA3_PORT;
+
+	mcpy_sw_reset();
+	mcpy_intr_init();
+
+	port_ctrl = &pctrl->port_ctrl;
+	port_ctrl->ctrl = pctrl;
+	spin_lock_init(&port_ctrl->mcpy_lock);
+
+	port_ctrl->hi_per_vpe = kmalloc(num_possible_cpus() * sizeof(u32),
+					GFP_KERNEL);
+	port_ctrl->lo_per_vpe = kmalloc(num_possible_cpus() * sizeof(u32),
+					GFP_KERNEL);
+	if (!port_ctrl->hi_per_vpe || !port_ctrl->lo_per_vpe) {
+		mcpy_dbg(MCPY_ERR, "alloc hi/lo array fail!\n");
+		return;
+	}
+	memset(port_ctrl->hi_per_vpe, -1, num_possible_cpus() * sizeof(u32));
+	memset(port_ctrl->lo_per_vpe, -1, num_possible_cpus() * sizeof(u32));
+	port_ctrl->lo_idx = 0;
+
+	for (i = 0; i < MCPY_PORTS_NUM; i++) {
+		pport = &port_ctrl->ports[i];
+		pport->ctrl = pctrl;
+		pport->pctrl = port_ctrl;
+		if (mcpy_port_cfg_init(pdev, pport, i) < 0)
+			continue;
+
+		if (mcpy_port_init(pport, i) < 0) {
+			port_ctrl->prio_lo_map &= ~(BIT(i));
+			continue;
+		}
+		port_map |= BIT(i);
+	}
+	ltq_mcpy_w32_mask((0xFF << 8), (port_map << 8 | 1), MCPY_GCTRL);
+
+	/* Copy GCR register to MCPY register */
+	mcpy_eva_cfg();
+}
+
+static inline struct mcpy_port *mcpy_get_port_by_id(u32 pid)
+{
+	return &ltq_mcpy_ctrl.port_ctrl.ports[pid];
+}
+
+static inline struct mcpy_port *mcpy_get_resv_port(u32 pid, u32 prio)
+{
+	struct mcpy_port *pport;
+
+	if (unlikely(pid >= MCPY_PORTS_NUM))
+		return NULL;
+
+	pport = mcpy_get_port_by_id(pid);
+	if (unlikely(pport->prio != prio ||
+		!(pport->status & MCPY_PORT_RESERVED)))
+		return NULL;
+
+	return pport;
+}
+/*
+ *  Find an available hwmcpy port
+ *  1. if Prio is high, then search dedicated port for that VPE.
+ *  High prio MCPY channel doesn't support dynamic allocate
+ *  due to the Yield feature.
+ *  2. if prio is low, then search dedicated port for that VPE, else search
+ *     on the prio_lo_map for the free hwmcpy port.
+ */
+static struct mcpy_port *mcpy_get_port_by_prio(u32 prio, u32 cpuid)
+{
+	int i;
+	struct mcpy_port *pport = NULL;
+	struct mcpy_port_ctrl *pctrl;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	if (prio == MCPY_PRIO_HIGH) {
+		if (likely(pctrl->hi_per_vpe[cpuid] < MCPY_PORTS_NUM))
+			return &pctrl->ports[pctrl->hi_per_vpe[cpuid]];
+		else
+			return NULL;
+	} else { /* LOW prio */
+		if (pctrl->lo_per_vpe[cpuid] < MCPY_PORTS_NUM)
+			pport = &pctrl->ports[pctrl->hi_per_vpe[cpuid]];
+		else if (pctrl->prio_lo_map) {
+			spin_lock_bh(&pctrl->mcpy_lock);
+			for (i = pctrl->lo_idx; i < MCPY_PORTS_NUM; i++) {
+				if (pctrl->prio_lo_map & BIT(i)) {
+					pport = &pctrl->ports[i];
+					pctrl->lo_idx
+						= (i + 1) % MCPY_PORTS_NUM;
+					break;
+				}
+			}
+			if (!pport) {
+				for (i = 0; i < pctrl->lo_idx; i++) {
+					if (pctrl->prio_lo_map & BIT(i)) {
+						pport = &pctrl->ports[i];
+						pctrl->lo_idx = i + 1;
+						break;
+					}
+				}
+			}
+			spin_unlock_bh(&pctrl->mcpy_lock);
+		}
+		return pport;
+	}
+
+	return NULL;
+}
+
+/*
+static void mcpy_dma_map(struct device *dev, void *addr,
+				size_t size, u32 flags)
+{
+	dma_addr_t dma_addr;
+	enum dma_data_direction dir = DMA_NONE;
+
+	if ((flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE))
+		== (DMA_TO_DEVICE | DMA_FROM_DEVICE)) {
+		dir = DMA_BIDIRECTIONAL;
+	} else if ((flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE)) == 0) {
+		dir = DMA_NONE;
+	} else {
+		dir = flags & (DMA_TO_DEVICE | DMA_FROM_DEVICE);
+	}
+
+	if (dir != DMA_NONE) {
+		dma_addr = dma_map_single(dev, (void *)addr, size, dir);
+		dma_unmap_single(dev, dma_addr, size, dir);
+	}
+}
+*/
+
+int mcpy_set_cpy_cmd(struct mcpy_cmd *pcmd, u32 dst,
+		u32 src, u32 len, enum mcpy_type mode)
+{
+	u32 dioc, sioc, ie, gather, last, ipc;
+	u32 cmd0, cmd1, cmd2, cmd3;
+	u32 pid;
+	const u32 sphy = 1; /* fixed use physical address */
+	const u32 dphy = 1;
+
+	src = __pa(src);
+	dst = __pa(dst);
+	mcpy_dbg(MCPY_DBG,
+		   "dst: 0x%x, src:0x%x, len:%d, mode: %d\n",
+		   dst, src, len, mode);
+
+	pid = pcmd->port->pid;
+	gather = pcmd->flags & MCPY_CMD_GATHER ? 1 : 0;
+	last   = pcmd->flags & MCPY_CMD_LAST   ? 1 : 0;
+	ipc    = pcmd->flags & MCPY_CMD_IPC    ? 1 : 0;
+
+	if (gather == 0 || last == 1)
+		ie = 1;
+	else
+		ie = 0;
+
+	dioc = sioc = 0;
+	switch (mode) {
+	case MCPY_PHY_TO_PHY:
+		dioc = sioc = 0;
+		break;
+	case MCPY_PHY_TO_IOCU:
+		dioc = 1;
+		sioc = 0;
+		break;
+	case MCPY_IOCU_TO_PHY:
+		sioc = 1;
+		dioc = 0;
+		break;
+	case MCPY_IOCU_TO_IOCU:
+		dioc = sioc = 1;
+		break;
+	default:
+		mcpy_dbg(MCPY_ERR, "MCPY type err!!\n");
+		return -EINVAL;
+	}
+
+	cmd0 = ((sphy & 0x1) << 31) | ((dphy & 0x1) << 30)
+		   | ((ie & 0x1) << 29) | ((pid & 0x7) << 26)
+		   | ((gather & 0x1) << 25) | ((last & 0x1) << 24)
+		   | ((ipc & 0x1) << 23)
+		   | ((pcmd->trunk_size & 0x7) << 20) | (len & 0xFFFFF);
+	cmd1 = src;
+	cmd2 = dst;
+	cmd3 = ((dioc & 0x1) << 30) | ((sioc & 0x1) << 29)
+		   | (pcmd->context & 0x1FFFFFFF);
+
+
+	mcpy_dbg(MCPY_DBG,
+		   "Pid:%d, CMD0:0x%x, CMD1:0x%x, CMD2: 0x%x, CMD3: 0x%x\n",
+		   pid, cmd0, cmd1, cmd2, cmd3);
+
+	/* Enable irq before issue the command */
+	if (in_interrupt() && ie)
+		enable_irq(pcmd->port->yld_no);
+
+	while(!(ltq_mcpy_r32(MCPY_CMD(3, pid)) & MCPY_OWN_BIT))
+		;
+
+	ltq_mcpy_w32(cmd0, MCPY_CMD(0, pid));
+	ltq_mcpy_w32(cmd1, MCPY_CMD(1, pid));
+	ltq_mcpy_w32(cmd2, MCPY_CMD(2, pid));
+	wmb();
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(hw_cycles, smp_processor_id())); 
+#endif
+
+	ltq_mcpy_w32(cmd3, MCPY_CMD(3, pid));
+
+	return 0;
+}
+
+static inline void mcpy_reset_cmd(struct mcpy_cmd *pcmd)
+{
+	pcmd->flags = 0;
+	pcmd->status = MCPY_CMD_RELEASED;
+}
+
+static inline void mcpy_port_release(struct mcpy_port *pport)
+{
+	mcpy_reset_cmd(&pport->cmd);
+}
+
+/*
+ * HW memcpy API: reserve one HW memcopy port
+ * Only free Low priority MCPY can be reserved
+ * return: pid<0-7>-Success/-1 -Failure
+ */
+int ltq_mcpy_reserve(void)
+{
+	struct mcpy_port_ctrl *pctrl;
+	int pid = -1;
+	int i;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	spin_lock_bh(&pctrl->mcpy_lock);
+	
+	if (pctrl->prio_lo_map) {
+		for (i = 0; i < MCPY_PORTS_NUM; i++) {
+			if (pctrl->prio_lo_map & BIT(i)) {
+				pid = i;
+				pctrl->prio_lo_map &= ~BIT(i);
+				pctrl->ports[i].status |= MCPY_PORT_RESERVED;
+			}
+		}
+	}
+
+	spin_unlock_bh(&pctrl->mcpy_lock);
+
+	return pid;
+}
+EXPORT_SYMBOL(ltq_mcpy_reserve);
+
+/*
+ * HW memcpy API: release one HW memcopy port
+ * @pid: reserved HW memcopy port id
+ */
+void ltq_mcpy_release(u32 pid)
+{
+	struct mcpy_port_ctrl *pctrl;
+
+	if (pid >= MCPY_PORTS_NUM)
+		return;
+
+	pctrl = &ltq_mcpy_ctrl.port_ctrl;
+	spin_lock_bh(&pctrl->mcpy_lock);
+	pctrl->prio_lo_map |= BIT(pid);
+	pctrl->ports[pid].status &= ~MCPY_PORT_RESERVED;
+	spin_unlock_bh(&pctrl->mcpy_lock);
+
+	return;
+}
+EXPORT_SYMBOL(ltq_mcpy_release);
+
+/*
+ * HW memcopy API
+ *   @dst:      Destionation address
+ *   @src:      Source address
+ *   @len:      Number of bytes to be copied
+ *   @pid:      HWMCPY Port ID if reserved flag is provided
+ *                 otherwise it will be ignored
+ *   @mode:   PHY_to_PHY, PHY_to_L2Cache,
+ *                 L2Cache_to_PHY, L2Cache_to_L2Cache
+ *                 (MCPY_PHY_TO_PHY, MCPY_PHY_TO_IOCU,
+ *                 MCPY_IOCU_TO_PHY,MCPY_IOCU_TO_IOCU)
+ *   @flags:    IPC/trunksize/port reserved
+ *   Note:       Non-Sleep process must use Yield Enabled Port and vice versa.
+ */
+void *ltq_hwmemcpy(void *dst, const void *src, u32 len,
+				u32 pid, enum mcpy_type mode, u32 flags)
+{
+	struct mcpy_port *pport;
+	enum mcpy_prio prio = MCPY_PRIO_HIGH;
+	u32 cpuid = smp_processor_id();
+	int sleep_en = !in_interrupt();
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(hwmcpy_cycles, smp_processor_id())); 
+#endif
+	if ((u32)src < (u32)dst && ((u32)src + len) > (u32)dst)
+		goto __SW_MEMMOVE_PATH;
+
+	if (len < g_mcpy_min_len)
+		goto __SW_MEMCPY_PATH;
+
+	if (unlikely(mode >= MCPY_SW_CPY))
+		goto __SW_MEMCPY_PATH;
+	/* TODO: Check TLB address */
+    
+	if (unlikely(sleep_en))
+		prio = MCPY_PRIO_LOW;
+
+	if (unlikely((flags & HWMCPY_F_RESERVED) != 0)) {
+		pport = mcpy_get_resv_port(pid, prio);
+		if (unlikely(!pport))
+			goto __SW_MEMCPY_PATH;
+	} else {
+		pport = mcpy_get_port_by_prio((u32)prio, cpuid);
+		if (!pport) {
+			mcpy_dbg(MCPY_ERR,
+				"Cannot find a valid port: prio: %d\n",	prio);
+			goto __SW_MEMCPY_PATH;
+		}
+	}
+
+	if (sleep_en)
+		mutex_lock(&pport->cmd.mtx_lock);
+
+	/*generate cmd and wait */
+	pport->cmd.status = MCPY_CMD_PROCESSING;
+	if (flags & HWMCPY_F_IPC)
+		pport->cmd.flags |= MCPY_CMD_IPC;
+
+	if (flags & HWMCPY_F_CHKSZ_SET)
+		pport->cmd.trunk_size = (flags & 0x1C) >> 2;
+	else
+		pport->cmd.trunk_size = pport->trunk_size;
+
+	mcpy_set_cpy_cmd(&pport->cmd, (u32)dst, (u32)src, len, mode);
+
+	/*Sleep or Yield */
+	if (sleep_en) {
+		wait_event_interruptible(pport->cmd.cmdq,
+			pport->cmd.status == MCPY_CMD_RELEASED);
+		mcpy_dbg(MCPY_DBG, "pid: %d wakeup\n", pport->pid);
+	} else {
+		do {
+			mcpy_dbg(MCPY_DBG, "HWMEMCPY Using Yield\n");
+			mcpy_yield(pport->yld_pin);
+		} while (!mcpy_inquire_irq_status(pport->pid));
+	}
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(hw_cycles, smp_processor_id())); 
+#endif
+
+	/*Wake up and clean function */
+	if (sleep_en) {
+		if (unlikely(pport->cmd.status != MCPY_CMD_RELEASED)) {
+			mcpy_dbg(MCPY_ERR, "Prio Low: hw memcopy interrupted!!\n");
+			/*TODO:  Should recover */
+			mcpy_port_release(pport);
+			mutex_unlock(&pport->cmd.mtx_lock);
+			return NULL;
+		}
+		mutex_unlock(&pport->cmd.mtx_lock);
+	} else { /* Yield Wake up*/
+		mcpy_yield_handler(pport);
+		/* Disable Yield irq if not in use */
+		disable_irq(pport->yld_no);
+	}
+
+	pport->mib_bytes += len;
+	pport->mib_use_times += 1;
+	mcpy_port_release(pport);
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(hwmcpy_cycles, smp_processor_id()));
+#endif    
+	return dst;
+
+__SW_MEMCPY_PATH:
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_Start(per_cpu(sw_cycles, smp_processor_id())); 
+#endif
+	memcpy(dst, src, len);
+
+#ifdef HWMCPY_PROFILE_CYCLE_CNT
+	CycleCounter_End(per_cpu(sw_cycles, smp_processor_id())); 
+#endif
+	return dst;
+__SW_MEMMOVE_PATH:
+	return memmove(dst, src, len);
+}
+EXPORT_SYMBOL(ltq_hwmemcpy);
+
+/*
+ * HW Memcpy API
+ * Copy small pieces scattered memory content into a whole continues memory
+ * Note: User only required call this function once to do the gathering.
+ *
+ *   @dst:            Destionation address
+ *   @src:            Source fragment list
+ *   @frag_num:    Number of fragments to be copied
+ *   @pid:            HWMCPY Port ID if reserved flag is provided
+ *                        otherwise it will be ignored
+ *   @mode:   PHY_to_PHY, PHY_to_L2Cache,
+ *                 L2Cache_to_PHY,  L2Cache_to_L2Cache
+ *                 (MCPY_PHY_TO_PHY, MCPY_PHY_TO_IOCU,
+ *                 MCPY_IOCU_TO_PHY,MCPY_IOCU_TO_IOCU)
+ *   @flags:    IPC/trunksize/port reserved
+ *   return:     0 if success
+ *                 -1 if failure
+ *   @limitation:  This API is not able to handle memory overlap copy.
+ *
+ */
+
+int ltq_hwmcpy_sg(void *dst, const struct mcpy_frag *src, u32 frag_num,
+				u32 pid, enum mcpy_type mode, u32 flags)
+{
+	struct mcpy_port *pport;
+	enum mcpy_prio prio = MCPY_PRIO_HIGH;
+	u32 cpuid = smp_processor_id();
+	int sleep_en = !in_interrupt();
+	int i, total_len = 0;
+
+	/* TODO: Check TLB address */
+	if (unlikely(sleep_en))
+		prio = MCPY_PRIO_LOW;
+
+	if (flags & HWMCPY_F_RESERVED) {
+		pport = mcpy_get_resv_port(pid, prio);
+		if (unlikely(!pport))
+			return -1;
+	} else {
+		pport = mcpy_get_port_by_prio(prio, cpuid);
+		if (!pport) {
+			mcpy_dbg(MCPY_ERR,
+				"Cannot find a valid port: prio: %d\n", prio);
+			return -1;
+		}
+	}
+
+	if (sleep_en)
+		mutex_lock(&pport->cmd.mtx_lock);
+
+	pport->cmd.status = MCPY_CMD_PROCESSING;
+	for (i = 0; i < frag_num; i++) {
+		/*generate cmd and wait */
+		if (flags & HWMCPY_F_IPC) /* ??? required for every segment or only for last one? */
+			pport->cmd.flags |= MCPY_CMD_IPC;
+
+		if ((flags & HWMCPY_F_CHKSZ_SET) != 0)
+			pport->cmd.trunk_size = (flags & 0x1C) >> 2;
+		else
+			pport->cmd.trunk_size = pport->trunk_size;
+
+		pport->cmd.flags |= MCPY_CMD_GATHER;
+
+		if (unlikely(i + 1 >= frag_num))
+			pport->cmd.flags |= MCPY_CMD_LAST;
+
+		mcpy_set_cpy_cmd(&pport->cmd, (u32)dst,
+			(u32)(src[i].ptr + src[i].offset),
+			src[i].size, mode);
+		total_len += src[i].size;
+	}
+
+	/*Sleep or Yield */
+	if (sleep_en)
+		wait_event_interruptible(pport->cmd.cmdq,
+			pport->cmd.status == MCPY_CMD_RELEASED);
+
+	else {
+		do {
+			mcpy_dbg(MCPY_DBG, "HWMEMCPY Using Yield\n");
+			mcpy_yield(pport->yld_pin);
+		} while (!mcpy_inquire_irq_status(pport->pid));
+	}
+
+	/*Wake up and clean function */
+	if (sleep_en) {
+		if (pport->cmd.status != MCPY_CMD_RELEASED) {
+			mcpy_dbg(MCPY_ERR, "Prio Low: hw memcopy gathering interrupted!!\n");
+			mcpy_port_release(pport);
+			mutex_unlock(&pport->cmd.mtx_lock);
+			return -1;
+		}
+		mutex_unlock(&pport->cmd.mtx_lock);
+	} else { /* Yield Wake up*/
+		mcpy_yield_handler(pport);
+		disable_irq(pport->yld_no);
+	}
+
+	pport->mib_bytes += total_len;
+	pport->mib_use_times += 1;
+	mcpy_port_release(pport);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_hwmcpy_sg);
+
+/**
+ *  Proc Functions
+ */
+static int mcpy_pctrl_read_proc(struct seq_file *s, void *v)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port_ctrl *port_ctrl = &pctrl->port_ctrl;
+	int i;
+
+	seq_puts(s, "\n MCPY Port Control\n");
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "MCPY base addr:0x%x, physical base addr: 0x%x\n",
+		(u32)pctrl->membase, pctrl->phybase);
+	seq_printf(s, "Low prio free mcpy port bitmap: 0x%x\n",
+		port_ctrl->prio_lo_map);
+	for (i = 0; i < num_possible_cpus(); i++) {
+		seq_printf(s, "VPE id: %d, Reserved: ", i);
+		if (port_ctrl->hi_per_vpe[i] <= MCPY_PORTS_NUM) {
+			seq_printf(s, "High Prio Port: %d ",
+				port_ctrl->hi_per_vpe[i]);
+		} else if (port_ctrl->lo_per_vpe[i] <= MCPY_PORTS_NUM) {
+			seq_printf(s, "Low Prio Port: %d ",
+				port_ctrl->lo_per_vpe[i]);
+		} else
+			seq_puts(s, "No mcpy ports");
+
+		seq_puts(s, "\n");
+	}
+	seq_printf(s, "MCPY Min Len threshold: %d\n", g_mcpy_min_len);
+
+	return 0;
+}
+
+
+static int mcpy_pctrl_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mcpy_pctrl_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations mcpy_pctrl_proc_fops = {
+	.open           = mcpy_pctrl_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+
+static int mcpy_dbg_read_proc(struct seq_file *s, void *v)
+{
+	int i;
+
+	seq_puts(s, "MCPY DBG Enable: ");
+	for (i = 0; i < DBG_MAX; i++) {
+		if (g_mcpy_dbg & BIT(i)) {
+			seq_printf(s, "%s ", g_mcpy_dbg_list[i]);
+		}
+	}
+	seq_puts(s, "\n");
+
+	return 0;
+}
+
+
+static int mcpy_dbg_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mcpy_dbg_read_proc, PDE_DATA(inode));
+}
+
+static ssize_t mcpy_dbg_write(struct file *file, const char __user *buf,
+			size_t count, loff_t *data)
+{
+	char str[32];
+	int len, rlen, i, j;
+	int num, enable = 0;
+	char *param_list[20];
+	len = count < sizeof(str) ? count : sizeof(str) - 1;
+	rlen = len - copy_from_user(str, buf, len);
+	str[rlen] = 0;
+
+	num = dp_split_buffer(str, param_list, ARRAY_SIZE(param_list));
+
+	if ((dp_strcmpi(param_list[0], "enable") == 0)
+		|| (dp_strcmpi(param_list[0], "en") == 0)) {
+		enable = 1;
+	} else if (dp_strcmpi(param_list[0], "disable") == 0
+		|| (dp_strcmpi(param_list[0], "dis") == 0)) {
+		enable = 0;
+	} else {
+		goto proc_dbg_help;
+	}
+	if (num <= 1) {
+		g_mcpy_dbg = 0;
+		return count;
+	}
+	for (i = 1; i < num; i++) {
+		for (j = 0; j < ARRAY_SIZE(g_mcpy_dbg_list); j++) {
+			if (dp_strcmpi(param_list[i], g_mcpy_dbg_list[j]) == 0) {
+				if (enable)
+					g_mcpy_dbg |= BIT(j);
+				else
+					g_mcpy_dbg &= ~(BIT(j));
+				break;
+			}
+		}
+	}
+
+	return count;
+
+proc_dbg_help:
+	mcpy_dbg(MCPY_INFO, "echo <enable/disable> err/event/init/info/dbg > dbg\n");
+
+	return count;
+}
+
+static const struct file_operations mcpy_dbg_proc_fops = {
+	.open           = mcpy_dbg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.write		= mcpy_dbg_write,
+	.release        = single_release,
+};
+
+static void *mcpy_port_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port *pport;
+
+	if (*pos >= MCPY_PORTS_NUM)
+		return NULL;
+
+	pport = &pctrl->port_ctrl.ports[*pos];
+
+	return pport;
+}
+
+static void *mcpy_port_seq_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct mcpy_ctrl *pctrl = s->private;
+	struct mcpy_port *pport;
+
+	if (++*pos >= MCPY_PORTS_NUM)
+		return NULL;
+	pport = &pctrl->port_ctrl.ports[*pos];
+	return pport;
+}
+
+static void mcpy_port_seq_stop(struct seq_file *s, void *v)
+{
+
+}
+
+static int mcpy_port_seq_show(struct seq_file *s, void *v)
+{
+	struct mcpy_port *pport = (struct mcpy_port *)v;
+
+	seq_printf(s, "----------HWMCPY Port[%d] info----------\n",
+		pport->pid);
+	seq_printf(s, "irq no: %d, yield no: %d, irq interval: %d\n",
+		pport->irq_no, pport->yld_no, pport->irq_intvl);
+	seq_printf(s, "priority: %s\n",
+		(pport->prio == MCPY_PRIO_HIGH) ?
+			"High Priority" : "Low Priority");
+	if (pport->irq_mode == MCPY_YLD_MODE)
+		seq_printf(s, "IRQ mode: Yield, Yield pin:%d, map to CPU:%d\n",
+			pport->yld_pin, pport->vpe_id);
+	else
+		seq_puts(s, "IRQ mode: Interrupt\n");
+	seq_printf(s, "port status: %s %s\n",
+		(pport->status & MCPY_PORT_DISABLED) ? "Disabled" : "Enabled",
+		(pport->status & MCPY_PORT_RESERVED) ? "Reserved" : "");
+	seq_printf(s, "dma rxch_id: %d, rch_base: 0x%x, rch_des_num: %d\n",
+		pport->chan.rch, pport->chan.rch_dbase,
+		pport->chan.rch_dnum);
+	seq_printf(s, "dma txch_id: %d, tch_base: 0x%x, tch_des_num: %d\n",
+		pport->chan.tch, pport->chan.tch_dbase,
+		pport->chan.tch_dnum);
+	seq_printf(s, "dma chan on/off: %s\n",
+		pport->chan.onoff == DMA_CH_ON ? "ON" : "OFF");
+	seq_printf(s, "trunk size: %s\n", g_trunksz[pport->trunk_size]);
+	seq_printf(s, "mib: use times: %llu, copied bytes: %llu\n",
+		pport->mib_use_times, pport->mib_bytes);
+
+	return 0;
+}
+
+static const struct seq_operations mcpy_port_seq_ops = {
+	.start = mcpy_port_seq_start,
+	.next = mcpy_port_seq_next,
+	.stop = mcpy_port_seq_stop,
+	.show = mcpy_port_seq_show,
+};
+
+static int mcpy_port_seq_open(struct inode *inode, struct file *file)
+{
+	int ret = seq_open(file, &mcpy_port_seq_ops);
+
+	if (ret == 0) {
+		struct seq_file *m = file->private_data;
+		m->private = PDE_DATA(inode);
+	}
+	return ret;
+}
+
+static const struct file_operations mcpy_port_proc_fops = {
+	.open = mcpy_port_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+static int mcpy_proc_init(struct mcpy_ctrl *pctrl)
+{
+	char proc_name[64] = {0};
+	struct proc_dir_entry *entry;
+
+	strcpy(proc_name, "driver/hwmcpy");
+	pctrl->proc = proc_mkdir(proc_name, NULL);
+	if (!pctrl->proc)
+		return -ENOMEM;
+
+	entry = proc_create_data("mcpy_ctrl", 0, pctrl->proc,
+			&mcpy_pctrl_proc_fops, pctrl);
+	if (!entry)
+		goto err1;
+	entry = proc_create_data("port_info", 0, pctrl->proc,
+			&mcpy_port_proc_fops, pctrl);
+	if (!entry)
+		goto err2;
+	/* entry = proc_create_data("umt_info", 0, pctrl->proc,
+			&mcpy_umt_proc_fops, &pctrl->umt); */
+
+	entry = proc_create_data("dbg", 0, pctrl->proc,
+			&mcpy_dbg_proc_fops, &pctrl);
+	if (!entry)
+		goto err3;
+
+	return 0;
+
+err3:
+	remove_proc_entry("port_info", pctrl->proc);
+err2:
+	remove_proc_entry("mcpy_ctrl", pctrl->proc);
+err1:
+	remove_proc_entry(proc_name, NULL);
+	return -ENOMEM;
+}
+
+struct mcpy_umt *mcpy_get_umt(void)
+{
+	return &ltq_mcpy_ctrl.umt;
+}
+
+static int mcpy_xrx500_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	struct mcpy_ctrl *pctrl;	
+	void __iomem *mcpy_addr_base;
+	struct device_node *np = pdev->dev.of_node;
+
+	pctrl = &ltq_mcpy_ctrl;
+	memset(pctrl, 0, sizeof(ltq_mcpy_ctrl));
+
+	/* load the memory ranges */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		panic("Failed to get HWMCPY resources\n");
+
+	pctrl->phybase = res->start;
+	mcpy_addr_base = devm_ioremap_resource(&pdev->dev, res);
+
+	if (IS_ERR(mcpy_addr_base))
+		panic("Failed to remap HWMCPY resources\n");
+
+	pctrl->membase = mcpy_addr_base;
+	g_mcpy_addr_base = mcpy_addr_base;
+	if (of_property_read_u32(np, "lantiq,mcpy-minlen", &g_mcpy_min_len) < 0) {
+		g_mcpy_min_len = MCPY_MIN_LEN;
+	}
+
+	pctrl->dev = &pdev->dev;
+
+	mcpy_init(pdev, pctrl);
+	/* Link platform with driver data for retrieving */
+	platform_set_drvdata(pdev, pctrl);
+	mcpy_proc_init(pctrl);
+	umt_init(pctrl);
+
+	mcpy_dbg(MCPY_INFO, "HW MCPY driver: Version: %s, Init Done !!",
+		MCPY_DRV_VERSION);
+
+	mcpy_profile_cycle_init();
+
+	return 0;
+
+}
+
+static int  mcpy_xrx500_release(struct platform_device *pdev)
+{
+	return 0;
+}
+static const struct of_device_id mcpy_xrx500_match[] = {
+	{ .compatible = "lantiq,mcpy-xrx500" },
+	{},
+};
+
+static struct platform_driver mcpy_xrx500_driver = {
+	.probe = mcpy_xrx500_probe,
+	.remove = mcpy_xrx500_release,
+	.driver = {
+		.name = "mcpy-xrx500",
+		.owner = THIS_MODULE,
+		.of_match_table = mcpy_xrx500_match,
+	},
+};
+
+int __init mcpy_xrx500_init(void)
+{
+	return platform_driver_register(&mcpy_xrx500_driver);
+}
+
+device_initcall(mcpy_xrx500_init);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Yixin.Zhu@lantiq.com");
+MODULE_DESCRIPTION("LTQ Hardware Memcpy Driver");
+MODULE_SUPPORTED_DEVICE("LTQ CPE Devices GRX35X, GRX5XX");
diff --git a/drivers/dma/ltq_hwmcpy.h b/drivers/dma/ltq_hwmcpy.h
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy.h
@@ -0,0 +1,248 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ */
+
+#ifndef __HWMCPY_H__
+#define __HWMCPY_H__
+
+#define MCPY_PORTS_NUM		8
+#define UMT_PORTS_NUM		4
+#define MCPY_CMD_DEPTH		16
+#define MCPY_OWN_BIT		BIT(31)
+#define MIN_UMT_PRD		20
+
+enum mcpy_prio {
+	MCPY_PRIO_LOW = 0,	/* Can Sleep */
+	MCPY_PRIO_HIGH,		/* Cannot Sleep */
+};
+
+enum {
+	MCPY_CMD_GATHER = 0x1,
+	MCPY_CMD_LAST   = 0x2,
+	MCPY_CMD_IPC	= 0x4,
+	MCPY_CMD_SRC_FLUSH = 0x8,
+	MCPY_CMD_SRC_INV   = 0x10,
+	MCPY_CMD_DST_FLUSH = 0x20,
+	MCPY_CMD_DST_INV   = 0x40,
+};
+
+enum mcpy_trunk_size {
+	MCPY_TKSZ_512B	= 0,
+	MCPY_TKSZ_1KB,
+	MCPY_TKSZ_2KB,
+	MCPY_TKSZ_4KB,
+	MCPY_TKSZ_8KB,
+	MCPY_TKSZ_16KB,
+	MCPY_TKSZ_32KB,
+	MCPY_TKSZ_64KB,
+	MCPY_TKSZ_MAX
+};
+
+enum mcpy_irq_md {
+	MCPY_IRQ_MODE = 0,
+	MCPY_YLD_MODE = 1,
+};
+
+enum mcpy_cmd_stat {
+	MCPY_CMD_PROCESSING	= 1,
+	MCPY_CMD_RELEASED	= 0,
+};
+
+enum dma_chan_on_off {
+	DMA_CH_OFF = 0,	/*!< DMA channel is OFF */
+	DMA_CH_ON = 1,	/*!< DMA channel is ON */
+};
+
+#define MCPY_DEF_TRKSZ		MCPY_TKSZ_512B
+#define MCPY_DEF_IRQ_INTVAL	50
+#define UMT_DEF_DMACID		7
+
+struct mcpy_port;
+struct mcpy_ctrl;
+
+struct mcpy_cmd {
+	struct mcpy_port *port;   /* back pointer */
+	u32 context;
+	u32 trunk_size;
+	enum mcpy_cmd_stat status;
+	enum mcpy_type type;
+	u32 flags;
+	wait_queue_head_t cmdq;
+	/*struct semaphore sem; */
+	struct mutex mtx_lock;
+	struct semaphore sem_lock;
+};
+
+struct dma_ch {
+	u32 rch;
+	u32 tch;
+	u32 rch_dbase;
+	u32 tch_dbase;
+	u32 rch_dnum;
+	u32 tch_dnum;
+	enum dma_chan_on_off onoff;
+	char rch_name[32];
+	char tch_name[32];
+};
+
+enum mcpy_port_status {
+	MCPY_PORT_DISABLED = 1,
+	MCPY_PORT_ENABLED = 2,
+	MCPY_PORT_RESERVED = 4,
+};
+
+enum umt_mode {
+	UMT_SELFCNT_MODE = 0,
+	UMT_USER_MODE    = 1,
+	UMT_MODE_MAX,
+};
+enum umt_msg_mode {
+	UMT_NO_MSG    = 0,
+	UMT_MSG0_ONLY = 1,
+	UMT_MSG1_ONLY = 2,
+	UMT_MSG0_MSG1 = 3,
+	UMT_MSG_MAX,
+};
+
+enum umt_status {
+	UMT_DISABLE = 0,
+	UMT_ENABLE  = 1,
+	UMT_STATUS_MAX,
+	UMT_BROKEN,
+};
+
+#ifdef CONFIG_LTQ_UMT_LEGACY_MODE
+struct mcpy_umt {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	enum umt_mode umt_mode;
+	enum umt_msg_mode msg_mode;
+	enum umt_status status;
+	u32 umt_period;
+	u32 umt_dst;
+	struct dma_ch chan;
+	spinlock_t umt_lock;
+	struct device *dev;
+};
+#else
+
+struct umt_port {
+	struct mcpy_umt *pctrl;
+	u32 umt_pid;
+	u32 ep_id;
+	enum umt_mode umt_mode;
+	enum umt_msg_mode msg_mode;
+	enum umt_status status;
+	u32 umt_period;
+	u32 umt_dst;
+	u32 cbm_pid; /* CBM WLAN ID (0 - 3) */
+	u32 dma_cid; /* DMA Chan ID */
+	enum umt_status suspend;
+	spinlock_t umt_port_lock;
+};
+
+struct mcpy_umt {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	struct umt_port ports[UMT_PORTS_NUM];
+	u32 dma_ctrlid;
+	enum umt_status status;
+	struct proc_dir_entry *proc;
+	spinlock_t umt_lock;
+};
+
+#endif
+
+struct mcpy_port {
+	struct mcpy_ctrl *ctrl;  /* back pointer */
+	struct mcpy_port_ctrl *pctrl; /* back pointer */
+	const char *name;
+	u32 prio;
+	int vpe_id;
+	u32 pid;
+	int irq_no;
+	int yld_no;
+	u32 yld_pin;
+	struct mcpy_cmd cmd;
+	struct dma_ch chan;
+	enum mcpy_port_status status;
+	spinlock_t port_lock;
+	enum mcpy_trunk_size trunk_size;
+	enum mcpy_irq_md irq_mode;
+	u32 irq_intvl;
+	/* atomic_t users; remove for performace inprovement*/
+	u64 mib_bytes;
+	u64 mib_use_times;
+};
+
+struct mcpy_port_ctrl {
+	struct mcpy_ctrl *ctrl; /*back pointer */
+	u32 prio_lo_map; /* available port bitmap for low priority */
+	int lo_idx;
+	u32 *hi_per_vpe; /*Dedicated mcpy per VPE */
+	u32 *lo_per_vpe; /*Dedicated mcpy per VPE */
+	struct mcpy_port ports[MCPY_PORTS_NUM];
+	spinlock_t mcpy_lock;
+};
+
+struct mcpy_ctrl {
+	void __iomem *membase;
+	u32 phybase;
+	u32 dma_ctrl_id;
+	u32 dma_port_id;
+	struct mcpy_port_ctrl port_ctrl;
+	struct mcpy_umt  umt;
+	struct device *dev;
+	struct proc_dir_entry *proc;
+};
+
+struct mcpy_cfg {
+	u32 prio;
+	int vpe_id;
+	u32 en;
+};
+
+/* Debug Level */
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value)  MCPY_##name = BIT(value),
+#define MCPY_DBG_LIST			\
+	MCPY_DBG_DEF(ERR, 0)		\
+	MCPY_DBG_DEF(EVENT, 1)		\
+	MCPY_DBG_DEF(INIT, 2)		\
+	MCPY_DBG_DEF(INFO, 3)		\
+	MCPY_DBG_DEF(DBG,  4)		\
+	MCPY_DBG_DEF(MAX, 10)		\
+
+enum {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+#define MCPY_DBG_DEF(name, value) DBG_##name,
+enum {
+	MCPY_DBG_LIST
+};
+#undef MCPY_DBG_DEF
+
+extern u32 g_mcpy_dbg;
+#define mcpy_dbg(dbg_level, fmt, arg...) \
+	do { \
+		if (unlikely(g_mcpy_dbg & dbg_level)) { \
+			if (dbg_level & MCPY_ERR) { \
+				pr_err_ratelimited(fmt, ##arg); \
+			} else if ((dbg_level & MCPY_INFO) || \
+					(dbg_level & MCPY_INIT)) { \
+				pr_info_ratelimited(fmt, ##arg); \
+			} else { \
+				pr_debug_ratelimited(fmt, ##arg); \
+			} \
+		} \
+	} \
+	while (0)
+
+void umt_init(struct mcpy_ctrl *);
+struct mcpy_umt *mcpy_get_umt(void);
+
+#endif  /* __HWMCPY_H__ */
+
diff --git a/drivers/dma/ltq_hwmcpy_addr.h b/drivers/dma/ltq_hwmcpy_addr.h
new file mode 100755
--- /dev/null
+++ b/drivers/dma/ltq_hwmcpy_addr.h
@@ -0,0 +1,92 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ */
+
+#ifndef __HWMCPY_ADDR_H__
+#define __HWMCPY_ADDR_H__
+
+#define MCPY_GCTRL				0x200
+#define MCPY_INTERNAL_INT_EN			0x8C
+#define MCPY_INTERNAL_INT_MASK			0x84
+#define MCPY_INT_EN				0x88
+#define MCPY_INT_MASK				0x80
+#define MCPY_INT_STAT				0x90
+#define MCPY_PORT_TO_CNT_0			0x300
+#define MCPY_MRES_REG0_0			0x100
+#define MCPY_MRES_REG1_0			0x104
+#define MCPY_MIPS_CFG_0				0x240
+#define MCPY_UMT_SW_MODE			0x94
+#define MCPY_UMT_PERD				0xDC
+#define MCPY_UMT_MSG(x)				(0x220 + (x) * 4)
+#define MCPY_UMT_DEST				0x230
+#define MCPY_UMT_XBASE				0x400
+#define MCPY_UMT_XOFFSET			0x100
+#define MCPY_UMT_XMSG(x)			(0x0 + (x) * 4)
+#define MCPY_UMT_XPERIOD			0x20
+#define MCPY_UMT_XDEST				0x30
+#define MCPY_UMT_XSW_MODE			0x34
+#define MCPY_UMT_TRG_MUX			0xE0
+#define MCPY_UMT_CNT_CTRL			0xE4
+
+#define MCPY_UMT_X_ADDR(x, off) (MCPY_UMT_XBASE + \
+				((x) - 1) * MCPY_UMT_XOFFSET + (off))
+
+#define MRES_ERROR				BIT(30)
+#define MRES_DONE				BIT(31)
+#define MCPY_CMD_ERR				BIT(16)
+#define MCPY_LEN_ERR				BIT(17)
+
+extern void __iomem *g_mcpy_addr_base;
+
+#define ltq_mcpy_r32(x)		ltq_r32(g_mcpy_addr_base + (x))
+#define ltq_mcpy_w32(x, y)	ltq_w32((x), g_mcpy_addr_base + (y))
+#define ltq_mcpy_w32_mask(x, y, z)	\
+			ltq_w32_mask((x), (y), g_mcpy_addr_base + (z))
+
+#define PORT_TO_CNT(pid)	(MCPY_PORT_TO_CNT_0 + (pid) * 0x10)
+#define PORT_MRES(pid)		(MCPY_MRES_REG1_0 + (pid) * 0x10)
+#define MCPY_SET_RESPONSE(pid)	ltq_mcpy_w32(0, PORT_MRES(pid))
+#define MCPY_GET_RESPONSE(pid)	ltq_mcpy_r32(PORT_MRES(pid))
+#define MCPY_MIPS_CFG(x)	(MCPY_MIPS_CFG_0 + (x) * 0x10)
+
+#define MCPY_CMD(cid, pid)	((cid) * 0x4 + (pid) * 0x10)
+
+/* GCR Address */
+#define GCR_BASE		0xB2300000
+#define GCR_CUS_BASE            0xB23F0000
+#define CCA_IC_MREQ0            0x18
+#define GCR_CCA_IC_MREQ(id)     (GCR_CUS_BASE + CCA_IC_MREQ0 + (id) * 4)
+#define GCR_REG0_BASE		0x0090
+#define GCR_REG1_BASE		0x00A0
+#define GCR_REG2_BASE		0x00B0
+#define GCR_REG3_BASE		0x00C0
+#define GCR_REG4_BASE		0x0190
+#define GCR_REG5_BASE		0x01A0
+#define GCR_REG0_MASK		0x0098
+#define GCR_REG1_MASK		0x00A8
+#define GCR_REG2_MASK		0x00B8
+#define GCR_REG3_MASK		0x00C8
+#define GCR_REG4_MASK		0x0198
+#define GCR_REG5_MASK		0x01A8
+#define GCR_CTRL_REG		(GCR_BASE + 0x10)
+#define GCR_CFG_ENABLE		(GCR_BASE + 0x60)
+#define GCR_UMT_REG		(GCR_BASE + GCR_REG3_BASE)
+#define GCR_UMT_MASK		(GCR_BASE + GCR_REG3_MASK)
+
+#define MCPY_DMA_RX_CID		12
+#define MCPY_DMA_TX_CID		(MCPY_DMA_RX_CID + 1)
+#define UMT_DMA_RX_CID		28
+#define UMT_DMA_TX_CID		(UMT_DMA_RX_CID + 1)
+#define MCPY_DBASE		0x10600
+#define MCPY_DBASE_OFFSET	0x100
+#define UMT_DBASE		0x11800
+#define DMA_DESC_SIZE		16 /* 4 DWs */
+
+
+#define DMA3_MASTERID           29
+
+#endif  /*__HWMCPY_ADDR_H__ */
diff --git a/drivers/dma/ltq_umt_expand.c b/drivers/dma/ltq_umt_expand.c
new file mode 100644
--- /dev/null
+++ b/drivers/dma/ltq_umt_expand.c
@@ -0,0 +1,701 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ * UMT Driver for GRX350 A21
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <lantiq_dmax.h>
+		  
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+		  
+#include <net/datapath_proc_api.h>
+#include "ltq_hwmcpy_addr.h"
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy.h"
+
+static u32 g_dma_ctrl = DMA1TX;
+
+static inline void umt_set_mode(u32 umt_id, enum umt_mode umt_mode)
+{
+	u32 val, off;
+	
+	if (!umt_id)
+		ltq_mcpy_w32_mask(0x2, ((u32)umt_mode) << 1, MCPY_GCTRL);
+	else {
+		off = 16 + (umt_id - 1) * 3;
+		val = ltq_mcpy_r32(MCPY_GCTRL) & ~(BIT(off));
+		ltq_mcpy_w32(val | (((u32)umt_mode) << off), MCPY_GCTRL);
+	}
+}
+
+static inline void umt_set_msgmode(u32 umt_id, enum umt_msg_mode msg_mode)
+{
+	if (!umt_id)
+		ltq_mcpy_w32((u32)msg_mode, MCPY_UMT_SW_MODE);
+	else
+		ltq_mcpy_w32((u32)msg_mode,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XSW_MODE));
+}
+
+/* input in term of microseconds */
+static inline u32 umt_us_to_cnt(int usec)
+{
+	struct clk *ngi_clk = clk_get_xbar();
+
+	return (usec * (clk_get_rate(ngi_clk) / 1000000));
+}
+
+static inline void umt_set_period(u32 umt_id, u32 umt_period)
+{
+	umt_period = umt_us_to_cnt(umt_period);
+
+	if (!umt_id)
+		ltq_mcpy_w32(umt_period, MCPY_UMT_PERD);
+	else
+		ltq_mcpy_w32(umt_period,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XPERIOD));
+}
+
+static inline void umt_set_dst(u32 umt_id, u32 umt_dst)
+{
+	if (!umt_id)
+		ltq_mcpy_w32(umt_dst, MCPY_UMT_DEST);
+	else
+		ltq_mcpy_w32(umt_dst,
+			MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XDEST));
+}
+
+static inline void umt_set_mux(u32 umt_id, u32 cbm_pid, u32 dma_cid)
+{
+	u32 mux_sel;
+
+	cbm_pid = cbm_pid & 0xF;
+	dma_cid = dma_cid & 0xF;
+	mux_sel = ltq_mcpy_r32(MCPY_UMT_TRG_MUX) & ~((0xF000F) <<(umt_id * 4));
+	mux_sel |= (dma_cid << (umt_id * 4)) | (cbm_pid << (16 + (umt_id * 4)));
+	ltq_mcpy_w32(mux_sel, MCPY_UMT_TRG_MUX);
+}
+
+static inline void umt_set_endian(int dw_swp, int byte_swp)
+{
+	u32 val;
+
+	val = ltq_mcpy_r32(MCPY_GCTRL);
+	if (byte_swp)
+		val |= BIT(28);
+	else
+		val &= ~(BIT(28));
+
+	if (dw_swp)
+		val |= BIT(29);
+	else
+		val &= ~(BIT(29));
+
+	ltq_mcpy_w32(val, MCPY_GCTRL);
+}
+
+static inline void umt_en_expand_mode(void)
+{
+	u32 val;
+
+	val = ltq_mcpy_r32(MCPY_GCTRL) | BIT(31);
+	ltq_mcpy_w32(val, MCPY_GCTRL);
+
+	if (IS_ENABLED(CONFIG_CPU_BIG_ENDIAN))
+		umt_set_endian(1, 0);
+	else
+		umt_set_endian(1, 1);
+}
+
+static inline void umt_enable(u32 umt_id, enum umt_status status)
+{
+	u32 val, off;
+
+	if (!umt_id)
+		ltq_mcpy_w32_mask(0x4, ((u32)status) << 2, MCPY_GCTRL);
+	else {
+		off = 17 + (umt_id - 1) * 3;
+		val = (ltq_mcpy_r32(MCPY_GCTRL) & ~BIT(off))
+				| (((u32)status) << off);
+		ltq_mcpy_w32(val, MCPY_GCTRL);
+	}
+}
+
+static inline void umt_suspend(u32 umt_id, enum umt_status status)
+{
+	u32 val;
+
+	if (status)
+		val = ltq_mcpy_r32(MCPY_UMT_CNT_CTRL) | BIT(umt_id);
+	else
+		val = ltq_mcpy_r32(MCPY_UMT_CNT_CTRL) & (~(BIT(umt_id)));
+
+	ltq_mcpy_w32(val, MCPY_UMT_CNT_CTRL); 
+}
+
+/*This function will disable umt */
+static inline void umt_reset_umt(u32 umt_id)
+{
+	u32 mode;
+	umt_enable(umt_id, UMT_DISABLE);
+
+	mode = ltq_mcpy_r32(MCPY_UMT_X_ADDR(umt_id, MCPY_UMT_XSW_MODE));
+
+	if (mode == UMT_SELFCNT_MODE) {
+		umt_set_mode(umt_id, UMT_USER_MODE);
+		umt_set_mode(umt_id, UMT_SELFCNT_MODE);
+	} else {
+		umt_set_mode(umt_id, UMT_SELFCNT_MODE);
+		umt_set_mode(umt_id, UMT_USER_MODE);
+	}
+
+	return;
+}
+
+/**
+ * intput:
+ * @umt_id: UMT port id, (0 - 3)
+ * @ep_id:  Aligned with datapath lib ep_id
+ * @period: measured in microseconds.
+ * ret:  Fail < 0 / Success: 0
+ */
+int ltq_umt_set_period(u32 umt_id, u32 ep_id, u32 period)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (period < MIN_UMT_PRD || umt_id >= UMT_PORTS_NUM)
+		goto period_err;
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		spin_unlock_bh(&port->umt_port_lock);
+		goto period_err;
+	}
+
+	if (port->umt_period != period) {
+		port->umt_period = period;
+		umt_set_period(umt_id, port->umt_period);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+period_err:
+	mcpy_dbg(MCPY_ERR, "umt_id: %d, ep_id: %d, period: %d\n",
+		umt_id, ep_id, period);
+
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_set_period);
+
+/**
+ * API to configure the UMT port.
+ * input:
+ * @umt_id: (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @umt_mode:  0-self-counting mode, 1-user mode.
+ * @msg_mode:  0-No MSG, 1-MSG0 Only, 2-MSG1 Only, 3-MSG0 & MSG1.
+ * @dst:  Destination PHY address.
+ * @period(ms): only applicable when set to self-counting mode. 
+ *              self-counting interval time. if 0, use the original setting.
+ * @enable: 1-Enable/0-Disable
+ * @ret:  Fail < 0 , SUCCESS:0
+ */
+int ltq_umt_set_mode(u32 umt_id, u32 ep_id, u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!!\n");
+		return -ENODEV;
+	}
+	if ((umt_mode >= (u32)UMT_MODE_MAX)
+			|| (msg_mode >= (u32)UMT_MSG_MAX)
+			|| (enable >= (u32)UMT_STATUS_MAX)
+			|| (phy_dst == 0)
+			|| (period == 0)
+			|| (umt_id >= UMT_PORTS_NUM)) {
+		mcpy_dbg(MCPY_ERR, "umt_id: %d, umt_mode: %d, msg_mode: %d, enable: %d, phy_dst: %d\n",
+			umt_id, umt_mode, msg_mode, enable, phy_dst);
+		return -EINVAL;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, port ep_id: %d\n",
+			ep_id, port->ep_id);
+		spin_unlock_bh(&port->umt_port_lock);
+		return -EINVAL;
+	}
+
+	umt_reset_umt(umt_id);
+
+	port->umt_mode = (enum umt_mode)umt_mode;
+	port->msg_mode = (enum umt_msg_mode)msg_mode;
+	port->umt_dst	= phy_dst;
+	port->umt_period = period;
+	port->status = (enum umt_status)enable;
+
+	umt_set_mode(umt_id, port->umt_mode);
+	umt_set_msgmode(umt_id, port->msg_mode);
+	umt_set_dst(umt_id, port->umt_dst);
+	umt_set_period(umt_id, port->umt_period);
+	umt_enable(umt_id, port->status);
+	/* setup the CBM/DMA mapping */
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_mode);
+
+/**
+ * API to enable/disable umt port
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @enable: Enable: 1 / Disable: 0
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_enable(u32 umt_id, u32 ep_id, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (umt_id >= UMT_PORTS_NUM)
+		return -EINVAL;
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status != UMT_ENABLE)
+		return -ENODEV;
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id || port->umt_dst == 0 || port->ep_id == 0) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, umt port ep_id: %d, umt_dst: 0x%x\n",
+			ep_id, port->ep_id, port->umt_dst);
+		goto en_err;
+	}
+
+	if (port->status != enable) {
+		port->status = (enum umt_status)enable;
+		umt_enable(umt_id, port->status);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+en_err:
+	spin_unlock_bh(&port->umt_port_lock);
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_enable);
+
+/**
+ * API to suspend/resume umt US/DS counter
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * @enable: suspend: 1 / resume: 0
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_suspend(u32 umt_id, u32 ep_id, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (umt_id >= UMT_PORTS_NUM)
+		return -EINVAL;
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status != UMT_ENABLE)
+		return -ENODEV;
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id || port->umt_dst == 0 || port->ep_id == 0) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, umt port ep_id: %d, umt_dst: 0x%x\n",
+			ep_id, port->ep_id, port->umt_dst);
+		goto en_err;
+	}
+
+	if (port->suspend != enable) {
+		port->suspend = (enum umt_status)enable;
+		umt_enable(umt_id, port->status);
+		umt_suspend(umt_id, port->suspend);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+en_err:
+	spin_unlock_bh(&port->umt_port_lock);
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_suspend);
+
+/**
+ * API to request and allocate UMT port
+ * input:
+ * @ep_id: aligned with datapath lib EP.
+ * @cbm_pid: CBM Port ID(0-3), 0 - CBM port 4, 1 - CBM port 24,
+ * 2 - CBM port 25, 3 - CBM port 26
+ * output:
+ * @dma_ctrlid: DMA controller ID. aligned with DMA driver DMA controller ID
+ * @dma_cid: DMA channel ID. 
+ * @umt_id: (0 - 3)
+ * ret: Fail: < 0,  Success: 0
+ */
+int ltq_umt_request(u32 ep_id, u32 cbm_pid,
+		u32 *dma_ctrlid, u32 *dma_cid, u32 *umt_id)
+{
+	int i, pid;
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	struct umt_port *port;
+
+	if (!dma_ctrlid || !dma_cid || !umt_id) {
+		mcpy_dbg(MCPY_ERR, "Output pointer is NULL!\n");
+		goto param_err;
+	}
+
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT not initialized!\n");
+		goto param_err;
+	}
+	if (!ep_id) {
+		mcpy_dbg(MCPY_ERR, "%s: ep_id cannot be zero!\n", __func__);
+		goto param_err;
+	}
+
+	if (cbm_pid >= UMT_PORTS_NUM) {
+		mcpy_dbg(MCPY_ERR, "%s: cbm pid must be in ranage(0 - %d)\n",
+			__func__, UMT_PORTS_NUM);
+		goto param_err;
+	}
+
+	pid = -1;
+	spin_lock_bh(&pumt->umt_lock);
+	for (i = 0; i < UMT_PORTS_NUM; i++) {
+		port = &pumt->ports[i];
+		spin_lock_bh(&port->umt_port_lock);
+		if (port->ep_id == ep_id) {
+			pid = i;
+			spin_unlock_bh(&port->umt_port_lock);
+			break;
+		} else if (port->ep_id == 0 && pid == -1)
+			pid = i;
+		spin_unlock_bh(&port->umt_port_lock);
+	}
+	spin_unlock_bh(&pumt->umt_lock);
+
+	if (pid < 0) {
+		mcpy_dbg(MCPY_ERR, "No free UMT port!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[pid];
+	spin_lock_bh(&port->umt_port_lock);
+	port->ep_id = ep_id;
+	port->cbm_pid = cbm_pid;
+	umt_set_mux(port->umt_pid, port->cbm_pid, port->dma_cid);
+	*dma_ctrlid = pumt->dma_ctrlid;
+	*dma_cid = port->dma_cid;
+	*umt_id = port->umt_pid;
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+
+param_err:
+	return -EINVAL;
+}
+EXPORT_SYMBOL(ltq_umt_request);
+
+/**
+ * API to release umt port
+ * input:
+ * @umt_id (0 - 3)
+ * @ep_id: aligned with datapath lib EP
+ * 
+ * ret:  Fail < 0, Success: 0
+ */
+int ltq_umt_release(u32 umt_id, u32 ep_id)
+{
+	struct mcpy_umt *pumt;
+	struct umt_port *port;
+	
+	if (umt_id >= UMT_PORTS_NUM)
+		return -ENODEV;
+
+	pumt = mcpy_get_umt();
+	if (pumt->status != UMT_ENABLE) {
+		mcpy_dbg(MCPY_ERR, "UMT is not initialized!\n");
+		return -ENODEV;
+	}
+
+	port = &pumt->ports[umt_id];
+
+	spin_lock_bh(&port->umt_port_lock);
+	if (port->ep_id != ep_id) {
+		mcpy_dbg(MCPY_ERR, "input ep_id: %d, UMT port ep_id: %d\n",
+			ep_id, port->ep_id);
+		spin_unlock_bh(&port->umt_port_lock);
+
+		return -ENODEV;
+	} else {
+		port->ep_id = 0;
+		port->cbm_pid = 0;
+		port->umt_dst = 0;
+		port->umt_period = 0;
+		port->status = UMT_DISABLE;
+
+		umt_enable(port->umt_pid, UMT_DISABLE);
+	}
+	spin_unlock_bh(&port->umt_port_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_release);
+
+
+
+static void umt_port_init(struct mcpy_umt *pumt,
+		struct device_node *node, int pid)
+{
+	char res_cid[32];
+	int cid;
+	struct umt_port *port;
+
+	port = &pumt->ports[pid];
+	sprintf(res_cid, "lantiq,umt%d-dmacid", pid);
+	if (of_property_read_u32(node, res_cid, &cid) < 0) {
+		cid = UMT_DEF_DMACID + pid;
+	}
+	port->pctrl = pumt;
+	port->umt_pid = pid;
+	port->dma_cid = cid;
+	port->ep_id = 0;
+	port->status = UMT_DISABLE;
+	spin_lock_init(&port->umt_port_lock);
+}
+
+static void *umt_port_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct mcpy_umt *pumt = s->private;
+	struct umt_port *port;
+
+	if (*pos >= UMT_PORTS_NUM)
+		return NULL;
+
+	port = &pumt->ports[*pos];
+
+	return port;
+}
+
+static void *umt_port_seq_next(struct seq_file *s, void *v, loff_t *pos)
+{
+	struct mcpy_umt *pumt = s->private;
+	struct umt_port *port;
+
+	if (++*pos >= UMT_PORTS_NUM)
+		return NULL;
+	port = &pumt->ports[*pos];
+	return port;
+}
+
+static void umt_port_seq_stop(struct seq_file *s, void *v)
+{
+
+}
+
+static int umt_port_seq_show(struct seq_file *s, void *v)
+{
+	struct umt_port *port = v;
+	int pid = port->umt_pid;
+	u32 val;
+
+	seq_printf(s, "\nUMT port %d configuration\n", pid);
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "UMT port ep_id: %d\n", port->ep_id);
+	seq_printf(s, "UMT Mode: \t%s\n",
+		port->umt_mode == UMT_SELFCNT_MODE ?
+		"UMT SelfCounting Mode" : "UMT User Mode");
+	switch (port->msg_mode) {
+	case UMT_NO_MSG:
+		seq_puts(s, "UMT MSG Mode: \tUMT NO MSG\n");
+		break;
+	case UMT_MSG0_ONLY:
+		seq_puts(s, "UMT MSG Mode: \tUMT MSG0 Only\n");
+		break;
+	case UMT_MSG1_ONLY:
+		seq_puts(s, "UMT MSG Mode: \tUMT MSG1 Only\n");
+		break;
+	case UMT_MSG0_MSG1:
+		seq_puts(s, "UMT MSG Mode: \tUMT_MSG0_And_MSG1\n");
+		break;
+	default:
+		seq_printf(s, "UMT MSG Mode Error! Msg_mode: %d\n",
+			port->msg_mode);
+	}
+	seq_printf(s, "UMT DST: \t0x%x\n", port->umt_dst);
+	if (port->umt_mode == UMT_SELFCNT_MODE)
+		seq_printf(s, "UMT Period: \t%d(us)\n", port->umt_period);
+	seq_printf(s, "UMT Status: \t%s\n",
+			port->status == UMT_ENABLE ? "Enable" :
+			port->status == UMT_DISABLE ? "Disable" : "Init Fail");
+	seq_printf(s, "UMT DMA CID: \t%d\n", port->dma_cid);
+	seq_printf(s, "UMT CBM PID: \t%d\n", port->cbm_pid);
+
+	seq_printf(s, "++++Register dump of umt port: %d++++\n", pid);
+	if (pid == 0) {
+		seq_printf(s, "UMT Status: \t%s\n", 
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(2)) != 0 ?
+			"Enable" : "Disable");
+		seq_printf(s, "UMT Mode: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(1)) != 0 ?
+			"UMT User MSG mode" : "UMT SelfCounting mode");
+		seq_printf(s, "UMT MSG Mode: \t%d\n",
+			ltq_mcpy_r32(MCPY_UMT_SW_MODE));
+		seq_printf(s, "UMT Dst: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_DEST));
+		seq_printf(s, "UMT Period: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_PERD));
+		seq_printf(s, "UMT MSG0: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_MSG(0)));
+		seq_printf(s, "UMT MSG1: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_MSG(1)));
+		
+	} else {
+		seq_printf(s, "UMT Status: \t%s\n", 
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(17 + 3 * (pid - 1))) != 0 ?
+			"Enable" : "Disable");
+		seq_printf(s, "UMT Mode: \t%s\n",
+			(ltq_mcpy_r32(MCPY_GCTRL) & BIT(16 + 3 * (pid - 1))) != 0 ?
+			"UMT User MSG mode" : "UMT SelfCounting mode");
+		seq_printf(s, "UMT MSG Mode: \t%d\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XSW_MODE)));
+		seq_printf(s, "UMT Dst: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XDEST)));
+		seq_printf(s, "UMT Period: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XPERIOD)));
+		seq_printf(s, "UMT MSG0: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XMSG(0))));
+		seq_printf(s, "UMT MSG1: \t0x%x\n",
+			ltq_mcpy_r32(MCPY_UMT_X_ADDR(pid, MCPY_UMT_XMSG(1))));
+	}
+
+	val = ltq_mcpy_r32(MCPY_UMT_TRG_MUX);
+	seq_printf(s, "DMA CID: \t%d\n",
+		(val & ((0xF) << (pid * 4))) >> (pid * 4));
+	seq_printf(s, "CBM PID: \t%d\n",
+		(val & ((0xF) << (16 + pid * 4))) >> (16 + pid * 4));
+
+	return 0;
+}
+
+
+static const struct seq_operations umt_port_seq_ops = {
+	.start = umt_port_seq_start,
+	.next = umt_port_seq_next,
+	.stop = umt_port_seq_stop,
+	.show = umt_port_seq_show,
+};
+
+static int umt_cfg_read_proc_open(struct inode *inode, struct file *file)
+{
+	int ret = seq_open(file, &umt_port_seq_ops);
+
+	if (ret == 0) {
+		struct seq_file *m = file->private_data;
+		m->private = PDE_DATA(inode);
+	}
+	return ret;
+}
+
+static const struct file_operations mcpy_umt_proc_fops = {
+	.open           = umt_cfg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = seq_release,
+};
+
+static int umt_proc_init(struct mcpy_umt *pumt)
+{
+	struct proc_dir_entry *entry;
+
+	pumt->proc = proc_mkdir("umt", pumt->ctrl->proc);
+	if (!pumt->proc)
+		return -ENOMEM;
+
+	entry = proc_create_data("umt_info", 0, pumt->proc,
+			&mcpy_umt_proc_fops, pumt);
+	if (!entry)
+		goto err1;
+
+	return 0;
+
+err1:
+	mcpy_dbg(MCPY_ERR, "UMT proc create fail!\n");
+	remove_proc_entry("umt", pumt->ctrl->proc);
+	return -1;
+}
+
+/* TODO: Register UMT error interrupt Handler */
+void umt_init(struct mcpy_ctrl *pctrl)
+{
+	struct device_node *node = pctrl->dev->of_node;
+	struct mcpy_umt *pumt;
+	int i;
+
+	pumt = &pctrl->umt;
+	pumt->ctrl = pctrl;
+	pumt->dma_ctrlid = g_dma_ctrl;
+	spin_lock_init(&pumt->umt_lock);
+	umt_en_expand_mode();
+
+	for (i = 0; i < UMT_PORTS_NUM; i++)
+		umt_port_init(pumt, node, i);
+
+	umt_proc_init(pumt);
+	pumt->status = UMT_ENABLE;
+
+	mcpy_dbg(MCPY_INFO, "UMT initialize success!\n");
+
+	return;
+}
+
+
+
diff --git a/drivers/dma/ltq_umt_legacy.c b/drivers/dma/ltq_umt_legacy.c
new file mode 100644
--- /dev/null
+++ b/drivers/dma/ltq_umt_legacy.c
@@ -0,0 +1,341 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License version 2 as published
+ * by the Free Software Foundation.
+ *
+ * Copyright (C) 2015 Zhu YiXin<yixin.zhu@lantiq.com>
+ * UMT Driver for GRX350 A11
+ */
+#define DEBUG
+#include <linux/init.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/dma-mapping.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/seq_file.h>
+#include <lantiq_dmax.h>
+	 
+#include <lantiq.h>
+#include <lantiq_soc.h>
+#include <lantiq_irq.h>
+	 
+#include <net/datapath_proc_api.h>
+#include "ltq_hwmcpy_addr.h"
+#include <linux/ltq_hwmcpy.h>
+#include "ltq_hwmcpy.h"
+
+static int umt_dma_init(struct dma_ch *pchan, struct mcpy_umt *pumt)
+{
+	u32 dma_rxch, dma_txch;
+
+	pchan->rch = UMT_DMA_RX_CID;
+	pchan->tch = UMT_DMA_TX_CID;
+	pchan->rch_dnum = 1;
+	pchan->tch_dnum = 1;
+	pchan->rch_dbase =
+		pumt->ctrl->phybase + UMT_DBASE;
+		pchan->tch_dbase = pchan->rch_dbase + 0x10;
+	pchan->onoff = DMA_CH_ON;
+	sprintf(pchan->rch_name, "UMT RXCH");
+	sprintf(pchan->tch_name, "UMT TXCH");
+
+	dma_rxch = _DMA_C(pumt->ctrl->dma_ctrl_id,
+		pumt->ctrl->dma_port_id,
+		pchan->rch);
+	mcpy_dbg(MCPY_DBG, "dma_ch: 0x%x, rch_name:%s\n",
+		dma_rxch, pchan->rch_name);
+	if (ltq_request_dma(dma_rxch, pchan->rch_name) < 0) {
+		mcpy_dbg(MCPY_ERR,
+			"umt request dma chan [0x%x] fail\n", dma_rxch);
+		goto __UMT_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_rxch,
+			pchan->rch_dbase, pchan->rch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_rxch);
+		goto __UMT_PORT_FAIL;
+	}
+
+	dma_txch = _DMA_C(pumt->ctrl->dma_ctrl_id,
+			pumt->ctrl->dma_port_id,
+			pchan->tch);
+	mcpy_dbg(MCPY_DBG, "dma_ch: 0x%x, rch_name:%s\n",
+		dma_txch, pchan->tch_name);
+	if (ltq_request_dma(dma_txch, pchan->tch_name) < 0) {
+		mcpy_dbg(MCPY_ERR, "request dma chan [0x%x] fail\n", dma_txch);
+		goto __UMT_PORT_FAIL;
+	}
+	if (ltq_dma_chan_desc_cfg(dma_txch,
+			pchan->tch_dbase, pchan->tch_dnum) < 0) {
+		mcpy_dbg(MCPY_ERR, "setup dma chan [0x%x] fail\n", dma_txch);
+		goto __UMT_PORT_FAIL;
+	}
+
+	if (pchan->onoff == DMA_CH_ON) {
+		ltq_dma_chan_on(dma_rxch);
+		ltq_dma_chan_on(dma_txch);
+	} else {
+		ltq_dma_chan_off(dma_rxch);
+		ltq_dma_chan_off(dma_txch);
+	}
+
+	return 0;
+
+__UMT_PORT_FAIL:
+	pumt->status = UMT_BROKEN;
+	return -1;
+}
+
+static inline void umt_set_mode(enum umt_mode umt_mode)
+{
+	ltq_mcpy_w32_mask(0x2, ((u32)umt_mode) << 1, MCPY_GCTRL);
+}
+
+static inline void umt_set_msgmode(enum umt_msg_mode msg_mode)
+{
+	ltq_mcpy_w32(msg_mode, MCPY_UMT_SW_MODE);
+}
+
+static inline u32 umt_msec_to_cnt(int msec)
+{
+	return (msec * 0x17AC / 20);
+}
+
+static inline void umt_set_period(u32 umt_period)
+{
+	umt_period = umt_msec_to_cnt(umt_period);
+	ltq_mcpy_w32(umt_period, MCPY_UMT_PERD);
+}
+
+static inline void umt_set_dst(u32 umt_dst)
+{
+	ltq_mcpy_w32(umt_dst, MCPY_UMT_DEST);
+}
+
+static inline void umt_enable(enum umt_status status)
+{
+	ltq_mcpy_w32_mask(0x4, ((u32)status) << 2, MCPY_GCTRL);
+}
+
+/*This function will disable umt */
+static inline void umt_reset_umt(enum umt_mode mode)
+{
+	umt_enable(UMT_DISABLE);
+	if (mode == UMT_SELFCNT_MODE) {
+		umt_set_mode(UMT_USER_MODE);
+		umt_set_mode(UMT_SELFCNT_MODE);
+	} else {
+		umt_set_mode(UMT_SELFCNT_MODE);
+		umt_set_mode(UMT_USER_MODE);
+	}
+	return;
+}
+
+int ltq_umt_set_period(u32 period)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	if (period == 0)
+		return -1;
+
+	if (pumt->umt_period == period)
+		return 0;
+	else {
+		spin_lock_bh(&pumt->umt_lock);
+		pumt->umt_period = period;
+		umt_set_period(pumt->umt_period);
+		spin_unlock_bh(&pumt->umt_lock);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_period);
+
+/*
+* @umt_mode:	0-self-counting mode, 1-user mode.
+* @msg_mode:	0-No MSG, 1-MSG0 Only, 2-MSG1 Only, 3-MSG0 & MSG1.
+* @dst:  Destination PHY address.
+* @period: only applicable when set to self-counting mode.
+*		 self-counting interval time. if 0, use the original setting.
+* @enable: 1-Enable/0-Disable
+* @ret: -1 Fail/0-SUCCESS
+*/
+int ltq_umt_set_mode(u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+
+	if (pumt->status == UMT_BROKEN)
+		return -ENODEV;
+	if (umt_mode >= (u32)UMT_MODE_MAX
+			|| msg_mode >= (u32)UMT_MSG_MAX
+			|| enable   >= (u32)UMT_STATUS_MAX
+			|| phy_dst  == 0)
+		return -EINVAL;
+
+	spin_lock_bh(&pumt->umt_lock);
+	umt_reset_umt(pumt->umt_mode);
+
+	pumt->umt_mode = (enum umt_mode)umt_mode;
+	pumt->msg_mode = (enum umt_msg_mode)msg_mode;
+	pumt->umt_dst	= phy_dst;
+	if (period)
+		pumt->umt_period = period;
+	pumt->status = (enum umt_status)enable;
+
+	umt_set_mode(pumt->umt_mode);
+	umt_set_msgmode(pumt->msg_mode);
+	umt_set_dst(pumt->umt_dst);
+	if (period)
+		umt_set_period(pumt->umt_period);
+	umt_enable(pumt->status);
+	spin_unlock_bh(&pumt->umt_lock);
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_set_mode);
+
+/*
+* Enable/Disable UMT
+* @enable: 1-Enable/0-Disable
+* ret: -1 Fail/0-Success
+*/
+int ltq_umt_enable(u32 enable)
+{
+	struct mcpy_umt *pumt = mcpy_get_umt();
+	if (enable >= (u32)UMT_STATUS_MAX
+			|| pumt->status == UMT_BROKEN)
+		return -ENODEV;
+
+	if (pumt->status != enable) {
+		spin_lock_bh(&pumt->umt_lock);
+		pumt->status = (enum umt_status)enable;
+		umt_enable(pumt->status);
+		spin_unlock_bh(&pumt->umt_lock);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ltq_umt_enable);
+
+static void umt_access_wkrd(void)
+{
+	unsigned int __iomem *addr;
+	u32 val;
+
+	addr = (unsigned int __iomem *)GCR_CFG_ENABLE;
+	ltq_w32(0x123F0001, addr);
+	addr = (unsigned int __iomem *)GCR_CCA_IC_MREQ(DMA3_MASTERID);
+	ltq_wk_w32(2, addr);
+	addr = (unsigned int __iomem *)GCR_CTRL_REG;
+	val = ltq_r32(addr);
+	val |= 2;
+	ltq_w32(val, addr);
+	mcpy_dbg(MCPY_INFO, "UMT workaround applied!!\n");
+}
+
+static void umt_chip_init(struct mcpy_umt *pumt)
+{
+	umt_access_wkrd();
+	umt_set_mode(pumt->umt_mode);
+	umt_set_msgmode(pumt->msg_mode);
+	if (pumt->umt_period)
+		umt_set_period(pumt->umt_period);
+	if (pumt->umt_dst)
+		umt_set_dst(pumt->umt_dst);
+	umt_enable((u32)pumt->status);
+}
+
+/* TODO: Register UMT error interrupt Handler */
+void umt_init(struct mcpy_ctrl *pctrl)
+{
+	struct device_node *node = pctrl->dev->of_node;
+	struct mcpy_umt *pumt;
+
+	pumt = &pctrl->umt;
+	pumt->ctrl = pctrl;
+	pumt->umt_mode = UMT_SELFCNT_MODE;
+	pumt->status = UMT_DISABLE;
+	pumt->msg_mode = UMT_MSG0_MSG1;
+	pumt->ctrl = pctrl;
+
+	if (of_property_read_u32(node,
+		"lantiq,umt-period", &pumt->umt_period) < 0)
+	pumt->umt_period = 0;	 
+	spin_lock_init(&pumt->umt_lock);
+	pumt->dev = pctrl->dev;
+
+	if (umt_dma_init(&pumt->chan, pumt) < 0)
+		return;
+	umt_chip_init(pumt);
+	return;
+}
+
+static int umt_cfg_read_proc(struct seq_file *s, void *v)
+{
+	struct mcpy_umt *pumt = s->private;
+
+	seq_puts(s, "\nUMT configuration\n");
+	seq_puts(s, "-----------------------------------------\n");
+	seq_printf(s, "UMT Mode                 %s\n",
+		pumt->umt_mode == UMT_SELFCNT_MODE ?
+		"UMT SelfCounting Mode" : "UMT User Mode");
+	switch (pumt->msg_mode) {
+	case UMT_NO_MSG:
+		seq_puts(s, "UMT MSG Mode             UMT NO MSG\n");
+		break;
+	case UMT_MSG0_ONLY:
+		seq_puts(s, "UMT MSG Mode             UMT MSG0 Only\n");
+		break;
+	case UMT_MSG1_ONLY:
+		seq_puts(s, "UMT MSG Mode             UMT MSG1 Only\n");
+		break;
+	case UMT_MSG0_MSG1:
+		seq_puts(s, "UMT MSG Mode             UMT_MSG0_And_MSG1\n");
+		break;
+	default:
+		seq_printf(s, "UMT MSG Mode Error! Msg_mode: %d\n",
+			pumt->msg_mode);
+	}
+	seq_printf(s, "UMT DST                  0x%x\n", pumt->umt_dst);
+	if (pumt->umt_mode == UMT_SELFCNT_MODE)
+		seq_printf(s, "UMT Period               0x%x\n",
+			pumt->umt_period == 0 ? 0x17AC : pumt->umt_period);
+	seq_printf(s, "UMT Status               %s\n",
+			pumt->status == UMT_ENABLE ? "Enable" :
+			pumt->status == UMT_DISABLE ? "Disable" : "Init Fail");
+	seq_printf(s, "dma rxch_id: %d, rch_base: 0x%x, rch_des_num: %d\n",
+		pumt->chan.rch, pumt->chan.rch_dbase,
+		pumt->chan.rch_dnum);
+	seq_printf(s, "dma txch_id: %d, tch_base: 0x%x, tch_des_num: %d\n",
+		pumt->chan.tch, pumt->chan.tch_dbase,
+		pumt->chan.tch_dnum);
+	seq_printf(s, "dma chan on/off: %s\n",
+		pumt->chan.onoff == DMA_CH_ON ? "ON" : "OFF");
+
+	return 0;
+}
+
+static int umt_cfg_read_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, umt_cfg_read_proc, PDE_DATA(inode));
+}
+
+static const struct file_operations mcpy_umt_proc_fops = {
+	.open           = umt_cfg_read_proc_open,
+	.read           = seq_read,
+	.llseek         = seq_lseek,
+	.release        = single_release,
+};
+
+
diff --git a/include/linux/ltq_hwmcpy.h b/include/linux/ltq_hwmcpy.h
new file mode 100755
--- /dev/null
+++ b/include/linux/ltq_hwmcpy.h
@@ -0,0 +1,70 @@
+/******************************************************************************
+ *
+ *                        Copyright (c) 2012, 2014, 2015
+ *                           Lantiq Deutschland GmbH
+ *
+ *  For licensing information, see the file 'LICENSE' in the root folder of
+ *  this software module.
+ *  
+ ******************************************************************************/
+
+#ifndef __LTQ_HWMCPY_H__
+#define __LTQ_HWMCPY_H__
+
+enum {
+	HWMCPY_F_PRIO_LOW	= 0x0,
+	HWMCPY_F_PRIO_HIGH	= 0x1,
+	HWMCPY_F_IPC		= 0x2,
+	HWMCPY_F_CHKSZ_1	= 0x4,
+	HWMCPY_F_CHKSZ_2	= 0x8,
+	HWMCPY_F_CHKSZ_3	= 0xC,
+	HWMCPY_F_CHKSZ_4	= 0x10,
+/*Trunk size support from 0 to 0x7(512B,1KB to 64KB),
+ start from bit 2, len 3 bits, max 0x1C */
+	HWMCPY_F_RESERVED	= 0x20,
+	HWMCPY_F_CHKSZ_SET	= 0x40,
+	HWMCPY_F_LAST		= 0x80,
+
+};
+
+enum mcpy_type {
+	MCPY_PHY_TO_PHY = 0,
+	MCPY_PHY_TO_IOCU,
+	MCPY_IOCU_TO_PHY,
+	MCPY_IOCU_TO_IOCU,
+	MCPY_SW_CPY,
+};
+
+struct mcpy_frag {
+	void *ptr;
+	u16 offset;
+	u16 size;
+};
+
+extern void *ltq_hwmemcpy(void *dst, const void *src, u32 len,
+				u32 portid, enum mcpy_type mode, u32 flags);
+extern int ltq_hwmcpy_sg(void *dst, const struct mcpy_frag *src, u32 frag_num,
+				u32 portid, enum mcpy_type mode, u32 flags);
+extern int ltq_mcpy_reserve(void);
+extern void ltq_mcpy_release(u32);
+
+#ifdef CONFIG_LTQ_UMT_LEGACY_MODE
+extern int ltq_umt_enable(u32 enable);
+extern int ltq_umt_set_mode(u32 umt_mode, u32 msg_mode,
+				u32 phy_dst, u32 period, u32 enable);
+extern int ltq_umt_set_period(u32 period);
+#else
+int ltq_umt_set_period(u32 umt_id, u32 ep_id, u32 period);
+int ltq_umt_set_mode(u32 umt_id, u32 ep_id, u32 umt_mode, u32 msg_mode,
+			u32 phy_dst, u32 period, u32 enable);
+int ltq_umt_enable(u32 umt_id, u32 ep_id, u32 enable);
+int ltq_umt_request(u32 ep_id, u32 cbm_pid,
+		u32 *dma_ctrlid, u32 *dma_cid, u32 *umt_id);
+int ltq_umt_release(u32 umt_id, u32 ep_id);
+int ltq_umt_suspend(u32 umt_id, u32 ep_id, u32 enable);
+
+
+
+#endif
+
+#endif /* __LTQ_HWMCPY_H__ */
