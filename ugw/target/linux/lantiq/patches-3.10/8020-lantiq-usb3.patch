Support for Lantiq USB 3

diff --git a/drivers/usb/host/xhci-dbg.c b/drivers/usb/host/xhci-dbg.c
--- a/drivers/usb/host/xhci-dbg.c
+++ b/drivers/usb/host/xhci-dbg.c
@@ -253,27 +253,27 @@ void xhci_print_trb_offsets(struct xhci_
 void xhci_debug_trb(struct xhci_hcd *xhci, union xhci_trb *trb)
 {
 	u64	address;
-	u32	type = le32_to_cpu(trb->link.control) & TRB_TYPE_BITMASK;
+	u32	type = XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control) & TRB_TYPE_BITMASK;
 
 	switch (type) {
 	case TRB_TYPE(TRB_LINK):
 		xhci_dbg(xhci, "Link TRB:\n");
 		xhci_print_trb_offsets(xhci, trb);
 
-		address = le64_to_cpu(trb->link.segment_ptr);
+		address = XHCI_LE64_TO_CPU(xhci->snoswap,trb->link.segment_ptr);
 		xhci_dbg(xhci, "Next ring segment DMA address = 0x%llx\n", address);
 
 		xhci_dbg(xhci, "Interrupter target = 0x%x\n",
-			 GET_INTR_TARGET(le32_to_cpu(trb->link.intr_target)));
+			 GET_INTR_TARGET(XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.intr_target)));
 		xhci_dbg(xhci, "Cycle bit = %u\n",
-			 le32_to_cpu(trb->link.control) & TRB_CYCLE);
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control) & TRB_CYCLE);
 		xhci_dbg(xhci, "Toggle cycle bit = %u\n",
-			 le32_to_cpu(trb->link.control) & LINK_TOGGLE);
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control) & LINK_TOGGLE);
 		xhci_dbg(xhci, "No Snoop bit = %u\n",
-			 le32_to_cpu(trb->link.control) & TRB_NO_SNOOP);
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control) & TRB_NO_SNOOP);
 		break;
 	case TRB_TYPE(TRB_TRANSFER):
-		address = le64_to_cpu(trb->trans_event.buffer);
+		address = XHCI_LE64_TO_CPU(xhci->snoswap,trb->trans_event.buffer);
 		/*
 		 * FIXME: look at flags to figure out if it's an address or if
 		 * the data is directly in the buffer field.
@@ -281,12 +281,12 @@ void xhci_debug_trb(struct xhci_hcd *xhc
 		xhci_dbg(xhci, "DMA address or buffer contents= %llu\n", address);
 		break;
 	case TRB_TYPE(TRB_COMPLETION):
-		address = le64_to_cpu(trb->event_cmd.cmd_trb);
+		address = XHCI_LE64_TO_CPU(xhci->snoswap,trb->event_cmd.cmd_trb);
 		xhci_dbg(xhci, "Command TRB pointer = %llu\n", address);
 		xhci_dbg(xhci, "Completion status = %u\n",
-			 GET_COMP_CODE(le32_to_cpu(trb->event_cmd.status)));
+			 GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,trb->event_cmd.status)));
 		xhci_dbg(xhci, "Flags = 0x%x\n",
-			 le32_to_cpu(trb->event_cmd.flags));
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->event_cmd.flags));
 		break;
 	default:
 		xhci_dbg(xhci, "Unknown TRB with TRB type ID %u\n",
@@ -318,10 +318,10 @@ void xhci_debug_segment(struct xhci_hcd 
 	for (i = 0; i < TRBS_PER_SEGMENT; ++i) {
 		trb = &seg->trbs[i];
 		xhci_dbg(xhci, "@%016llx %08x %08x %08x %08x\n", addr,
-			 lower_32_bits(le64_to_cpu(trb->link.segment_ptr)),
-			 upper_32_bits(le64_to_cpu(trb->link.segment_ptr)),
-			 le32_to_cpu(trb->link.intr_target),
-			 le32_to_cpu(trb->link.control));
+			 lower_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,trb->link.segment_ptr)),
+			 upper_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,trb->link.segment_ptr)),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.intr_target),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control));
 		addr += sizeof(*trb);
 	}
 }
@@ -400,10 +400,10 @@ void xhci_dbg_erst(struct xhci_hcd *xhci
 		entry = &erst->entries[i];
 		xhci_dbg(xhci, "@%016llx %08x %08x %08x %08x\n",
 			 addr,
-			 lower_32_bits(le64_to_cpu(entry->seg_addr)),
-			 upper_32_bits(le64_to_cpu(entry->seg_addr)),
-			 le32_to_cpu(entry->seg_size),
-			 le32_to_cpu(entry->rsvd));
+			 lower_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,entry->seg_addr)),
+			 upper_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,entry->seg_addr)),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,entry->seg_size),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,entry->rsvd));
 		addr += sizeof(*entry);
 	}
 }
@@ -437,7 +437,7 @@ char *xhci_get_slot_state(struct xhci_hc
 {
 	struct xhci_slot_ctx *slot_ctx = xhci_get_slot_ctx(xhci, ctx);
 
-	switch (GET_SLOT_STATE(le32_to_cpu(slot_ctx->dev_state))) {
+	switch (GET_SLOT_STATE(XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_state))) {
 	case SLOT_STATE_ENABLED:
 		return "enabled/disabled";
 	case SLOT_STATE_DEFAULT:
diff --git a/drivers/usb/host/xhci-hub.c b/drivers/usb/host/xhci-hub.c
--- a/drivers/usb/host/xhci-hub.c
+++ b/drivers/usb/host/xhci-hub.c
@@ -68,7 +68,7 @@ static void xhci_common_hub_descriptor(s
 	temp |= HUB_CHAR_INDV_PORT_OCPM;
 	/* Bits 6:5 - no TTs in root ports */
 	/* Bit  7 - no port indicators */
-	desc->wHubCharacteristics = cpu_to_le16(temp);
+	desc->wHubCharacteristics = cpu_to_le16(temp);//ltq_h: always swap
 }
 
 /* Fill in the USB 2.0 roothub descriptor */
@@ -152,7 +152,7 @@ static void xhci_usb3_hub_descriptor(str
 			port_removable |= 1 << (i + 1);
 	}
 
-	desc->u.ss.DeviceRemovable = cpu_to_le16(port_removable);
+	desc->u.ss.DeviceRemovable = cpu_to_le16(port_removable);//ltq_h: always swap
 }
 
 static void xhci_hub_descriptor(struct usb_hcd *hcd, struct xhci_hcd *xhci,
@@ -701,7 +701,7 @@ int xhci_hub_control(struct usb_hcd *hcd
 		if (bus_state->port_c_suspend & (1 << wIndex))
 			status |= 1 << USB_PORT_FEAT_C_SUSPEND;
 		xhci_dbg(xhci, "Get port status returned 0x%x\n", status);
-		put_unaligned(cpu_to_le32(status), (__le32 *) buf);
+		put_unaligned(cpu_to_le32(status), (__le32 *) buf);//ltq_h: always swap
 		break;
 	case SetPortFeature:
 		if (wValue == USB_PORT_FEAT_LINK_STATE)
@@ -734,7 +734,7 @@ int xhci_hub_control(struct usb_hcd *hcd
 			}
 			/* In spec software should not attempt to suspend
 			 * a port unless the port reports that it is in the
-			 * enabled (PED = ‘1’,PLS < ‘3’) state.
+			 * enabled (PED = ????PLS < ???? state.
 			 */
 			temp = xhci_readl(xhci, port_array[wIndex]);
 			if ((temp & PORT_PE) == 0 || (temp & PORT_RESET)
diff --git a/drivers/usb/host/xhci-mem.c b/drivers/usb/host/xhci-mem.c
--- a/drivers/usb/host/xhci-mem.c
+++ b/drivers/usb/host/xhci-mem.c
@@ -103,10 +103,10 @@ static void xhci_link_segments(struct xh
 	prev->next = next;
 	if (type != TYPE_EVENT) {
 		prev->trbs[TRBS_PER_SEGMENT-1].link.segment_ptr =
-			cpu_to_le64(next->dma);
+			XHCI_CPU_TO_LE64(xhci->snoswap,next->dma);
 
 		/* Set the last TRB in the segment to have a TRB type ID of Link TRB */
-		val = le32_to_cpu(prev->trbs[TRBS_PER_SEGMENT-1].link.control);
+		val = XHCI_LE32_TO_CPU(xhci->snoswap,prev->trbs[TRBS_PER_SEGMENT-1].link.control);
 		val &= ~TRB_TYPE_BITMASK;
 		val |= TRB_TYPE(TRB_LINK);
 		/* Always set the chain bit with 0.95 hardware */
@@ -115,7 +115,7 @@ static void xhci_link_segments(struct xh
 				(type == TYPE_ISOC &&
 				 (xhci->quirks & XHCI_AMD_0x96_HOST)))
 			val |= TRB_CHAIN;
-		prev->trbs[TRBS_PER_SEGMENT-1].link.control = cpu_to_le32(val);
+		prev->trbs[TRBS_PER_SEGMENT-1].link.control = XHCI_CPU_TO_LE32(xhci->snoswap,val);
 	}
 }
 
@@ -140,9 +140,9 @@ static void xhci_link_rings(struct xhci_
 
 	if (ring->type != TYPE_EVENT && ring->enq_seg == ring->last_seg) {
 		ring->last_seg->trbs[TRBS_PER_SEGMENT-1].link.control
-			&= ~cpu_to_le32(LINK_TOGGLE);
+			&= ~XHCI_CPU_TO_LE32(xhci->snoswap,LINK_TOGGLE);
 		last->trbs[TRBS_PER_SEGMENT-1].link.control
-			|= cpu_to_le32(LINK_TOGGLE);
+			|= XHCI_CPU_TO_LE32(xhci->snoswap,LINK_TOGGLE);
 		ring->last_seg = last;
 	}
 }
@@ -257,7 +257,7 @@ static struct xhci_ring *xhci_ring_alloc
 	if (type != TYPE_EVENT) {
 		/* See section 4.9.2.1 and 6.4.4.1 */
 		ring->last_seg->trbs[TRBS_PER_SEGMENT - 1].link.control |=
-			cpu_to_le32(LINK_TOGGLE);
+			XHCI_CPU_TO_LE32(xhci->snoswap,LINK_TOGGLE);
 	}
 	xhci_initialize_ring_info(ring, cycle_state);
 	return ring;
@@ -661,7 +661,7 @@ struct xhci_stream_info *xhci_alloc_stre
 			SCT_FOR_CTX(SCT_PRI_TR) |
 			cur_ring->cycle_state;
 		stream_info->stream_ctx_array[cur_stream].stream_ring =
-			cpu_to_le64(addr);
+			XHCI_CPU_TO_LE64(xhci->snoswap,addr);
 		xhci_dbg(xhci, "Setting stream %d ring ptr to 0x%08llx\n",
 				cur_stream, (unsigned long long) addr);
 
@@ -727,10 +727,10 @@ void xhci_setup_streams_ep_input_ctx(str
 	max_primary_streams = fls(stream_info->num_stream_ctxs) - 2;
 	xhci_dbg(xhci, "Setting number of stream ctx array entries to %u\n",
 			1 << (max_primary_streams + 1));
-	ep_ctx->ep_info &= cpu_to_le32(~EP_MAXPSTREAMS_MASK);
-	ep_ctx->ep_info |= cpu_to_le32(EP_MAXPSTREAMS(max_primary_streams)
+	ep_ctx->ep_info &= XHCI_CPU_TO_LE32(xhci->snoswap,~EP_MAXPSTREAMS_MASK);
+	ep_ctx->ep_info |= XHCI_CPU_TO_LE32(xhci->snoswap,EP_MAXPSTREAMS(max_primary_streams)
 				       | EP_HAS_LSA);
-	ep_ctx->deq  = cpu_to_le64(stream_info->ctx_array_dma);
+	ep_ctx->deq  = XHCI_CPU_TO_LE64(xhci->snoswap,stream_info->ctx_array_dma);
 }
 
 /*
@@ -743,9 +743,9 @@ void xhci_setup_no_streams_ep_input_ctx(
 		struct xhci_virt_ep *ep)
 {
 	dma_addr_t addr;
-	ep_ctx->ep_info &= cpu_to_le32(~(EP_MAXPSTREAMS_MASK | EP_HAS_LSA));
+	ep_ctx->ep_info &= XHCI_CPU_TO_LE32(xhci->snoswap,~(EP_MAXPSTREAMS_MASK | EP_HAS_LSA));
 	addr = xhci_trb_virt_to_dma(ep->ring->deq_seg, ep->ring->dequeue);
-	ep_ctx->deq  = cpu_to_le64(addr | ep->ring->cycle_state);
+	ep_ctx->deq  = XHCI_CPU_TO_LE64(xhci->snoswap,addr | ep->ring->cycle_state);
 }
 
 /* Frees all stream contexts associated with the endpoint,
@@ -983,11 +983,11 @@ int xhci_alloc_virt_device(struct xhci_h
 	dev->udev = udev;
 
 	/* Point to output device context in dcbaa. */
-	xhci->dcbaa->dev_context_ptrs[slot_id] = cpu_to_le64(dev->out_ctx->dma);
+	xhci->dcbaa->dev_context_ptrs[slot_id] = XHCI_CPU_TO_LE64(xhci->snoswap,dev->out_ctx->dma);
 	xhci_dbg(xhci, "Set slot id %d dcbaa entry %p to 0x%llx\n",
 		 slot_id,
 		 &xhci->dcbaa->dev_context_ptrs[slot_id],
-		 le64_to_cpu(xhci->dcbaa->dev_context_ptrs[slot_id]));
+		 XHCI_LE64_TO_CPU(xhci->snoswap,xhci->dcbaa->dev_context_ptrs[slot_id]));
 
 	return 1;
 fail:
@@ -1012,7 +1012,7 @@ void xhci_copy_ep0_dequeue_into_input_ct
 	 * configured device has reset, so all control transfers should have
 	 * been completed or cancelled before the reset.
 	 */
-	ep0_ctx->deq = cpu_to_le64(xhci_trb_virt_to_dma(ep_ring->enq_seg,
+	ep0_ctx->deq = XHCI_CPU_TO_LE64(xhci->snoswap,xhci_trb_virt_to_dma(ep_ring->enq_seg,
 							ep_ring->enqueue)
 				   | ep_ring->cycle_state);
 }
@@ -1066,19 +1066,19 @@ int xhci_setup_addressable_virt_dev(stru
 	slot_ctx = xhci_get_slot_ctx(xhci, dev->in_ctx);
 
 	/* 3) Only the control endpoint is valid - one endpoint context */
-	slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(1) | udev->route);
+	slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,LAST_CTX(1) | udev->route);
 	switch (udev->speed) {
 	case USB_SPEED_SUPER:
-		slot_ctx->dev_info |= cpu_to_le32(SLOT_SPEED_SS);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_SPEED_SS);
 		break;
 	case USB_SPEED_HIGH:
-		slot_ctx->dev_info |= cpu_to_le32(SLOT_SPEED_HS);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_SPEED_HS);
 		break;
 	case USB_SPEED_FULL:
-		slot_ctx->dev_info |= cpu_to_le32(SLOT_SPEED_FS);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_SPEED_FS);
 		break;
 	case USB_SPEED_LOW:
-		slot_ctx->dev_info |= cpu_to_le32(SLOT_SPEED_LS);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_SPEED_LS);
 		break;
 	case USB_SPEED_WIRELESS:
 		xhci_dbg(xhci, "FIXME xHCI doesn't support wireless speeds\n");
@@ -1092,7 +1092,7 @@ int xhci_setup_addressable_virt_dev(stru
 	port_num = xhci_find_real_port_number(xhci, udev);
 	if (!port_num)
 		return -EINVAL;
-	slot_ctx->dev_info2 |= cpu_to_le32(ROOT_HUB_PORT(port_num));
+	slot_ctx->dev_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,ROOT_HUB_PORT(port_num));
 	/* Set the port number in the virtual_device to the faked port number */
 	for (top_dev = udev; top_dev->parent && top_dev->parent->parent;
 			top_dev = top_dev->parent)
@@ -1134,31 +1134,31 @@ int xhci_setup_addressable_virt_dev(stru
 
 	/* Is this a LS/FS device under an external HS hub? */
 	if (udev->tt && udev->tt->hub->parent) {
-		slot_ctx->tt_info = cpu_to_le32(udev->tt->hub->slot_id |
+		slot_ctx->tt_info = XHCI_CPU_TO_LE32(xhci->snoswap,udev->tt->hub->slot_id |
 						(udev->ttport << 8));
 		if (udev->tt->multi)
-			slot_ctx->dev_info |= cpu_to_le32(DEV_MTT);
+			slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,DEV_MTT);
 	}
 	xhci_dbg(xhci, "udev->tt = %p\n", udev->tt);
 	xhci_dbg(xhci, "udev->ttport = 0x%x\n", udev->ttport);
 
 	/* Step 4 - ring already allocated */
 	/* Step 5 */
-	ep0_ctx->ep_info2 = cpu_to_le32(EP_TYPE(CTRL_EP));
+	ep0_ctx->ep_info2 = XHCI_CPU_TO_LE32(xhci->snoswap,EP_TYPE(CTRL_EP));
 	/*
 	 * XXX: Not sure about wireless USB devices.
 	 */
 	switch (udev->speed) {
 	case USB_SPEED_SUPER:
-		ep0_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(512));
+		ep0_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_PACKET(512));
 		break;
 	case USB_SPEED_HIGH:
 	/* USB core guesses at a 64-byte max packet first for FS devices */
 	case USB_SPEED_FULL:
-		ep0_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(64));
+		ep0_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_PACKET(64));
 		break;
 	case USB_SPEED_LOW:
-		ep0_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(8));
+		ep0_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_PACKET(8));
 		break;
 	case USB_SPEED_WIRELESS:
 		xhci_dbg(xhci, "FIXME xHCI doesn't support wireless speeds\n");
@@ -1169,9 +1169,9 @@ int xhci_setup_addressable_virt_dev(stru
 		BUG();
 	}
 	/* EP 0 can handle "burst" sizes of 1, so Max Burst Size field is 0 */
-	ep0_ctx->ep_info2 |= cpu_to_le32(MAX_BURST(0) | ERROR_COUNT(3));
+	ep0_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_BURST(0) | ERROR_COUNT(3));
 
-	ep0_ctx->deq = cpu_to_le64(dev->eps[0].ring->first_seg->dma |
+	ep0_ctx->deq = XHCI_CPU_TO_LE64(xhci->snoswap,dev->eps[0].ring->first_seg->dma |
 				   dev->eps[0].ring->cycle_state);
 
 	/* Steps 7 and 8 were done in xhci_alloc_virt_device() */
@@ -1364,7 +1364,7 @@ static u32 xhci_get_max_esit_payload(str
 		return 0;
 
 	if (udev->speed == USB_SPEED_SUPER)
-		return le16_to_cpu(ep->ss_ep_comp.wBytesPerInterval);
+		return le16_to_cpu(ep->ss_ep_comp.wBytesPerInterval);//ltq_h:always swap
 
 	max_packet = GET_MAX_PACKET(usb_endpoint_maxp(&ep->desc));
 	max_burst = (usb_endpoint_maxp(&ep->desc) & 0x1800) >> 11;
@@ -1409,9 +1409,9 @@ int xhci_endpoint_init(struct xhci_hcd *
 	}
 	virt_dev->eps[ep_index].skip = false;
 	ep_ring = virt_dev->eps[ep_index].new_ring;
-	ep_ctx->deq = cpu_to_le64(ep_ring->first_seg->dma | ep_ring->cycle_state);
+	ep_ctx->deq = XHCI_CPU_TO_LE64(xhci->snoswap,ep_ring->first_seg->dma | ep_ring->cycle_state);
 
-	ep_ctx->ep_info = cpu_to_le32(xhci_get_endpoint_interval(udev, ep)
+	ep_ctx->ep_info = XHCI_CPU_TO_LE32(xhci->snoswap,xhci_get_endpoint_interval(udev, ep)
 				      | EP_MULT(xhci_get_endpoint_mult(udev, ep)));
 
 	/* FIXME dig Mult and streams info out of ep companion desc */
@@ -1420,11 +1420,11 @@ int xhci_endpoint_init(struct xhci_hcd *
 	 * CErr shall be set to 0 for Isoch endpoints.
 	 */
 	if (!usb_endpoint_xfer_isoc(&ep->desc))
-		ep_ctx->ep_info2 = cpu_to_le32(ERROR_COUNT(3));
+		ep_ctx->ep_info2 = XHCI_CPU_TO_LE32(xhci->snoswap,ERROR_COUNT(3));
 	else
-		ep_ctx->ep_info2 = cpu_to_le32(ERROR_COUNT(0));
+		ep_ctx->ep_info2 = XHCI_CPU_TO_LE32(xhci->snoswap,ERROR_COUNT(0));
 
-	ep_ctx->ep_info2 |= cpu_to_le32(xhci_get_endpoint_type(udev, ep));
+	ep_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,xhci_get_endpoint_type(udev, ep));
 
 	/* Set the max packet size and max burst */
 	max_packet = GET_MAX_PACKET(usb_endpoint_maxp(&ep->desc));
@@ -1453,10 +1453,10 @@ int xhci_endpoint_init(struct xhci_hcd *
 	default:
 		BUG();
 	}
-	ep_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(max_packet) |
+	ep_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_PACKET(max_packet) |
 			MAX_BURST(max_burst));
 	max_esit_payload = xhci_get_max_esit_payload(xhci, udev, ep);
-	ep_ctx->tx_info = cpu_to_le32(MAX_ESIT_PAYLOAD_FOR_EP(max_esit_payload));
+	ep_ctx->tx_info = XHCI_CPU_TO_LE32(xhci->snoswap,MAX_ESIT_PAYLOAD_FOR_EP(max_esit_payload));
 
 	/*
 	 * XXX no idea how to calculate the average TRB buffer length for bulk
@@ -1533,15 +1533,15 @@ void xhci_update_bw_info(struct xhci_hcd
 		 * endpoints should be harmless because the info will never be
 		 * set in the first place.
 		 */
-		if (!EP_IS_ADDED(ctrl_ctx, i) && EP_IS_DROPPED(ctrl_ctx, i)) {
+		if (!EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i) && EP_IS_DROPPED(xhci->snoswap, ctrl_ctx, i)) {
 			/* Dropped endpoint */
 			xhci_clear_endpoint_bw_info(bw_info);
 			continue;
 		}
 
-		if (EP_IS_ADDED(ctrl_ctx, i)) {
+		if (EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i)) {
 			ep_ctx = xhci_get_ep_ctx(xhci, in_ctx, i);
-			ep_type = CTX_TO_EP_TYPE(le32_to_cpu(ep_ctx->ep_info2));
+			ep_type = CTX_TO_EP_TYPE(XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info2));
 
 			/* Ignore non-periodic endpoints */
 			if (ep_type != ISOC_OUT_EP && ep_type != INT_OUT_EP &&
@@ -1551,20 +1551,20 @@ void xhci_update_bw_info(struct xhci_hcd
 
 			/* Added or changed endpoint */
 			bw_info->ep_interval = CTX_TO_EP_INTERVAL(
-					le32_to_cpu(ep_ctx->ep_info));
+					XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info));
 			/* Number of packets and mult are zero-based in the
 			 * input context, but we want one-based for the
 			 * interval table.
 			 */
 			bw_info->mult = CTX_TO_EP_MULT(
-					le32_to_cpu(ep_ctx->ep_info)) + 1;
+					XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info)) + 1;
 			bw_info->num_packets = CTX_TO_MAX_BURST(
-					le32_to_cpu(ep_ctx->ep_info2)) + 1;
+					XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info2)) + 1;
 			bw_info->max_packet_size = MAX_PACKET_DECODED(
-					le32_to_cpu(ep_ctx->ep_info2));
+					XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info2));
 			bw_info->type = ep_type;
 			bw_info->max_esit_payload = CTX_TO_MAX_ESIT_PAYLOAD(
-					le32_to_cpu(ep_ctx->tx_info));
+					XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->tx_info));
 		}
 	}
 }
@@ -1643,7 +1643,7 @@ static int scratchpad_alloc(struct xhci_
 	if (!xhci->scratchpad->sp_dma_buffers)
 		goto fail_sp4;
 
-	xhci->dcbaa->dev_context_ptrs[0] = cpu_to_le64(xhci->scratchpad->sp_dma);
+	xhci->dcbaa->dev_context_ptrs[0] = XHCI_CPU_TO_LE64(xhci->snoswap,xhci->scratchpad->sp_dma);
 	for (i = 0; i < num_sp; i++) {
 		dma_addr_t dma;
 		void *buf = dma_alloc_coherent(dev, xhci->page_size, &dma,
@@ -2069,7 +2069,7 @@ static void xhci_add_in_port(struct xhci
 			addr, port_offset, port_count, major_revision);
 	/* Port count includes the current port offset */
 	if (port_offset == 0 || (port_offset + port_count - 1) > num_ports)
-		/* WTF? "Valid values are ‘1’ to MaxPorts" */
+		/* WTF? "Valid values are ????to MaxPorts" */
 		return;
 
 	/* Check the host's USB2 LPM capability */
@@ -2409,8 +2409,8 @@ int xhci_mem_init(struct xhci_hcd *xhci,
 	/* set ring base address and size for each segment table entry */
 	for (val = 0, seg = xhci->event_ring->first_seg; val < ERST_NUM_SEGS; val++) {
 		struct xhci_erst_entry *entry = &xhci->erst.entries[val];
-		entry->seg_addr = cpu_to_le64(seg->dma);
-		entry->seg_size = cpu_to_le32(TRBS_PER_SEGMENT);
+		entry->seg_addr = XHCI_CPU_TO_LE64(xhci->snoswap,seg->dma);
+		entry->seg_size = XHCI_CPU_TO_LE32(xhci->snoswap,TRBS_PER_SEGMENT);
 		entry->rsvd = 0;
 		seg = seg->next;
 	}
diff --git a/drivers/usb/host/xhci-ring.c b/drivers/usb/host/xhci-ring.c
--- a/drivers/usb/host/xhci-ring.c
+++ b/drivers/usb/host/xhci-ring.c
@@ -100,7 +100,7 @@ static bool last_trb_on_last_seg(struct 
 		return (trb == &seg->trbs[TRBS_PER_SEGMENT]) &&
 			(seg->next == xhci->event_ring->first_seg);
 	else
-		return le32_to_cpu(trb->link.control) & LINK_TOGGLE;
+		return XHCI_LE32_TO_CPU(xhci->snoswap,trb->link.control) & LINK_TOGGLE;
 }
 
 /* Is this TRB a link TRB or was the last TRB the last TRB in this event ring
@@ -113,13 +113,13 @@ static int last_trb(struct xhci_hcd *xhc
 	if (ring == xhci->event_ring)
 		return trb == &seg->trbs[TRBS_PER_SEGMENT];
 	else
-		return TRB_TYPE_LINK_LE32(trb->link.control);
+		return TRB_TYPE_LINK_LE32(xhci->snoswap, trb->link.control);
 }
 
-static int enqueue_is_link_trb(struct xhci_ring *ring)
+static int enqueue_is_link_trb(struct xhci_hcd *xhci, struct xhci_ring *ring)
 {
 	struct xhci_link_trb *link = &ring->enqueue->link;
-	return TRB_TYPE_LINK_LE32(link->control);
+	return TRB_TYPE_LINK_LE32(xhci->snoswap, link->control);
 }
 
 union xhci_trb *xhci_find_next_enqueue(struct xhci_ring *ring)
@@ -127,7 +127,7 @@ union xhci_trb *xhci_find_next_enqueue(s
 	/* Enqueue pointer can be left pointing to the link TRB,
 	 * we must handle that
 	 */
-	if (TRB_TYPE_LINK_LE32(ring->enqueue->link.control))
+	if (TRB_TYPE_LINK_LE32(ring->snoswap,ring->enqueue->link.control))
 		return ring->enq_seg->next->trbs;
 	return ring->enqueue;
 }
@@ -213,7 +213,7 @@ static void inc_enq(struct xhci_hcd *xhc
 	union xhci_trb *next;
 	unsigned long long addr;
 
-	chain = le32_to_cpu(ring->enqueue->generic.field[3]) & TRB_CHAIN;
+	chain = XHCI_LE32_TO_CPU(xhci->snoswap,ring->enqueue->generic.field[3]) & TRB_CHAIN;
 	/* If this is not event ring, there is one less usable TRB */
 	if (ring->type != TYPE_EVENT &&
 			!last_trb(xhci, ring, ring->enq_seg, ring->enqueue))
@@ -246,13 +246,13 @@ static void inc_enq(struct xhci_hcd *xhc
 					(xhci->quirks & XHCI_AMD_0x96_HOST))
 						&& !xhci_link_trb_quirk(xhci)) {
 				next->link.control &=
-					cpu_to_le32(~TRB_CHAIN);
+					XHCI_CPU_TO_LE32(xhci->snoswap,~TRB_CHAIN);
 				next->link.control |=
-					cpu_to_le32(chain);
+					XHCI_CPU_TO_LE32(xhci->snoswap,chain);
 			}
 			/* Give this link TRB to the hardware */
 			wmb();
-			next->link.control ^= cpu_to_le32(TRB_CYCLE);
+			next->link.control ^= XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CYCLE);
 
 			/* Toggle the cycle bit after the last ring segment. */
 			if (last_trb_on_last_seg(xhci, ring, ring->enq_seg, next)) {
@@ -464,6 +464,7 @@ static void ring_doorbell_for_active_rin
  * bit set, then we will toggle the value pointed at by cycle_state.
  */
 static struct xhci_segment *find_trb_seg(
+		struct xhci_hcd *xhci,
 		struct xhci_segment *start_seg,
 		union xhci_trb	*trb, int *cycle_state)
 {
@@ -473,7 +474,7 @@ static struct xhci_segment *find_trb_seg
 	while (cur_seg->trbs > trb ||
 			&cur_seg->trbs[TRBS_PER_SEGMENT - 1] < trb) {
 		generic_trb = &cur_seg->trbs[TRBS_PER_SEGMENT - 1].generic;
-		if (generic_trb->field[3] & cpu_to_le32(LINK_TOGGLE))
+		if (generic_trb->field[3] & XHCI_CPU_TO_LE32(xhci->snoswap,LINK_TOGGLE))
 			*cycle_state ^= 0x1;
 		cur_seg = cur_seg->next;
 		if (cur_seg == start_seg)
@@ -566,7 +567,8 @@ void xhci_find_new_dequeue_state(struct 
 	}
 	state->new_cycle_state = 0;
 	xhci_dbg(xhci, "Finding segment containing stopped TRB.\n");
-	state->new_deq_seg = find_trb_seg(cur_td->start_seg,
+	state->new_deq_seg = find_trb_seg(xhci,
+			cur_td->start_seg,
 			dev->eps[ep_index].stopped_trb,
 			&state->new_cycle_state);
 	if (!state->new_deq_seg) {
@@ -577,11 +579,12 @@ void xhci_find_new_dequeue_state(struct 
 	/* Dig out the cycle state saved by the xHC during the stop ep cmd */
 	xhci_dbg(xhci, "Finding endpoint context\n");
 	ep_ctx = xhci_get_ep_ctx(xhci, dev->out_ctx, ep_index);
-	state->new_cycle_state = 0x1 & le64_to_cpu(ep_ctx->deq);
+	state->new_cycle_state = 0x1 & XHCI_LE64_TO_CPU(xhci->snoswap,ep_ctx->deq);
 
 	state->new_deq_ptr = cur_td->last_trb;
 	xhci_dbg(xhci, "Finding segment containing last TRB in TD.\n");
-	state->new_deq_seg = find_trb_seg(state->new_deq_seg,
+	state->new_deq_seg = find_trb_seg(xhci,
+			state->new_deq_seg,
 			state->new_deq_ptr,
 			&state->new_cycle_state);
 	if (!state->new_deq_seg) {
@@ -590,8 +593,8 @@ void xhci_find_new_dequeue_state(struct 
 	}
 
 	trb = &state->new_deq_ptr->generic;
-	if (TRB_TYPE_LINK_LE32(trb->field[3]) &&
-	    (trb->field[3] & cpu_to_le32(LINK_TOGGLE)))
+	if (TRB_TYPE_LINK_LE32(xhci->snoswap, trb->field[3]) &&
+	    (trb->field[3] & XHCI_CPU_TO_LE32(xhci->snoswap,LINK_TOGGLE)))
 		state->new_cycle_state ^= 0x1;
 	next_trb(xhci, ep_ring, &state->new_deq_seg, &state->new_deq_ptr);
 
@@ -630,17 +633,17 @@ static void td_to_noop(struct xhci_hcd *
 	for (cur_seg = cur_td->start_seg, cur_trb = cur_td->first_trb;
 			true;
 			next_trb(xhci, ep_ring, &cur_seg, &cur_trb)) {
-		if (TRB_TYPE_LINK_LE32(cur_trb->generic.field[3])) {
+		if (TRB_TYPE_LINK_LE32(xhci->snoswap, cur_trb->generic.field[3])) {
 			/* Unchain any chained Link TRBs, but
 			 * leave the pointers intact.
 			 */
-			cur_trb->generic.field[3] &= cpu_to_le32(~TRB_CHAIN);
+			cur_trb->generic.field[3] &= XHCI_CPU_TO_LE32(xhci->snoswap,~TRB_CHAIN);
 			/* Flip the cycle bit (link TRBs can't be the first
 			 * or last TRB).
 			 */
 			if (flip_cycle)
 				cur_trb->generic.field[3] ^=
-					cpu_to_le32(TRB_CYCLE);
+					XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CYCLE);
 			xhci_dbg(xhci, "Cancel (unchain) link TRB\n");
 			xhci_dbg(xhci, "Address = %p (0x%llx dma); "
 					"in seg %p (0x%llx dma)\n",
@@ -653,13 +656,13 @@ static void td_to_noop(struct xhci_hcd *
 			cur_trb->generic.field[1] = 0;
 			cur_trb->generic.field[2] = 0;
 			/* Preserve only the cycle bit of this TRB */
-			cur_trb->generic.field[3] &= cpu_to_le32(TRB_CYCLE);
+			cur_trb->generic.field[3] &= XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CYCLE);
 			/* Flip the cycle bit except on the first or last TRB */
 			if (flip_cycle && cur_trb != cur_td->first_trb &&
 					cur_trb != cur_td->last_trb)
 				cur_trb->generic.field[3] ^=
-					cpu_to_le32(TRB_CYCLE);
-			cur_trb->generic.field[3] |= cpu_to_le32(
+					XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CYCLE);
+			cur_trb->generic.field[3] |= XHCI_CPU_TO_LE32(xhci->snoswap,
 				TRB_TYPE(TRB_TR_NOOP));
 			xhci_dbg(xhci, "TRB to noop at offset 0x%llx\n",
 					(unsigned long long)
@@ -769,9 +772,9 @@ static void handle_stopped_endpoint(stru
 	struct xhci_dequeue_state deq_state;
 
 	if (unlikely(TRB_TO_SUSPEND_PORT(
-			     le32_to_cpu(xhci->cmd_ring->dequeue->generic.field[3])))) {
+			     XHCI_LE32_TO_CPU(xhci->snoswap,xhci->cmd_ring->dequeue->generic.field[3])))) {
 		slot_id = TRB_TO_SLOT_ID(
-			le32_to_cpu(xhci->cmd_ring->dequeue->generic.field[3]));
+			XHCI_LE32_TO_CPU(xhci->snoswap,xhci->cmd_ring->dequeue->generic.field[3]));
 		virt_dev = xhci->devs[slot_id];
 		if (virt_dev)
 			handle_cmd_in_cmd_wait_list(xhci, virt_dev,
@@ -784,8 +787,8 @@ static void handle_stopped_endpoint(stru
 	}
 
 	memset(&deq_state, 0, sizeof(deq_state));
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(trb->generic.field[3]));
-	ep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
+	ep_index = TRB_TO_EP_INDEX(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
 	ep = &xhci->devs[slot_id]->eps[ep_index];
 
 	if (list_empty(&ep->cancelled_td_list)) {
@@ -1072,9 +1075,9 @@ static void handle_set_deq_completion(st
 	struct xhci_ep_ctx *ep_ctx;
 	struct xhci_slot_ctx *slot_ctx;
 
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(trb->generic.field[3]));
-	ep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));
-	stream_id = TRB_TO_STREAM_ID(le32_to_cpu(trb->generic.field[2]));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
+	ep_index = TRB_TO_EP_INDEX(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
+	stream_id = TRB_TO_STREAM_ID(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[2]));
 	dev = xhci->devs[slot_id];
 
 	ep_ring = xhci_stream_id_to_ring(dev, ep_index, stream_id);
@@ -1090,11 +1093,11 @@ static void handle_set_deq_completion(st
 	ep_ctx = xhci_get_ep_ctx(xhci, dev->out_ctx, ep_index);
 	slot_ctx = xhci_get_slot_ctx(xhci, dev->out_ctx);
 
-	if (GET_COMP_CODE(le32_to_cpu(event->status)) != COMP_SUCCESS) {
+	if (GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)) != COMP_SUCCESS) {
 		unsigned int ep_state;
 		unsigned int slot_state;
 
-		switch (GET_COMP_CODE(le32_to_cpu(event->status))) {
+		switch (GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status))) {
 		case COMP_TRB_ERR:
 			xhci_warn(xhci, "WARN Set TR Deq Ptr cmd invalid because "
 					"of stream ID configuration\n");
@@ -1102,9 +1105,9 @@ static void handle_set_deq_completion(st
 		case COMP_CTX_STATE:
 			xhci_warn(xhci, "WARN Set TR Deq Ptr cmd failed due "
 					"to incorrect slot or ep state.\n");
-			ep_state = le32_to_cpu(ep_ctx->ep_info);
+			ep_state = XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info);
 			ep_state &= EP_STATE_MASK;
-			slot_state = le32_to_cpu(slot_ctx->dev_state);
+			slot_state = XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_state);
 			slot_state = GET_SLOT_STATE(slot_state);
 			xhci_dbg(xhci, "Slot state = %u, EP state = %u\n",
 					slot_state, ep_state);
@@ -1116,7 +1119,7 @@ static void handle_set_deq_completion(st
 		default:
 			xhci_warn(xhci, "WARN Set TR Deq Ptr cmd with unknown "
 					"completion code of %u.\n",
-				  GET_COMP_CODE(le32_to_cpu(event->status)));
+				  GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)));
 			break;
 		}
 		/* OK what do we do now?  The endpoint state is hosed, and we
@@ -1127,10 +1130,10 @@ static void handle_set_deq_completion(st
 		 */
 	} else {
 		xhci_dbg(xhci, "Successful Set TR Deq Ptr cmd, deq = @%08llx\n",
-			 le64_to_cpu(ep_ctx->deq));
+			 XHCI_LE64_TO_CPU(xhci->snoswap,ep_ctx->deq));
 		if (xhci_trb_virt_to_dma(dev->eps[ep_index].queued_deq_seg,
 					 dev->eps[ep_index].queued_deq_ptr) ==
-		    (le64_to_cpu(ep_ctx->deq) & ~(EP_CTX_CYCLE_MASK))) {
+		    (XHCI_LE64_TO_CPU(xhci->snoswap,ep_ctx->deq) & ~(EP_CTX_CYCLE_MASK))) {
 			/* Update the ring's dequeue segment and dequeue pointer
 			 * to reflect the new position.
 			 */
@@ -1159,13 +1162,13 @@ static void handle_reset_ep_completion(s
 	int slot_id;
 	unsigned int ep_index;
 
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(trb->generic.field[3]));
-	ep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
+	ep_index = TRB_TO_EP_INDEX(XHCI_LE32_TO_CPU(xhci->snoswap,trb->generic.field[3]));
 	/* This command will only fail if the endpoint wasn't halted,
 	 * but we don't care.
 	 */
 	xhci_dbg(xhci, "Ignoring reset ep completion code of %u\n",
-		 GET_COMP_CODE(le32_to_cpu(event->status)));
+		 GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)));
 
 	/* HW with the reset endpoint quirk needs to have a configure endpoint
 	 * command complete before the endpoint can be used.  Queue that here
@@ -1216,7 +1219,7 @@ static int handle_cmd_in_cmd_wait_list(s
 		return 0;
 
 	xhci_complete_cmd_in_cmd_wait_list(xhci, command,
-			GET_COMP_CODE(le32_to_cpu(event->status)));
+			GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)));
 	return 1;
 }
 
@@ -1238,7 +1241,8 @@ static void xhci_cmd_to_noop(struct xhci
 		return;
 
 	/* find the current segment of command ring */
-	cur_seg = find_trb_seg(xhci->cmd_ring->first_seg,
+	cur_seg = find_trb_seg(xhci,
+			xhci->cmd_ring->first_seg,
 			xhci->cmd_ring->dequeue, &cycle_state);
 
 	if (!cur_seg) {
@@ -1257,7 +1261,7 @@ static void xhci_cmd_to_noop(struct xhci
 			cmd_trb != xhci->cmd_ring->enqueue;
 			next_trb(xhci, xhci->cmd_ring, &cur_seg, &cmd_trb)) {
 		/* If the trb is link trb, continue */
-		if (TRB_TYPE_LINK_LE32(cmd_trb->generic.field[3]))
+		if (TRB_TYPE_LINK_LE32(xhci->snoswap, cmd_trb->generic.field[3]))
 			continue;
 
 		if (cur_cd->cmd_trb == cmd_trb) {
@@ -1270,14 +1274,14 @@ static void xhci_cmd_to_noop(struct xhci
 					cur_cd->command, COMP_CMD_STOP);
 
 			/* get cycle state from the origin command trb */
-			cycle_state = le32_to_cpu(cmd_trb->generic.field[3])
+			cycle_state = XHCI_LE32_TO_CPU(xhci->snoswap,cmd_trb->generic.field[3])
 				& TRB_CYCLE;
 
 			/* modify the command trb to NO OP command */
 			cmd_trb->generic.field[0] = 0;
 			cmd_trb->generic.field[1] = 0;
 			cmd_trb->generic.field[2] = 0;
-			cmd_trb->generic.field[3] = cpu_to_le32(
+			cmd_trb->generic.field[3] = XHCI_CPU_TO_LE32(xhci->snoswap,
 					TRB_TYPE(TRB_CMD_NOOP) | cycle_state);
 			break;
 		}
@@ -1367,7 +1371,7 @@ static int handle_stopped_cmd_ring(struc
 static void handle_cmd_completion(struct xhci_hcd *xhci,
 		struct xhci_event_cmd *event)
 {
-	int slot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));
+	int slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 	u64 cmd_dma;
 	dma_addr_t cmd_dequeue_dma;
 	struct xhci_input_control_ctx *ctrl_ctx;
@@ -1376,7 +1380,7 @@ static void handle_cmd_completion(struct
 	struct xhci_ring *ep_ring;
 	unsigned int ep_state;
 
-	cmd_dma = le64_to_cpu(event->cmd_trb);
+	cmd_dma = XHCI_LE64_TO_CPU(xhci->snoswap,event->cmd_trb);
 	cmd_dequeue_dma = xhci_trb_virt_to_dma(xhci->cmd_ring->deq_seg,
 			xhci->cmd_ring->dequeue);
 	/* Is the command ring deq ptr out of sync with the deq seg ptr? */
@@ -1390,8 +1394,8 @@ static void handle_cmd_completion(struct
 		return;
 	}
 
-	if ((GET_COMP_CODE(le32_to_cpu(event->status)) == COMP_CMD_ABORT) ||
-		(GET_COMP_CODE(le32_to_cpu(event->status)) == COMP_CMD_STOP)) {
+	if ((GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)) == COMP_CMD_ABORT) ||
+		(GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)) == COMP_CMD_STOP)) {
 		/* If the return value is 0, we think the trb pointed by
 		 * command ring dequeue pointer is a good trb. The good
 		 * trb means we don't want to cancel the trb, but it have
@@ -1444,7 +1448,7 @@ static void handle_cmd_completion(struct
 		ctrl_ctx = xhci_get_input_control_ctx(xhci,
 				virt_dev->in_ctx);
 		/* Input ctx add_flags are the endpoint index plus one */
-		ep_index = xhci_last_valid_endpoint(le32_to_cpu(ctrl_ctx->add_flags)) - 1;
+		ep_index = xhci_last_valid_endpoint(XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags)) - 1;
 		/* A usb_set_interface() call directly after clearing a halted
 		 * condition may race on this quirky hardware.  Not worth
 		 * worrying about, since this is prototype hardware.  Not sure
@@ -1453,8 +1457,8 @@ static void handle_cmd_completion(struct
 		 */
 		if (xhci->quirks & XHCI_RESET_EP_QUIRK &&
 				ep_index != (unsigned int) -1 &&
-		    le32_to_cpu(ctrl_ctx->add_flags) - SLOT_FLAG ==
-		    le32_to_cpu(ctrl_ctx->drop_flags)) {
+		    XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags) - SLOT_FLAG ==
+		    XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags)) {
 			ep_ring = xhci->devs[slot_id]->eps[ep_index].ring;
 			ep_state = xhci->devs[slot_id]->eps[ep_index].ep_state;
 			if (!(ep_state & EP_HALTED))
@@ -1471,18 +1475,18 @@ static void handle_cmd_completion(struct
 bandwidth_change:
 		xhci_dbg(xhci, "Completed config ep cmd\n");
 		xhci->devs[slot_id]->cmd_status =
-			GET_COMP_CODE(le32_to_cpu(event->status));
+			GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status));
 		complete(&xhci->devs[slot_id]->cmd_completion);
 		break;
 	case TRB_TYPE(TRB_EVAL_CONTEXT):
 		virt_dev = xhci->devs[slot_id];
 		if (handle_cmd_in_cmd_wait_list(xhci, virt_dev, event))
 			break;
-		xhci->devs[slot_id]->cmd_status = GET_COMP_CODE(le32_to_cpu(event->status));
+		xhci->devs[slot_id]->cmd_status = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status));
 		complete(&xhci->devs[slot_id]->cmd_completion);
 		break;
 	case TRB_TYPE(TRB_ADDR_DEV):
-		xhci->devs[slot_id]->cmd_status = GET_COMP_CODE(le32_to_cpu(event->status));
+		xhci->devs[slot_id]->cmd_status = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->status));
 		complete(&xhci->addr_dev);
 		break;
 	case TRB_TYPE(TRB_STOP_RING):
@@ -1499,7 +1503,7 @@ bandwidth_change:
 	case TRB_TYPE(TRB_RESET_DEV):
 		xhci_dbg(xhci, "Completed reset device command.\n");
 		slot_id = TRB_TO_SLOT_ID(
-			le32_to_cpu(xhci->cmd_ring->dequeue->generic.field[3]));
+			XHCI_LE32_TO_CPU(xhci->snoswap,xhci->cmd_ring->dequeue->generic.field[3]));
 		virt_dev = xhci->devs[slot_id];
 		if (virt_dev)
 			handle_cmd_in_cmd_wait_list(xhci, virt_dev, event);
@@ -1513,8 +1517,8 @@ bandwidth_change:
 			break;
 		}
 		xhci_dbg(xhci, "NEC firmware version %2x.%02x\n",
-			 NEC_FW_MAJOR(le32_to_cpu(event->status)),
-			 NEC_FW_MINOR(le32_to_cpu(event->status)));
+			 NEC_FW_MAJOR(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)),
+			 NEC_FW_MINOR(XHCI_LE32_TO_CPU(xhci->snoswap,event->status)));
 		break;
 	default:
 		/* Skip over unknown commands on the event ring */
@@ -1529,7 +1533,7 @@ static void handle_vendor_event(struct x
 {
 	u32 trb_type;
 
-	trb_type = TRB_FIELD_TO_TYPE(le32_to_cpu(event->generic.field[3]));
+	trb_type = TRB_FIELD_TO_TYPE(XHCI_LE32_TO_CPU(xhci->snoswap,event->generic.field[3]));
 	xhci_dbg(xhci, "Vendor specific event TRB type = %u\n", trb_type);
 	if (trb_type == TRB_NEC_CMD_COMP && (xhci->quirks & XHCI_NEC_HOST))
 		handle_cmd_completion(xhci, &event->event_cmd);
@@ -1608,11 +1612,11 @@ static void handle_port_status(struct xh
 	bool bogus_port_status = false;
 
 	/* Port status change events always have a successful completion code */
-	if (GET_COMP_CODE(le32_to_cpu(event->generic.field[2])) != COMP_SUCCESS) {
+	if (GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->generic.field[2])) != COMP_SUCCESS) {
 		xhci_warn(xhci, "WARN: xHC returned failed port status event\n");
 		xhci->error_bitmask |= 1 << 8;
 	}
-	port_id = GET_PORT_ID(le32_to_cpu(event->generic.field[0]));
+	port_id = GET_PORT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->generic.field[0]));
 	xhci_dbg(xhci, "Port Status Change Event for port %d\n", port_id);
 
 	max_ports = HCS_MAX_PORTS(xhci->hcs_params1);
@@ -1861,8 +1865,8 @@ static int xhci_requires_manual_halt_cle
 		 * endpoint anyway.  Check if a babble halted the
 		 * endpoint.
 		 */
-		if ((ep_ctx->ep_info & cpu_to_le32(EP_STATE_MASK)) ==
-		    cpu_to_le32(EP_STATE_HALTED))
+		if ((ep_ctx->ep_info & XHCI_CPU_TO_LE32(xhci->snoswap,EP_STATE_MASK)) ==
+		    XHCI_CPU_TO_LE32(xhci->snoswap,EP_STATE_HALTED))
 			return 1;
 
 	return 0;
@@ -1900,12 +1904,12 @@ static int finish_td(struct xhci_hcd *xh
 	struct urb_priv	*urb_priv;
 	u32 trb_comp_code;
 
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 	xdev = xhci->devs[slot_id];
-	ep_index = TRB_TO_EP_ID(le32_to_cpu(event->flags)) - 1;
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
+	ep_index = TRB_TO_EP_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)) - 1;
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
 	ep_ctx = xhci_get_ep_ctx(xhci, xdev->out_ctx, ep_index);
-	trb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));
+	trb_comp_code = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 
 	if (skip)
 		goto td_cleanup;
@@ -2008,12 +2012,12 @@ static int process_ctrl_td(struct xhci_h
 	struct xhci_ep_ctx *ep_ctx;
 	u32 trb_comp_code;
 
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 	xdev = xhci->devs[slot_id];
-	ep_index = TRB_TO_EP_ID(le32_to_cpu(event->flags)) - 1;
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
+	ep_index = TRB_TO_EP_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)) - 1;
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
 	ep_ctx = xhci_get_ep_ctx(xhci, xdev->out_ctx, ep_index);
-	trb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));
+	trb_comp_code = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 
 	switch (trb_comp_code) {
 	case COMP_SUCCESS:
@@ -2052,7 +2056,7 @@ static int process_ctrl_td(struct xhci_h
 				event_trb != td->last_trb)
 			td->urb->actual_length =
 				td->urb->transfer_buffer_length -
-				EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));
+				EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 		else
 			td->urb->actual_length = 0;
 
@@ -2090,7 +2094,7 @@ static int process_ctrl_td(struct xhci_h
 			td->urb_length_set = true;
 			td->urb->actual_length =
 				td->urb->transfer_buffer_length -
-				EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));
+				EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 			xhci_dbg(xhci, "Waiting for status "
 					"stage event\n");
 			return 0;
@@ -2117,8 +2121,8 @@ static int process_isoc_td(struct xhci_h
 	u32 trb_comp_code;
 	bool skip_td = false;
 
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
-	trb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
+	trb_comp_code = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 	urb_priv = td->urb->hcpriv;
 	idx = urb_priv->td_cnt;
 	frame = &td->urb->iso_frame_desc[idx];
@@ -2126,7 +2130,7 @@ static int process_isoc_td(struct xhci_h
 	/* handle completion code */
 	switch (trb_comp_code) {
 	case COMP_SUCCESS:
-		if (EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)) == 0) {
+		if (EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)) == 0) {
 			frame->status = 0;
 			break;
 		}
@@ -2171,12 +2175,12 @@ static int process_isoc_td(struct xhci_h
 		for (cur_trb = ep_ring->dequeue,
 		     cur_seg = ep_ring->deq_seg; cur_trb != event_trb;
 		     next_trb(xhci, ep_ring, &cur_seg, &cur_trb)) {
-			if (!TRB_TYPE_NOOP_LE32(cur_trb->generic.field[3]) &&
-			    !TRB_TYPE_LINK_LE32(cur_trb->generic.field[3]))
-				len += TRB_LEN(le32_to_cpu(cur_trb->generic.field[2]));
+			if (!TRB_TYPE_NOOP_LE32(xhci->snoswap, cur_trb->generic.field[3]) &&
+			    !TRB_TYPE_LINK_LE32(xhci->snoswap, cur_trb->generic.field[3]))
+				len += TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,cur_trb->generic.field[2]));
 		}
-		len += TRB_LEN(le32_to_cpu(cur_trb->generic.field[2])) -
-			EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));
+		len += TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,cur_trb->generic.field[2])) -
+			EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 
 		if (trb_comp_code != COMP_STOP_INVAL) {
 			frame->actual_length = len;
@@ -2196,7 +2200,7 @@ static int skip_isoc_td(struct xhci_hcd 
 	struct usb_iso_packet_descriptor *frame;
 	int idx;
 
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
 	urb_priv = td->urb->hcpriv;
 	idx = urb_priv->td_cnt;
 	frame = &td->urb->iso_frame_desc[idx];
@@ -2227,14 +2231,14 @@ static int process_bulk_intr_td(struct x
 	struct xhci_segment *cur_seg;
 	u32 trb_comp_code;
 
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
-	trb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
+	trb_comp_code = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 
 	switch (trb_comp_code) {
 	case COMP_SUCCESS:
 		/* Double check that the HW transferred everything. */
 		if (event_trb != td->last_trb ||
-		    EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)) != 0) {
+		    EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)) != 0) {
 			xhci_warn(xhci, "WARN Successful completion "
 					"on short TX\n");
 			if (td->urb->transfer_flags & URB_SHORT_NOT_OK)
@@ -2262,18 +2266,18 @@ static int process_bulk_intr_td(struct x
 				"%d bytes untransferred\n",
 				td->urb->ep->desc.bEndpointAddress,
 				td->urb->transfer_buffer_length,
-				EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)));
+				EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)));
 	/* Fast path - was this the last TRB in the TD for this URB? */
 	if (event_trb == td->last_trb) {
-		if (EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)) != 0) {
+		if (EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)) != 0) {
 			td->urb->actual_length =
 				td->urb->transfer_buffer_length -
-				EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));
+				EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 			if (td->urb->transfer_buffer_length <
 					td->urb->actual_length) {
 				xhci_warn(xhci, "HC gave bad length "
 						"of %d bytes left\n",
-					  EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)));
+					  EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)));
 				td->urb->actual_length = 0;
 				if (td->urb->transfer_flags & URB_SHORT_NOT_OK)
 					*status = -EREMOTEIO;
@@ -2304,18 +2308,18 @@ static int process_bulk_intr_td(struct x
 		for (cur_trb = ep_ring->dequeue, cur_seg = ep_ring->deq_seg;
 				cur_trb != event_trb;
 				next_trb(xhci, ep_ring, &cur_seg, &cur_trb)) {
-			if (!TRB_TYPE_NOOP_LE32(cur_trb->generic.field[3]) &&
-			    !TRB_TYPE_LINK_LE32(cur_trb->generic.field[3]))
+			if (!TRB_TYPE_NOOP_LE32(xhci->snoswap, cur_trb->generic.field[3]) &&
+			    !TRB_TYPE_LINK_LE32(xhci->snoswap, cur_trb->generic.field[3]))
 				td->urb->actual_length +=
-					TRB_LEN(le32_to_cpu(cur_trb->generic.field[2]));
+					TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,cur_trb->generic.field[2]));
 		}
 		/* If the ring didn't stop on a Link or No-op TRB, add
 		 * in the actual bytes transferred from the Normal TRB
 		 */
 		if (trb_comp_code != COMP_STOP_INVAL)
 			td->urb->actual_length +=
-				TRB_LEN(le32_to_cpu(cur_trb->generic.field[2])) -
-				EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));
+				TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,cur_trb->generic.field[2])) -
+				EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 	}
 
 	return finish_td(xhci, td, event_trb, event, ep, status, false);
@@ -2350,7 +2354,7 @@ static int handle_tx_event(struct xhci_h
 	int td_num = 0;
 	bool handling_skipped_tds = false;
 
-	slot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));
+	slot_id = TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 	xdev = xhci->devs[slot_id];
 	if (!xdev) {
 		xhci_err(xhci, "ERROR Transfer event pointed to bad slot\n");
@@ -2358,22 +2362,22 @@ static int handle_tx_event(struct xhci_h
 			 (unsigned long long) xhci_trb_virt_to_dma(
 				 xhci->event_ring->deq_seg,
 				 xhci->event_ring->dequeue),
-			 lower_32_bits(le64_to_cpu(event->buffer)),
-			 upper_32_bits(le64_to_cpu(event->buffer)),
-			 le32_to_cpu(event->transfer_len),
-			 le32_to_cpu(event->flags));
+			 lower_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer)),
+			 upper_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer)),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 		xhci_dbg(xhci, "Event ring:\n");
 		xhci_debug_segment(xhci, xhci->event_ring->deq_seg);
 		return -ENODEV;
 	}
 
 	/* Endpoint ID is 1 based, our index is zero based */
-	ep_index = TRB_TO_EP_ID(le32_to_cpu(event->flags)) - 1;
+	ep_index = TRB_TO_EP_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)) - 1;
 	ep = &xdev->eps[ep_index];
-	ep_ring = xhci_dma_to_transfer_ring(ep, le64_to_cpu(event->buffer));
+	ep_ring = xhci_dma_to_transfer_ring(ep, XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer));
 	ep_ctx = xhci_get_ep_ctx(xhci, xdev->out_ctx, ep_index);
 	if (!ep_ring ||
-	    (le32_to_cpu(ep_ctx->ep_info) & EP_STATE_MASK) ==
+	    (XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info) & EP_STATE_MASK) ==
 	    EP_STATE_DISABLED) {
 		xhci_err(xhci, "ERROR Transfer event for disabled endpoint "
 				"or incorrect stream ring\n");
@@ -2381,10 +2385,10 @@ static int handle_tx_event(struct xhci_h
 			 (unsigned long long) xhci_trb_virt_to_dma(
 				 xhci->event_ring->deq_seg,
 				 xhci->event_ring->dequeue),
-			 lower_32_bits(le64_to_cpu(event->buffer)),
-			 upper_32_bits(le64_to_cpu(event->buffer)),
-			 le32_to_cpu(event->transfer_len),
-			 le32_to_cpu(event->flags));
+			 lower_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer)),
+			 upper_32_bits(XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer)),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len),
+			 XHCI_LE32_TO_CPU(xhci->snoswap,event->flags));
 		xhci_dbg(xhci, "Event ring:\n");
 		xhci_debug_segment(xhci, xhci->event_ring->deq_seg);
 		return -ENODEV;
@@ -2396,15 +2400,15 @@ static int handle_tx_event(struct xhci_h
 			td_num++;
 	}
 
-	event_dma = le64_to_cpu(event->buffer);
-	trb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));
+	event_dma = XHCI_LE64_TO_CPU(xhci->snoswap,event->buffer);
+	trb_comp_code = GET_COMP_CODE(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len));
 	/* Look for common error cases */
 	switch (trb_comp_code) {
 	/* Skip codes that require special handling depending on
 	 * transfer type
 	 */
 	case COMP_SUCCESS:
-		if (EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)) == 0)
+		if (EVENT_TRB_LEN(XHCI_LE32_TO_CPU(xhci->snoswap,event->transfer_len)) == 0)
 			break;
 		if (xhci->quirks & XHCI_TRUST_TX_LENGTH)
 			trb_comp_code = COMP_SHORT_TX;
@@ -2457,7 +2461,7 @@ static int handle_tx_event(struct xhci_h
 		if (!list_empty(&ep_ring->td_list))
 			xhci_dbg(xhci, "Underrun Event for slot %d ep %d "
 					"still with TDs queued?\n",
-				 TRB_TO_SLOT_ID(le32_to_cpu(event->flags)),
+				 TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)),
 				 ep_index);
 		goto cleanup;
 	case COMP_OVERRUN:
@@ -2465,7 +2469,7 @@ static int handle_tx_event(struct xhci_h
 		if (!list_empty(&ep_ring->td_list))
 			xhci_dbg(xhci, "Overrun Event for slot %d ep %d "
 					"still with TDs queued?\n",
-				 TRB_TO_SLOT_ID(le32_to_cpu(event->flags)),
+				 TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)),
 				 ep_index);
 		goto cleanup;
 	case COMP_DEV_ERR:
@@ -2509,10 +2513,10 @@ static int handle_tx_event(struct xhci_h
 			if (!(trb_comp_code == COMP_STOP ||
 						trb_comp_code == COMP_STOP_INVAL)) {
 				xhci_warn(xhci, "WARN Event TRB for slot %d ep %d with no TDs queued?\n",
-						TRB_TO_SLOT_ID(le32_to_cpu(event->flags)),
+						TRB_TO_SLOT_ID(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags)),
 						ep_index);
 				xhci_dbg(xhci, "Event TRB with TRB type ID %u\n",
-						(le32_to_cpu(event->flags) &
+						(XHCI_LE32_TO_CPU(xhci->snoswap,event->flags) &
 						 TRB_TYPE_BITMASK)>>10);
 				xhci_print_trb_offsets(xhci, (union xhci_trb *) event);
 			}
@@ -2563,7 +2567,7 @@ static int handle_tx_event(struct xhci_h
 				 * successful event after a short transfer.
 				 * Ignore it.
 				 */
-				if ((xhci->quirks & XHCI_SPURIOUS_SUCCESS) && 
+				if ((xhci->quirks & XHCI_SPURIOUS_SUCCESS) &&
 						ep_ring->last_td_was_short) {
 					ep_ring->last_td_was_short = false;
 					ret = 0;
@@ -2597,7 +2601,7 @@ static int handle_tx_event(struct xhci_h
 		 * corresponding TD has been cancelled. Just ignore
 		 * the TD.
 		 */
-		if (TRB_TYPE_NOOP_LE32(event_trb->generic.field[3])) {
+		if (TRB_TYPE_NOOP_LE32(xhci->snoswap, event_trb->generic.field[3])) {
 			xhci_dbg(xhci,
 				 "event_trb is a no-op TRB. Skip it\n");
 			goto cleanup;
@@ -2696,7 +2700,7 @@ static int xhci_handle_event(struct xhci
 
 	event = xhci->event_ring->dequeue;
 	/* Does the HC or OS own the TRB? */
-	if ((le32_to_cpu(event->event_cmd.flags) & TRB_CYCLE) !=
+	if ((XHCI_LE32_TO_CPU(xhci->snoswap,event->event_cmd.flags) & TRB_CYCLE) !=
 	    xhci->event_ring->cycle_state) {
 		xhci->error_bitmask |= 1 << 2;
 		return 0;
@@ -2708,7 +2712,7 @@ static int xhci_handle_event(struct xhci
 	 */
 	rmb();
 	/* FIXME: Handle more event types. */
-	switch ((le32_to_cpu(event->event_cmd.flags) & TRB_TYPE_BITMASK)) {
+	switch ((XHCI_LE32_TO_CPU(xhci->snoswap,event->event_cmd.flags) & TRB_TYPE_BITMASK)) {
 	case TRB_TYPE(TRB_COMPLETION):
 		handle_cmd_completion(xhci, &event->event_cmd);
 		break;
@@ -2727,7 +2731,7 @@ static int xhci_handle_event(struct xhci
 		handle_device_notification(xhci, event);
 		break;
 	default:
-		if ((le32_to_cpu(event->event_cmd.flags) & TRB_TYPE_BITMASK) >=
+		if ((XHCI_LE32_TO_CPU(xhci->snoswap,event->event_cmd.flags) & TRB_TYPE_BITMASK) >=
 		    TRB_TYPE(48))
 			handle_vendor_event(xhci, event);
 		else
@@ -2864,10 +2868,10 @@ static void queue_trb(struct xhci_hcd *x
 	struct xhci_generic_trb *trb;
 
 	trb = &ring->enqueue->generic;
-	trb->field[0] = cpu_to_le32(field1);
-	trb->field[1] = cpu_to_le32(field2);
-	trb->field[2] = cpu_to_le32(field3);
-	trb->field[3] = cpu_to_le32(field4);
+	trb->field[0] = XHCI_CPU_TO_LE32(xhci->snoswap,field1);
+	trb->field[1] = XHCI_CPU_TO_LE32(xhci->snoswap,field2);
+	trb->field[2] = XHCI_CPU_TO_LE32(xhci->snoswap,field3);
+	trb->field[3] = XHCI_CPU_TO_LE32(xhci->snoswap,field4);
 	inc_enq(xhci, ring, more_trbs_coming);
 }
 
@@ -2927,7 +2931,7 @@ static int prepare_ring(struct xhci_hcd 
 		}
 	}
 
-	if (enqueue_is_link_trb(ep_ring)) {
+	if (enqueue_is_link_trb(xhci, ep_ring)) {
 		struct xhci_ring *ring = ep_ring;
 		union xhci_trb *next;
 
@@ -2940,12 +2944,12 @@ static int prepare_ring(struct xhci_hcd 
 			if (!xhci_link_trb_quirk(xhci) &&
 					!(ring->type == TYPE_ISOC &&
 					 (xhci->quirks & XHCI_AMD_0x96_HOST)))
-				next->link.control &= cpu_to_le32(~TRB_CHAIN);
+				next->link.control &= XHCI_CPU_TO_LE32(xhci->snoswap,~TRB_CHAIN);
 			else
-				next->link.control |= cpu_to_le32(TRB_CHAIN);
+				next->link.control |= XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CHAIN);
 
 			wmb();
-			next->link.control ^= cpu_to_le32(TRB_CYCLE);
+			next->link.control ^= XHCI_CPU_TO_LE32(xhci->snoswap,TRB_CYCLE);
 
 			/* Toggle the cycle bit after the last ring segment. */
 			if (last_trb_on_last_seg(xhci, ring, ring->enq_seg, next)) {
@@ -2983,7 +2987,7 @@ static int prepare_transfer(struct xhci_
 	}
 
 	ret = prepare_ring(xhci, ep_ring,
-			   le32_to_cpu(ep_ctx->ep_info) & EP_STATE_MASK,
+			   XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info) & EP_STATE_MASK,
 			   num_trbs, mem_flags);
 	if (ret)
 		return ret;
@@ -3070,9 +3074,9 @@ static void giveback_first_trb(struct xh
 	 */
 	wmb();
 	if (start_cycle)
-		start_trb->field[3] |= cpu_to_le32(start_cycle);
+		start_trb->field[3] |= XHCI_CPU_TO_LE32(xhci->snoswap,start_cycle);
 	else
-		start_trb->field[3] &= cpu_to_le32(~TRB_CYCLE);
+		start_trb->field[3] &= XHCI_CPU_TO_LE32(xhci->snoswap,~TRB_CYCLE);
 	xhci_ring_ep_doorbell(xhci, slot_id, ep_index, stream_id);
 }
 
@@ -3090,7 +3094,7 @@ int xhci_queue_intr_tx(struct xhci_hcd *
 	int xhci_interval;
 	int ep_interval;
 
-	xhci_interval = EP_INTERVAL_TO_UFRAMES(le32_to_cpu(ep_ctx->ep_info));
+	xhci_interval = EP_INTERVAL_TO_UFRAMES(XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info));
 	ep_interval = urb->interval;
 	/* Convert to microframes */
 	if (urb->dev->speed == USB_SPEED_LOW ||
@@ -3904,7 +3908,7 @@ int xhci_queue_isoc_tx_prepare(struct xh
 	/* Check the ring to guarantee there is enough room for the whole urb.
 	 * Do not insert any td of the urb to the ring if the check failed.
 	 */
-	ret = prepare_ring(xhci, ep_ring, le32_to_cpu(ep_ctx->ep_info) & EP_STATE_MASK,
+	ret = prepare_ring(xhci, ep_ring, XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info) & EP_STATE_MASK,
 			   num_trbs, mem_flags);
 	if (ret)
 		return ret;
@@ -3917,7 +3921,7 @@ int xhci_queue_isoc_tx_prepare(struct xh
 			urb->dev->speed == USB_SPEED_FULL)
 		urb->start_frame >>= 3;
 
-	xhci_interval = EP_INTERVAL_TO_UFRAMES(le32_to_cpu(ep_ctx->ep_info));
+	xhci_interval = EP_INTERVAL_TO_UFRAMES(XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info));
 	ep_interval = urb->interval;
 	/* Convert to microframes */
 	if (urb->dev->speed == USB_SPEED_LOW ||
diff --git a/drivers/usb/host/xhci.c b/drivers/usb/host/xhci.c
--- a/drivers/usb/host/xhci.c
+++ b/drivers/usb/host/xhci.c
@@ -854,7 +854,7 @@ static void xhci_clear_command_ring(stru
 		memset(seg->trbs, 0,
 			sizeof(union xhci_trb) * (TRBS_PER_SEGMENT - 1));
 		seg->trbs[TRBS_PER_SEGMENT - 1].link.control &=
-			cpu_to_le32(~TRB_CYCLE);
+			XHCI_CPU_TO_LE32(xhci->snoswap,~TRB_CYCLE);
 		seg = seg->next;
 	} while (seg != ring->deq_seg);
 
@@ -1228,7 +1228,7 @@ static int xhci_check_maxpacket(struct x
 
 	out_ctx = xhci->devs[slot_id]->out_ctx;
 	ep_ctx = xhci_get_ep_ctx(xhci, out_ctx, ep_index);
-	hw_max_packet_size = MAX_PACKET_DECODED(le32_to_cpu(ep_ctx->ep_info2));
+	hw_max_packet_size = MAX_PACKET_DECODED(XHCI_LE32_TO_CPU(xhci->snoswap,ep_ctx->ep_info2));
 	max_packet_size = usb_endpoint_maxp(&urb->dev->ep0.desc);
 	if (hw_max_packet_size != max_packet_size) {
 		xhci_dbg(xhci, "Max Packet Size for ep 0 changed.\n");
@@ -1243,15 +1243,15 @@ static int xhci_check_maxpacket(struct x
 				xhci->devs[slot_id]->out_ctx, ep_index);
 		in_ctx = xhci->devs[slot_id]->in_ctx;
 		ep_ctx = xhci_get_ep_ctx(xhci, in_ctx, ep_index);
-		ep_ctx->ep_info2 &= cpu_to_le32(~MAX_PACKET_MASK);
-		ep_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(max_packet_size));
+		ep_ctx->ep_info2 &= XHCI_CPU_TO_LE32(xhci->snoswap,~MAX_PACKET_MASK);
+		ep_ctx->ep_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,MAX_PACKET(max_packet_size));
 
 		/* Set up the input context flags for the command */
 		/* FIXME: This won't work if a non-default control endpoint
 		 * changes max packet sizes.
 		 */
 		ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
-		ctrl_ctx->add_flags = cpu_to_le32(EP0_FLAG);
+		ctrl_ctx->add_flags = XHCI_CPU_TO_LE32(xhci->snoswap,EP0_FLAG);
 		ctrl_ctx->drop_flags = 0;
 
 		xhci_dbg(xhci, "Slot %d input context\n", slot_id);
@@ -1265,7 +1265,7 @@ static int xhci_check_maxpacket(struct x
 		/* Clean up the input context for later use by bandwidth
 		 * functions.
 		 */
-		ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG);
+		ctrl_ctx->add_flags = XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG);
 	}
 	return ret;
 }
@@ -1622,30 +1622,30 @@ int xhci_drop_endpoint(struct usb_hcd *h
 	/* If the HC already knows the endpoint is disabled,
 	 * or the HCD has noted it is disabled, ignore this request
 	 */
-	if (((ep_ctx->ep_info & cpu_to_le32(EP_STATE_MASK)) ==
-	     cpu_to_le32(EP_STATE_DISABLED)) ||
-	    le32_to_cpu(ctrl_ctx->drop_flags) &
+	if (((ep_ctx->ep_info & XHCI_CPU_TO_LE32(xhci->snoswap,EP_STATE_MASK)) ==
+	     XHCI_CPU_TO_LE32(xhci->snoswap,EP_STATE_DISABLED)) ||
+	    XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags) &
 	    xhci_get_endpoint_flag(&ep->desc)) {
 		xhci_warn(xhci, "xHCI %s called with disabled ep %p\n",
 				__func__, ep);
 		return 0;
 	}
 
-	ctrl_ctx->drop_flags |= cpu_to_le32(drop_flag);
-	new_drop_flags = le32_to_cpu(ctrl_ctx->drop_flags);
-
-	ctrl_ctx->add_flags &= cpu_to_le32(~drop_flag);
-	new_add_flags = le32_to_cpu(ctrl_ctx->add_flags);
-
-	last_ctx = xhci_last_valid_endpoint(le32_to_cpu(ctrl_ctx->add_flags));
+	ctrl_ctx->drop_flags |= XHCI_CPU_TO_LE32(xhci->snoswap,drop_flag);
+	new_drop_flags = XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags);
+
+	ctrl_ctx->add_flags &= XHCI_CPU_TO_LE32(xhci->snoswap,~drop_flag);
+	new_add_flags = XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags);
+
+	last_ctx = xhci_last_valid_endpoint(XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags));
 	slot_ctx = xhci_get_slot_ctx(xhci, in_ctx);
 	/* Update the last valid endpoint context, if we deleted the last one */
-	if ((le32_to_cpu(slot_ctx->dev_info) & LAST_CTX_MASK) >
+	if ((XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info) & LAST_CTX_MASK) >
 	    LAST_CTX(last_ctx)) {
-		slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
-		slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(last_ctx));
+		slot_ctx->dev_info &= XHCI_CPU_TO_LE32(xhci->snoswap,~LAST_CTX_MASK);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,LAST_CTX(last_ctx));
 	}
-	new_slot_info = le32_to_cpu(slot_ctx->dev_info);
+	new_slot_info = XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info);
 
 	xhci_endpoint_zero(xhci, xhci->devs[udev->slot_id], ep);
 
@@ -1717,7 +1717,7 @@ int xhci_add_endpoint(struct usb_hcd *hc
 	 * to add it again without dropping it, reject the addition.
 	 */
 	if (virt_dev->eps[ep_index].ring &&
-			!(le32_to_cpu(ctrl_ctx->drop_flags) &
+			!(XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags) &
 				xhci_get_endpoint_flag(&ep->desc))) {
 		xhci_warn(xhci, "Trying to add endpoint 0x%x "
 				"without dropping it.\n",
@@ -1728,7 +1728,7 @@ int xhci_add_endpoint(struct usb_hcd *hc
 	/* If the HCD has already noted the endpoint is enabled,
 	 * ignore this request.
 	 */
-	if (le32_to_cpu(ctrl_ctx->add_flags) &
+	if (XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags) &
 	    xhci_get_endpoint_flag(&ep->desc)) {
 		xhci_warn(xhci, "xHCI %s called with enabled ep %p\n",
 				__func__, ep);
@@ -1746,8 +1746,8 @@ int xhci_add_endpoint(struct usb_hcd *hc
 		return -ENOMEM;
 	}
 
-	ctrl_ctx->add_flags |= cpu_to_le32(added_ctxs);
-	new_add_flags = le32_to_cpu(ctrl_ctx->add_flags);
+	ctrl_ctx->add_flags |= XHCI_CPU_TO_LE32(xhci->snoswap,added_ctxs);
+	new_add_flags = XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags);
 
 	/* If xhci_endpoint_disable() was called for this endpoint, but the
 	 * xHC hasn't been notified yet through the check_bandwidth() call,
@@ -1755,16 +1755,16 @@ int xhci_add_endpoint(struct usb_hcd *hc
 	 * descriptors.  We must drop and re-add this endpoint, so we leave the
 	 * drop flags alone.
 	 */
-	new_drop_flags = le32_to_cpu(ctrl_ctx->drop_flags);
+	new_drop_flags = XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags);
 
 	slot_ctx = xhci_get_slot_ctx(xhci, in_ctx);
 	/* Update the last valid endpoint context, if we just added one past */
-	if ((le32_to_cpu(slot_ctx->dev_info) & LAST_CTX_MASK) <
+	if ((XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info) & LAST_CTX_MASK) <
 	    LAST_CTX(last_ctx)) {
-		slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
-		slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(last_ctx));
+		slot_ctx->dev_info &= XHCI_CPU_TO_LE32(xhci->snoswap,~LAST_CTX_MASK);
+		slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,LAST_CTX(last_ctx));
 	}
-	new_slot_info = le32_to_cpu(slot_ctx->dev_info);
+	new_slot_info = XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info);
 
 	/* Store the usb_device pointer for later use */
 	ep->hcpriv = udev;
@@ -1794,9 +1794,9 @@ static void xhci_zero_in_ctx(struct xhci
 	ctrl_ctx->drop_flags = 0;
 	ctrl_ctx->add_flags = 0;
 	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx);
-	slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
+	slot_ctx->dev_info &= XHCI_CPU_TO_LE32(xhci->snoswap,~LAST_CTX_MASK);
 	/* Endpoint 0 is always valid */
-	slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(1));
+	slot_ctx->dev_info |= XHCI_CPU_TO_LE32(xhci->snoswap,LAST_CTX(1));
 	for (i = 1; i < 31; ++i) {
 		ep_ctx = xhci_get_ep_ctx(xhci, virt_dev->in_ctx, i);
 		ep_ctx->ep_info = 0;
@@ -2492,7 +2492,7 @@ static int xhci_reserve_bandwidth(struct
 	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
 
 	for (i = 0; i < 31; i++) {
-		if (!EP_IS_ADDED(ctrl_ctx, i) && !EP_IS_DROPPED(ctrl_ctx, i))
+		if (!EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i) && !EP_IS_DROPPED(xhci->snoswap, ctrl_ctx, i))
 			continue;
 
 		/* Make a copy of the BW info in case we need to revert this */
@@ -2501,7 +2501,7 @@ static int xhci_reserve_bandwidth(struct
 		/* Drop the endpoint from the interval table if the endpoint is
 		 * being dropped or changed.
 		 */
-		if (EP_IS_DROPPED(ctrl_ctx, i))
+		if (EP_IS_DROPPED(xhci->snoswap,ctrl_ctx, i))
 			xhci_drop_ep_from_interval_table(xhci,
 					&virt_dev->eps[i].bw_info,
 					virt_dev->bw_table,
@@ -2513,7 +2513,7 @@ static int xhci_reserve_bandwidth(struct
 	xhci_update_bw_info(xhci, virt_dev->in_ctx, ctrl_ctx, virt_dev);
 	for (i = 0; i < 31; i++) {
 		/* Add any changed or added endpoints to the interval table */
-		if (EP_IS_ADDED(ctrl_ctx, i))
+		if (EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i))
 			xhci_add_ep_to_interval_table(xhci,
 					&virt_dev->eps[i].bw_info,
 					virt_dev->bw_table,
@@ -2532,13 +2532,13 @@ static int xhci_reserve_bandwidth(struct
 
 	/* We don't have enough bandwidth for this, revert the stored info. */
 	for (i = 0; i < 31; i++) {
-		if (!EP_IS_ADDED(ctrl_ctx, i) && !EP_IS_DROPPED(ctrl_ctx, i))
+		if (!EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i) && !EP_IS_DROPPED(xhci->snoswap, ctrl_ctx, i))
 			continue;
 
 		/* Drop the new copies of any added or changed endpoints from
 		 * the interval table.
 		 */
-		if (EP_IS_ADDED(ctrl_ctx, i)) {
+		if (EP_IS_ADDED(xhci->snoswap, ctrl_ctx, i)) {
 			xhci_drop_ep_from_interval_table(xhci,
 					&virt_dev->eps[i].bw_info,
 					virt_dev->bw_table,
@@ -2550,7 +2550,7 @@ static int xhci_reserve_bandwidth(struct
 		memcpy(&virt_dev->eps[i].bw_info, &ep_bw_info[i],
 				sizeof(ep_bw_info[i]));
 		/* Add any changed or dropped endpoints back into the table */
-		if (EP_IS_DROPPED(ctrl_ctx, i))
+		if (EP_IS_DROPPED(xhci->snoswap,ctrl_ctx, i))
 			xhci_add_ep_to_interval_table(xhci,
 					&virt_dev->eps[i].bw_info,
 					virt_dev->bw_table,
@@ -2607,7 +2607,15 @@ static int xhci_configure_endpoint(struc
 	if (command) {
 		cmd_completion = command->completion;
 		cmd_status = &command->status;
-		command->command_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+		command->command_trb = xhci->cmd_ring->enqueue;
+
+		/* Enqueue pointer can be left pointing to the link TRB,
+		 * we must handle that
+		 */
+		if (TRB_TYPE_LINK_LE32(xhci->snoswap, command->command_trb->link.control))
+			command->command_trb =
+				xhci->cmd_ring->enq_seg->next->trbs;
+
 		list_add_tail(&command->cmd_list, &virt_dev->cmd_list);
 	} else {
 		cmd_completion = &virt_dev->cmd_completion;
@@ -2615,7 +2623,7 @@ static int xhci_configure_endpoint(struc
 	}
 	init_completion(cmd_completion);
 
-	cmd_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+	cmd_trb = xhci->cmd_ring->dequeue;
 	if (!ctx_change)
 		ret = xhci_queue_configure_endpoint(xhci, in_ctx->dma,
 				udev->slot_id, must_succeed);
@@ -2701,19 +2709,19 @@ int xhci_check_bandwidth(struct usb_hcd 
 
 	/* See section 4.6.6 - A0 = 1; A1 = D0 = D1 = 0 */
 	ctrl_ctx = xhci_get_input_control_ctx(xhci, virt_dev->in_ctx);
-	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
-	ctrl_ctx->add_flags &= cpu_to_le32(~EP0_FLAG);
-	ctrl_ctx->drop_flags &= cpu_to_le32(~(SLOT_FLAG | EP0_FLAG));
+	ctrl_ctx->add_flags |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG);
+	ctrl_ctx->add_flags &= XHCI_CPU_TO_LE32(xhci->snoswap,~EP0_FLAG);
+	ctrl_ctx->drop_flags &= XHCI_CPU_TO_LE32(xhci->snoswap,~(SLOT_FLAG | EP0_FLAG));
 
 	/* Don't issue the command if there's no endpoints to update. */
-	if (ctrl_ctx->add_flags == cpu_to_le32(SLOT_FLAG) &&
+	if (ctrl_ctx->add_flags == XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG) &&
 			ctrl_ctx->drop_flags == 0)
 		return 0;
 
 	xhci_dbg(xhci, "New Input Control Context:\n");
 	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx);
 	xhci_dbg_ctx(xhci, virt_dev->in_ctx,
-		     LAST_CTX_TO_EP_NUM(le32_to_cpu(slot_ctx->dev_info)));
+		     LAST_CTX_TO_EP_NUM(XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info)));
 
 	ret = xhci_configure_endpoint(xhci, udev, NULL,
 			false, false);
@@ -2724,12 +2732,12 @@ int xhci_check_bandwidth(struct usb_hcd 
 
 	xhci_dbg(xhci, "Output context after successful config ep cmd:\n");
 	xhci_dbg_ctx(xhci, virt_dev->out_ctx,
-		     LAST_CTX_TO_EP_NUM(le32_to_cpu(slot_ctx->dev_info)));
+		     LAST_CTX_TO_EP_NUM(XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_info)));
 
 	/* Free any rings that were dropped, but not changed. */
 	for (i = 1; i < 31; ++i) {
-		if ((le32_to_cpu(ctrl_ctx->drop_flags) & (1 << (i + 1))) &&
-		    !(le32_to_cpu(ctrl_ctx->add_flags) & (1 << (i + 1))))
+		if ((XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->drop_flags) & (1 << (i + 1))) &&
+		    !(XHCI_LE32_TO_CPU(xhci->snoswap,ctrl_ctx->add_flags) & (1 << (i + 1))))
 			xhci_free_or_cache_endpoint_ring(xhci, virt_dev, i);
 	}
 	xhci_zero_in_ctx(xhci, virt_dev);
@@ -2783,10 +2791,10 @@ static void xhci_setup_input_ctx_for_con
 {
 	struct xhci_input_control_ctx *ctrl_ctx;
 	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
-	ctrl_ctx->add_flags = cpu_to_le32(add_flags);
-	ctrl_ctx->drop_flags = cpu_to_le32(drop_flags);
+	ctrl_ctx->add_flags = XHCI_CPU_TO_LE32(xhci->snoswap,add_flags);
+	ctrl_ctx->drop_flags = XHCI_CPU_TO_LE32(xhci->snoswap,drop_flags);
 	xhci_slot_copy(xhci, in_ctx, out_ctx);
-	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+	ctrl_ctx->add_flags |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG);
 
 	xhci_dbg(xhci, "Input Context:\n");
 	xhci_dbg_ctx(xhci, in_ctx, xhci_last_valid_endpoint(add_flags));
@@ -2815,7 +2823,7 @@ static void xhci_setup_input_ctx_for_qui
 				deq_state->new_deq_ptr);
 		return;
 	}
-	ep_ctx->deq = cpu_to_le64(addr | deq_state->new_cycle_state);
+	ep_ctx->deq = XHCI_CPU_TO_LE64(xhci->snoswap,addr | deq_state->new_cycle_state);
 
 	added_ctxs = xhci_get_endpoint_flag_from_index(ep_index);
 	xhci_setup_input_ctx_for_config_ep(xhci, xhci->devs[slot_id]->in_ctx,
@@ -3384,7 +3392,7 @@ int xhci_discover_or_reset_device(struct
 
 	/* If device is not setup, there is no point in resetting it */
 	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx);
-	if (GET_SLOT_STATE(le32_to_cpu(slot_ctx->dev_state)) ==
+	if (GET_SLOT_STATE(XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_state)) ==
 						SLOT_STATE_DISABLED)
 		return 0;
 
@@ -3727,7 +3735,7 @@ int xhci_address_device(struct usb_hcd *
 	else
 		xhci_copy_ep0_dequeue_into_input_ctx(xhci, udev);
 	ctrl_ctx = xhci_get_input_control_ctx(xhci, virt_dev->in_ctx);
-	ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG | EP0_FLAG);
+	ctrl_ctx->add_flags = XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG | EP0_FLAG);
 	ctrl_ctx->drop_flags = 0;
 
 	xhci_dbg(xhci, "Slot ID %d Input Context:\n", udev->slot_id);
@@ -3798,7 +3806,7 @@ int xhci_address_device(struct usb_hcd *
 		 udev->slot_id,
 		 &xhci->dcbaa->dev_context_ptrs[udev->slot_id],
 		 (unsigned long long)
-		 le64_to_cpu(xhci->dcbaa->dev_context_ptrs[udev->slot_id]));
+		 XHCI_LE64_TO_CPU(xhci->snoswap,xhci->dcbaa->dev_context_ptrs[udev->slot_id]));
 	xhci_dbg(xhci, "Output Context DMA address = %#08llx\n",
 			(unsigned long long)virt_dev->out_ctx->dma);
 	xhci_dbg(xhci, "Slot ID %d Input Context:\n", udev->slot_id);
@@ -3812,7 +3820,7 @@ int xhci_address_device(struct usb_hcd *
 	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx);
 	/* Use kernel assigned address for devices; store xHC assigned
 	 * address locally. */
-	virt_dev->address = (le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK)
+	virt_dev->address = (XHCI_LE32_TO_CPU(xhci->snoswap,slot_ctx->dev_state) & DEV_ADDR_MASK)
 		+ 1;
 	/* Zero the input context control for later use */
 	ctrl_ctx->add_flags = 0;
@@ -3860,7 +3868,7 @@ static int xhci_calculate_hird_besl(stru
 	u32 field;
 
 	u2del = HCS_U2_LATENCY(xhci->hcs_params3);
-	field = le32_to_cpu(udev->bos->ext_cap->bmAttributes);
+	field = XHCI_LE32_TO_CPU(xhci->snoswap,udev->bos->ext_cap->bmAttributes);
 
 	if (field & USB_BESL_SUPPORT) {
 		for (besl_host = 0; besl_host < 16; besl_host++) {
@@ -3912,7 +3920,7 @@ static int xhci_usb2_software_lpm_test(s
 
 	/* Look for devices in lpm_failed_devs list */
 	dev_id = le16_to_cpu(udev->descriptor.idVendor) << 16 |
-			le16_to_cpu(udev->descriptor.idProduct);
+			le16_to_cpu(udev->descriptor.idProduct);//ltq_h:always swap
 	list_for_each_entry(dev_info, &xhci->lpm_failed_devs, list) {
 		if (dev_info->dev_id == dev_id) {
 			ret = -EINVAL;
@@ -4225,7 +4233,7 @@ static u16 xhci_calculate_intel_u2_timeo
 			(xhci_service_interval_to_ns(desc) > timeout_ns))
 		timeout_ns = xhci_service_interval_to_ns(desc);
 
-	u2_del_ns = le16_to_cpu(udev->bos->ss_cap->bU2DevExitLat) * 1000ULL;
+	u2_del_ns = le16_to_cpu(udev->bos->ss_cap->bU2DevExitLat) * 1000ULL;//ltq_h: always swap
 	if (u2_del_ns > timeout_ns)
 		timeout_ns = u2_del_ns;
 
@@ -4438,10 +4446,10 @@ static int xhci_change_max_exit_latency(
 	spin_unlock_irqrestore(&xhci->lock, flags);
 
 	ctrl_ctx = xhci_get_input_control_ctx(xhci, command->in_ctx);
-	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+	ctrl_ctx->add_flags |= XHCI_CPU_TO_LE32(xhci->snoswap,SLOT_FLAG);
 	slot_ctx = xhci_get_slot_ctx(xhci, command->in_ctx);
-	slot_ctx->dev_info2 &= cpu_to_le32(~((u32) MAX_EXIT));
-	slot_ctx->dev_info2 |= cpu_to_le32(max_exit_latency);
+	slot_ctx->dev_info2 &= XHCI_CPU_TO_LE32(xhci->snoswap,~((u32) MAX_EXIT));
+	slot_ctx->dev_info2 |= XHCI_CPU_TO_LE32(xhci->snoswap,max_exit_latency);
 
 	xhci_dbg(xhci, "Set up evaluate context for LPM MEL change.\n");
 	xhci_dbg(xhci, "Slot %u Input Context:\n", udev->slot_id);
@@ -4644,7 +4652,7 @@ int xhci_update_hub_device(struct usb_hc
 			think_time = (think_time / 666) - 1;
 		if (xhci->hci_version < 0x100 || hdev->speed == USB_SPEED_HIGH)
 			slot_ctx->tt_info |=
-				cpu_to_le32(TT_THINK_TIME(think_time));
+				XHCI_CPU_TO_LE32(xhci->snoswap,TT_THINK_TIME(think_time));
 	} else {
 		xhci_dbg(xhci, "xHCI version %x doesn't need hub "
 				"TT think time or number of ports\n",
@@ -4732,6 +4740,32 @@ int xhci_gen_setup(struct usb_hcd *hcd, 
 		HC_LENGTH(xhci_readl(xhci, &xhci->cap_regs->hc_capbase));
 	xhci->run_regs = hcd->regs +
 		(xhci_readl(xhci, &xhci->cap_regs->run_regs_off) & RTSOFF_MASK);
+
+#ifdef __LQ_XHCI_MSWAP__
+	{
+		unsigned char *c,c1,c2,c3,c4;
+		xhci->mswap=0;
+		xhci->hcc_params = xhci_readl(xhci, &xhci->cap_regs->hc_capbase);
+		c=&(xhci->hcc_params);
+		c1=*(c+0);
+		c2=*(c+1);
+		c3=*(c+2);
+		c4=*(c+3);
+		if(c1>0x10&&c2==0x00)
+			xhci->mswap=1;
+		//now checking
+		xhci->hcc_params = xhci_readl(xhci, &xhci->cap_regs->hc_capbase);
+		xhci->hci_version = HC_VERSION(xhci->hcc_params);
+		printk(KERN_INFO "%s() %d  MSWAP=%d\n",__func__,__LINE__,xhci->mswap);
+		printk(KERN_INFO "%s() %d  VERSION=0x%04X\n",__func__,__LINE__,xhci->hci_version);
+		printk(KERN_INFO "%s() %d  HC_LENGTH=0x%02X\n",__func__,__LINE__,HC_LENGTH(xhci->hcc_params));
+	}
+#endif
+#ifdef __LQ_XHCI_SSWAP__
+		xhci->snoswap=0;
+#endif
+
+
 	/* Cache read-only capability registers */
 	xhci->hcs_params1 = xhci_readl(xhci, &xhci->cap_regs->hcs_params1);
 	xhci->hcs_params2 = xhci_readl(xhci, &xhci->cap_regs->hcs_params2);
diff --git a/drivers/usb/host/xhci.h b/drivers/usb/host/xhci.h
--- a/drivers/usb/host/xhci.h
+++ b/drivers/usb/host/xhci.h
@@ -24,6 +24,15 @@
 #ifndef __LINUX_XHCI_HCD_H
 #define __LINUX_XHCI_HCD_H
 
+#define __LQ_XHCI_MSWAP__
+#define __LQ_XHCI_SSWAP__
+//#define __LQ_XHCI_CIP__
+
+#ifndef __LQ_XHCI_CIP__
+	#undef __LQ_XHCI_SSWAP__
+#endif
+
+
 #include <linux/usb.h>
 #include <linux/timer.h>
 #include <linux/kernel.h>
@@ -694,10 +703,34 @@ struct xhci_input_control_ctx {
 	__le32	rsvd2[6];
 };
 
-#define	EP_IS_ADDED(ctrl_ctx, i) \
-	(le32_to_cpu(ctrl_ctx->add_flags) & (1 << (i + 1)))
-#define	EP_IS_DROPPED(ctrl_ctx, i)       \
-	(le32_to_cpu(ctrl_ctx->drop_flags) & (1 << (i + 1)))
+#ifdef __LQ_XHCI_SSWAP__
+	#define	EP_IS_ADDED(dontcare, ctrl_ctx, i) \
+		(dontcare)?((ctrl_ctx->add_flags) & (1 << (i + 1))):(le32_to_cpu(ctrl_ctx->add_flags) & (1 << (i + 1)))
+	#define	EP_IS_DROPPED(dontcare, ctrl_ctx, i)       \
+		(dontcare)?((ctrl_ctx->drop_flags) & (1 << (i + 1))):(le32_to_cpu(ctrl_ctx->drop_flags) & (1 << (i + 1)))
+
+	#define	XHCI_LE32_TO_CPU(dontcare, data) \
+		(dontcare)?(data):(le32_to_cpu(data))
+	#define	XHCI_CPU_TO_LE32(dontcare, data) \
+		(dontcare)?(data):(cpu_to_le32(data))
+
+	#define	XHCI_LE64_TO_CPU(dontcare, data) \
+		(dontcare)?((((data) & 0x00000000ffffffffULL) << 32) |  (((data) & 0xffffffff00000000ULL) >> 32)):(le64_to_cpu(data))
+	#define	XHCI_CPU_TO_LE64(dontcare, data) \
+		(dontcare)?((((data) & 0x00000000ffffffffULL) << 32) |  (((data) & 0xffffffff00000000ULL) >> 32)):(cpu_to_le64(data))
+#else
+	#define	EP_IS_ADDED(dontcare, ctrl_ctx, i) \
+		(le32_to_cpu(ctrl_ctx->add_flags) & (1 << (i + 1)))
+	#define	EP_IS_DROPPED(dontcare, ctrl_ctx, i)       \
+		(le32_to_cpu(ctrl_ctx->drop_flags) & (1 << (i + 1)))
+
+	#define	XHCI_LE32_TO_CPU(dontcare, data) le32_to_cpu(data)
+	#define	XHCI_CPU_TO_LE32(dontcare, data) cpu_to_le32(data)
+
+	#define	XHCI_LE64_TO_CPU(dontcare, data) le64_to_cpu(data)
+	#define	XHCI_CPU_TO_LE64(dontcare, data) cpu_to_le64(data)
+#endif
+
 
 /* Represents everything that is needed to issue a command on the command ring.
  * It's useful to pre-allocate these for commands that cannot fail due to
@@ -1225,10 +1258,21 @@ union xhci_trb {
 
 #define TRB_TYPE_LINK(x)	(((x) & TRB_TYPE_BITMASK) == TRB_TYPE(TRB_LINK))
 /* Above, but for __le32 types -- can avoid work by swapping constants: */
-#define TRB_TYPE_LINK_LE32(x)	(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == \
-				 cpu_to_le32(TRB_TYPE(TRB_LINK)))
-#define TRB_TYPE_NOOP_LE32(x)	(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == \
-				 cpu_to_le32(TRB_TYPE(TRB_TR_NOOP)))
+#ifdef __LQ_XHCI_SSWAP__
+	#define TRB_TYPE_LINK_LE32(dontcare,x)	(dontcare)? \
+					(((x) & (TRB_TYPE_BITMASK)) == (TRB_TYPE(TRB_LINK))): \
+					(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == cpu_to_le32(TRB_TYPE(TRB_LINK)))
+
+	#define TRB_TYPE_NOOP_LE32(dontcare,x)	(dontcare)? \
+					(((x) & (TRB_TYPE_BITMASK)) == (TRB_TYPE(TRB_TR_NOOP))): \
+					(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == cpu_to_le32(TRB_TYPE(TRB_TR_NOOP)))
+#else
+	#define TRB_TYPE_LINK_LE32(dontcare,x) \
+					(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == cpu_to_le32(TRB_TYPE(TRB_LINK)))
+
+	#define TRB_TYPE_NOOP_LE32(dontcare,x) \
+					(((x) & cpu_to_le32(TRB_TYPE_BITMASK)) == cpu_to_le32(TRB_TYPE(TRB_TR_NOOP)))
+#endif
 
 #define NEC_FW_MINOR(p)		(((p) >> 0) & 0xff)
 #define NEC_FW_MAJOR(p)		(((p) >> 8) & 0xff)
@@ -1543,6 +1587,12 @@ struct xhci_hcd {
 	u32			port_status_u0;
 /* Compliance Mode Timer Triggered every 2 seconds */
 #define COMP_MODE_RCVRY_MSECS 2000
+	#ifdef __LQ_XHCI_MSWAP__
+	u8 mswap;
+	#endif
+	#ifdef __LQ_XHCI_SSWAP__
+	u8 snoswap;
+	#endif
 };
 
 /* convert between an HCD pointer and the corresponding EHCI_HCD */
@@ -1578,12 +1628,26 @@ static inline struct usb_hcd *xhci_to_hc
 static inline unsigned int xhci_readl(const struct xhci_hcd *xhci,
 		__le32 __iomem *regs)
 {
+#ifdef __LQ_XHCI_MSWAP__
+	if(xhci->mswap)
+		return swab32(readl(regs));
+	else
+		return readl(regs);
+#else
 	return readl(regs);
+#endif
 }
 static inline void xhci_writel(struct xhci_hcd *xhci,
 		const unsigned int val, __le32 __iomem *regs)
 {
+#ifdef __LQ_XHCI_MSWAP__
+	if(xhci->mswap)
+		writel(swab32(val), regs);
+	else
+		writel(val, regs);
+#else
 	writel(val, regs);
+#endif
 }
 
 /*
@@ -1599,9 +1663,25 @@ static inline u64 xhci_read_64(const str
 		__le64 __iomem *regs)
 {
 	__u32 __iomem *ptr = (__u32 __iomem *) regs;
+#ifdef __LQ_XHCI_MSWAP__
+	u64 val_lo;
+	u64 val_hi;
+	if(xhci->mswap)
+	{
+		val_lo = swab32(readl(ptr));
+		val_hi = swab32(readl(ptr + 1));
+	}
+	else
+	{
+		val_lo = readl(ptr);
+		val_hi = readl(ptr + 1);
+	}
+	return val_lo + (val_hi << 32);
+#else
 	u64 val_lo = readl(ptr);
 	u64 val_hi = readl(ptr + 1);
 	return val_lo + (val_hi << 32);
+#endif
 }
 static inline void xhci_write_64(struct xhci_hcd *xhci,
 				 const u64 val, __le64 __iomem *regs)
@@ -1609,9 +1689,22 @@ static inline void xhci_write_64(struct 
 	__u32 __iomem *ptr = (__u32 __iomem *) regs;
 	u32 val_lo = lower_32_bits(val);
 	u32 val_hi = upper_32_bits(val);
-
+#ifdef __LQ_XHCI_MSWAP__
+	if(xhci->mswap)
+	{
+		writel(swab32(val_lo), ptr);
+		writel(swab32(val_hi), ptr + 1);
+	}
+	else
+	{
+		writel(val_lo, ptr);
+		writel(val_hi, ptr + 1);
+	}
+#else
 	writel(val_lo, ptr);
 	writel(val_hi, ptr + 1);
+#endif
+
 }
 
 static inline int xhci_link_trb_quirk(struct xhci_hcd *xhci)
